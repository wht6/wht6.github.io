{"meta":{"title":"WHT","subtitle":"Enjoy your life","description":"分享技术,分享知识,分享感悟","author":"王贺天","url":"http://wht6.github.io","root":"/"},"pages":[{"title":"about","date":"2021-03-14T05:54:32.000Z","updated":"2022-04-07T00:34:13.001Z","comments":true,"path":"about/index.html","permalink":"http://wht6.github.io/about/index.html","excerpt":"","text":"me语言：Shell、Python、Go。 技能：Linux、Docker、K8S、Ansible、LVS。 知识：计算机基础、网络基础、云计算基础。 motto 已识乾坤大，犹怜草木青。 hobby阅读、电影、旅行、道家养生、跑步、围棋 connect Home: wht6 Email: wanght586@gmail.com GitHub: wht6"},{"title":"categories","date":"2021-03-14T05:52:18.000Z","updated":"2021-03-14T05:53:15.911Z","comments":true,"path":"categories/index.html","permalink":"http://wht6.github.io/categories/index.html","excerpt":"","text":""},{"title":"friends","date":"2021-03-14T05:55:08.000Z","updated":"2021-03-14T05:55:28.248Z","comments":true,"path":"friends/index.html","permalink":"http://wht6.github.io/friends/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-03-14T05:53:45.000Z","updated":"2021-03-14T05:54:10.945Z","comments":true,"path":"tags/index.html","permalink":"http://wht6.github.io/tags/index.html","excerpt":"","text":""},{"title":"contact","date":"2021-03-14T13:02:01.000Z","updated":"2021-03-14T13:02:47.365Z","comments":true,"path":"contact/index.html","permalink":"http://wht6.github.io/contact/index.html","excerpt":"","text":""},{"title":"musics","date":"2021-03-15T08:46:07.000Z","updated":"2021-03-15T08:47:22.470Z","comments":true,"path":"musics/index.html","permalink":"http://wht6.github.io/musics/index.html","excerpt":"","text":""}],"posts":[{"title":"e2ec代码浅析","slug":"e2ec代码浅析","date":"2022-08-29T08:00:00.000Z","updated":"2022-10-23T08:36:52.962Z","comments":true,"path":"posts/9d01.html","link":"","permalink":"http://wht6.github.io/posts/9d01.html","excerpt":"","text":"首先用于训练的脚本为tain_net.py 12345def main(): cfg = get_cfg(args) torch.cuda.set_device(args.device) network = make_network.get_network(cfg) train(network, cfg) 可以看到主函数main获取了基本配置，指定了训练设备，获取网络结构并进行训练。 先看train 123456789101112131415161718192021222324252627def train(network, cfg): trainer = make_trainer(network, cfg) optimizer = make_optimizer(network, cfg) scheduler = make_lr_scheduler(optimizer, cfg) recorder = make_recorder(cfg.commen.record_dir) evaluator = make_evaluator(cfg) if args.type == &#x27;finetune&#x27;: begin_epoch = load_network(network, model_dir=args.checkpoint) else: begin_epoch = load_model(network, optimizer, scheduler, recorder, args.checkpoint) train_loader, val_loader = make_data_loader(cfg=cfg) for epoch in range(begin_epoch, cfg.train.epoch): recorder.epoch = epoch trainer.train(epoch, train_loader, optimizer, recorder) scheduler.step() if (epoch + 1) % cfg.train.save_ep == 0: save_model(network, optimizer, scheduler, recorder, epoch, cfg.commen.model_dir) if (epoch + 1) % cfg.train.eval_ep == 0: trainer.val(epoch, val_loader, evaluator, recorder) return network 这里初始化了训练优化器，周期，学习率lr等，然后就是更加epoch进行训练和评估。这里看到将网络对象传入了make_trainer。 123def _wrapper_factory(network, cfg): return NetworkWrapper(network, with_dml=cfg.train.with_dml, start_epoch=cfg.train.start_epoch, weight_dict=cfg.train.weight_dict) make_trainer做的工作就是在原网络上进行一层包装，在网络之后加入了损失的计算。损失计算那部分在snake.py。 make_network函数的作用就是生成网络的整体结构，这部分在network目录下。 1234567891011121314151617181920212223242526272829303132333435363738class Network(nn.Module): def __init__(self, cfg=None): super(Network, self).__init__() num_layers = cfg.model.dla_layer head_conv = cfg.model.head_conv down_ratio = cfg.commen.down_ratio heads = cfg.model.heads self.test_stage = cfg.test.test_stage self.dla = DLASeg(&#x27;dla&#123;&#125;&#x27;.format(num_layers), heads, pretrained=True, down_ratio=down_ratio, final_kernel=1, last_level=5, head_conv=head_conv, use_dcn=cfg.model.use_dcn) self.train_decoder = Decode(num_point=cfg.commen.points_per_poly, init_stride=cfg.model.init_stride, coarse_stride=cfg.model.coarse_stride, down_sample=cfg.commen.down_ratio, min_ct_score=cfg.test.ct_score) self.gcn = Evolution(evole_ietr_num=cfg.model.evolve_iters, evolve_stride=cfg.model.evolve_stride, ro=cfg.commen.down_ratio) def forward(self, x, batch=None): output, cnn_feature = self.dla(x) if &#x27;test&#x27; not in batch[&#x27;meta&#x27;]: self.train_decoder(batch, cnn_feature, output, is_training=True) else: with torch.no_grad(): if self.test_stage == &#x27;init&#x27;: ignore = True else: ignore = False self.train_decoder(batch, cnn_feature, output, is_training=False, ignore_gloabal_deform=ignore) output = self.gcn(output, cnn_feature, batch, test_stage=self.test_stage) return outputdef get_network(cfg): network = Network(cfg) return network 网络从整体上有分为三个子部分，分别是dla、train_decoder和gcn，对应的三个函数是DLASeg、Decode和Evolution，这三个函数分别在文件dla.py、refine_decode.py和evolve.py中。首先看dla 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class DLASeg(nn.Module): def __init__(self, base_name, heads, pretrained, down_ratio, final_kernel, last_level, head_conv, out_channel=0, use_dcn=True): super(DLASeg, self).__init__() assert down_ratio in [2, 4, 8, 16] self.first_level = int(np.log2(down_ratio)) self.last_level = last_level self.base = globals()[base_name](pretrained=pretrained) channels = self.base.channels scales = [2 ** i for i in range(len(channels[self.first_level:]))] self.dla_up = DLAUp(self.first_level, channels[self.first_level:], scales, use_dcn=use_dcn) if out_channel == 0: out_channel = channels[self.first_level] self.ida_up = IDAUp(out_channel, channels[self.first_level:self.last_level], [2 ** i for i in range(self.last_level - self.first_level)], use_dcn=use_dcn) self.heads = heads for head in self.heads: classes = self.heads[head] if head_conv &gt; 0: fc = nn.Sequential( nn.Conv2d(channels[self.first_level], head_conv, kernel_size=3, padding=1, bias=True), nn.ReLU(inplace=True), nn.Conv2d(head_conv, classes, kernel_size=final_kernel, stride=1, padding=final_kernel // 2, bias=True)) if &#x27;hm&#x27; in head: fc[-1].bias.data.fill_(-2.19) else: fill_fc_weights(fc) else: fc = nn.Conv2d(channels[self.first_level], classes, kernel_size=final_kernel, stride=1, padding=final_kernel // 2, bias=True) if &#x27;hm&#x27; in head: fc.bias.data.fill_(-2.19) else: fill_fc_weights(fc) self.__setattr__(head, fc) def forward(self, x): x = self.base(x) x = self.dla_up(x) y = [] for i in range(self.last_level - self.first_level): y.append(x[i].clone()) self.ida_up(y, 0, len(y)) z = &#123;&#125; for head in self.heads: z[head] = self.__getattr__(head)(y[-1]) return z, y[-1] 函数DLASeg传过来的第一个参数base_name的值实际是’dla{}’.format(num_layers)，而num_layers值在框架的配置文件中 123456789101112class model(object): dla_layer = 34 head_conv = 256 use_dcn = True points_per_poly = commen.points_per_poly down_ratio = commen.down_ratio init_stride = 10. coarse_stride = 4. evolve_stride = 1. backbone_num_layers = 34 heads = &#123;&#x27;ct_hm&#x27;: 20, &#x27;wh&#x27;: commen.points_per_poly * 2&#125; evolve_iters = 3 所以basename的值是dla34，下面的代码是获取预训练权重的地方，确实是dla34 12345678910def get_model_url(data=&#x27;imagenet&#x27;, name=&#x27;dla34&#x27;, hash=&#x27;ba72cf86&#x27;): return join(&#x27;http://dl.yf.io/dla/models&#x27;, data, &#x27;&#123;&#125;-&#123;&#125;.pth&#x27;.format(name, hash))def dla34(pretrained=True, **kwargs): # DLA-34 model = DLA([1, 1, 1, 2, 2, 1], [16, 32, 64, 128, 256, 512], block=BasicBlock, **kwargs) if pretrained: model.load_pretrained_model(data=&#x27;imagenet&#x27;, name=&#x27;dla34&#x27;, hash=&#x27;ba72cf86&#x27;) return model 知道这部分是backbone。最终输出的通道数是512。但是后面在调用dla的时候返回的是两个特征，有点迷。 然后就是train_decoder部分： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Decode(torch.nn.Module): def __init__(self, c_in=64, num_point=128, init_stride=10., coarse_stride=4., down_sample=4., min_ct_score=0.05): super(Decode, self).__init__() self.stride = init_stride self.down_sample = down_sample self.min_ct_score = min_ct_score self.refine = Refine(c_in=c_in, num_point=num_point, stride=coarse_stride) def train_decode(self, data_input, output, cnn_feature): wh_pred = output[&#x27;wh&#x27;] ct_01 = data_input[&#x27;ct_01&#x27;].bool() ct_ind = data_input[&#x27;ct_ind&#x27;][ct_01] ct_img_idx = data_input[&#x27;ct_img_idx&#x27;][ct_01] _, _, height, width = data_input[&#x27;ct_hm&#x27;].size() ct_x, ct_y = ct_ind % width, ct_ind // width if ct_x.size(0) == 0: ct_offset = wh_pred[ct_img_idx, :, ct_y, ct_x].view(ct_x.size(0), 1, 2) else: ct_offset = wh_pred[ct_img_idx, :, ct_y, ct_x].view(ct_x.size(0), -1, 2) ct_x, ct_y = ct_x[:, None].to(torch.float32), ct_y[:, None].to(torch.float32) ct = torch.cat([ct_x, ct_y], dim=1) init_polys = ct_offset * self.stride + ct.unsqueeze(1).expand(ct_offset.size(0), ct_offset.size(1), ct_offset.size(2)) coarse_polys = self.refine(cnn_feature, ct, init_polys, ct_img_idx.clone()) output.update(&#123;&#x27;poly_init&#x27;: init_polys * self.down_sample&#125;) output.update(&#123;&#x27;poly_coarse&#x27;: coarse_polys * self.down_sample&#125;) return def test_decode(self, cnn_feature, output, K=100, min_ct_score=0.05, ignore_gloabal_deform=False): hm_pred, wh_pred = output[&#x27;ct_hm&#x27;], output[&#x27;wh&#x27;] poly_init, detection = decode_ct_hm(torch.sigmoid(hm_pred), wh_pred, K=K, stride=self.stride) valid = detection[0, :, 2] &gt;= min_ct_score poly_init, detection = poly_init[0][valid], detection[0][valid] init_polys = clip_to_image(poly_init, cnn_feature.size(2), cnn_feature.size(3)) output.update(&#123;&#x27;poly_init&#x27;: init_polys * self.down_sample&#125;) img_id = torch.zeros((len(poly_init), ), dtype=torch.int64) poly_coarse = self.refine(cnn_feature, detection[:, :2], poly_init, img_id, ignore=ignore_gloabal_deform) coarse_polys = clip_to_image(poly_coarse, cnn_feature.size(2), cnn_feature.size(3)) output.update(&#123;&#x27;poly_coarse&#x27;: coarse_polys * self.down_sample&#125;) output.update(&#123;&#x27;detection&#x27;: detection&#125;) return def forward(self, data_input, cnn_feature, output=None, is_training=True, ignore_gloabal_deform=False): if is_training: self.train_decode(data_input, output, cnn_feature) else: self.test_decode(cnn_feature, output, min_ct_score=self.min_ct_score, ignore_gloabal_deform=ignore_gloabal_deform) 分析这部分网络结构之前需要先确定它的输入特征和输出特征分别是什么。 1self.train_decoder(batch, cnn_feature, output, is_training=False, ignore_gloabal_deform=ignore) cnn_feature和output是dla输出的两个特征，但是我始终没找到这个batch的输入，但是从后面的代码可以看出来应该是groud truth。 output特征输出的直接就是回归的结果了，所以目标检测的部分在dla中。cnn_feature则参与了coarse_polys的计算，coarse_polys的计算则是由Refine函数完成的。 1234567891011121314151617181920212223242526272829303132333435class Refine(torch.nn.Module): def __init__(self, c_in=64, num_point=128, stride=4.): super(Refine, self).__init__() self.num_point = num_point self.stride = stride self.trans_feature = torch.nn.Sequential(torch.nn.Conv2d(c_in, 256, kernel_size=3, padding=1, bias=True), torch.nn.ReLU(inplace=True), torch.nn.Conv2d(256, 64, kernel_size=1, stride=1, padding=0, bias=True)) self.trans_poly = torch.nn.Linear(in_features=((num_point + 1) * 64), out_features=num_point * 4, bias=False) self.trans_fuse = torch.nn.Linear(in_features=num_point * 4, out_features=num_point * 2, bias=True) def global_deform(self, points_features, init_polys): poly_num = init_polys.size(0) points_features = self.trans_poly(points_features) offsets = self.trans_fuse(points_features).view(poly_num, self.num_point, 2) coarse_polys = offsets * self.stride + init_polys.detach() return coarse_polys def forward(self, feature, ct_polys, init_polys, ct_img_idx, ignore=False): if ignore or len(init_polys) == 0: return init_polys h, w = feature.size(2), feature.size(3) poly_num = ct_polys.size(0) feature = self.trans_feature(feature) ct_polys = ct_polys.unsqueeze(1).expand(init_polys.size(0), 1, init_polys.size(2)) points = torch.cat([ct_polys, init_polys], dim=1) feature_points = get_gcn_feature(feature, points, ct_img_idx, h, w).view(poly_num, -1) coarse_polys = self.global_deform(feature_points, init_polys) return coarse_polys 这里先用self.trans_feature进行通道降维，然后用get_gcn_feature进行了特征压缩还是采样，反正维数是降低了。之后self.global_deform就是两个线性MLP层计算offset，之后再对init_polys进行变形。 最后一个子部分就是Evolution， 12345678910111213141516171819202122232425262728def evolve_poly(self, snake, cnn_feature, i_it_poly, c_it_poly, ind, stride=1., ignore=False): if ignore: return i_it_poly * self.ro if len(i_it_poly) == 0: return torch.zeros_like(i_it_poly) h, w = cnn_feature.size(2), cnn_feature.size(3) init_feature = get_gcn_feature(cnn_feature, i_it_poly, ind, h, w) c_it_poly = c_it_poly * self.ro init_input = torch.cat([init_feature, c_it_poly.permute(0, 2, 1)], dim=1) offset = snake(init_input).permute(0, 2, 1) i_poly = i_it_poly * self.ro + offset * stride return i_polydef foward_train(self, output, batch, cnn_feature): ret = output init = self.prepare_training(output, batch) py_pred = self.evolve_poly(self.evolve_gcn, cnn_feature, init[&#x27;img_init_polys&#x27;], init[&#x27;can_init_polys&#x27;], init[&#x27;py_ind&#x27;], stride=self.evolve_stride) py_preds = [py_pred] for i in range(self.iter): py_pred = py_pred / self.ro c_py_pred = img_poly_to_can_poly(py_pred) evolve_gcn = self.__getattr__(&#x27;evolve_gcn&#x27; + str(i)) py_pred = self.evolve_poly(evolve_gcn, cnn_feature, py_pred, c_py_pred, init[&#x27;py_ind&#x27;], stride=self.evolve_stride) py_preds.append(py_pred) ret.update(&#123;&#x27;py_pred&#x27;: py_preds, &#x27;img_gt_polys&#x27;: init[&#x27;img_gt_polys&#x27;] * self.ro&#125;) return output self.iter的值为2，所以进行了两次重复的优化过程。这里的self.ro是4，表示固定四个方向，即将轮廓分成四个部分，每个部分的真值只和每个部分的预测值匹配，降低了点匹配的计算量，右加快了模型收敛的速度。不固定的点则预测一次偏移之后，再调整为均匀分布，然后再预测，在调整，一共两次。 DML那部分代码在network的包装中，主要是运用了两种匹配方法来分别计算loss后再求平均，一个匹配方法就是根据最小距离，另一个匹配方法是通过一个算法找到特殊的关键点（一般是明显的拐点），然后计算将关键点根据距离匹配后其他点依次匹配。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://wht6.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"实例分割","slug":"实例分割","permalink":"http://wht6.github.io/tags/%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2/"}]},{"title":"Python和Go中内置的排序和查找","slug":"Python和Go中内置的排序和查找","date":"2022-06-29T08:00:00.000Z","updated":"2022-09-07T06:45:08.453Z","comments":true,"path":"posts/255d.html","link":"","permalink":"http://wht6.github.io/posts/255d.html","excerpt":"","text":"Python排序sort()和sorted()是Python中的内置函数，常规用法是对一个列表进行排序，【排序稳定】。 123lings = [2, 3, 8, 5]lings.sort() # 正序，从小到大lings.sort(reverse=True)# 反序，从大到小 sort的key参数可以指定排序的key值，通过lambda函数的返回值确定排序的key。 12name_mark_age = [(&#x27;zhangsan&#x27;,&#x27;A&#x27;,15),(&#x27;LISI&#x27;,&#x27;B&#x27;,14),(&#x27;WANGWU&#x27;,&#x27;A&#x27;,16)]name_mark_age.sort(key = lambda x: x[2]) #根据年龄排序 1234567891011121314151617class Student: def __init__(self, name, grade, age): self.name = name self.grade = grade self.age = age def __repr__(self): return repr((self.name, self.grade, self.age))student_objects = [ Student(&#x27;john&#x27;, &#x27;A&#x27;, 15), # 注意这里，用class Student来生成列表内的值 Student(&#x27;jane&#x27;, &#x27;B&#x27;, 12), # 因此，可以通过student_objects[i].age来访问某个名称的年龄,i=0,则是john的年龄 Student(&#x27;dave&#x27;, &#x27;B&#x27;, 10),]student_objects.sort(key=lambda x: x.age)print(student_objects) 12345# 包含多个key的条件排序# 1、按照元组的第一个从小到大排序# 2、如果第一个相同 则按照元组第2个从大到小 排序a = [[2,3],[4,1],(2,8),(2,1),(3,4)]b = a.sort(key=lambda x: (x[0], -x[1])) 12345678d = &#123;&#x27;b&#x27;: 1, &#x27;a&#x27;: 2, &#x27;c&#x27;: 10&#125;d_sorted_by_key = sorted(d.items(), key=lambda x: x[0]) # 根据字典键的升序排序d_sorted_by_value = sorted(d.items(), key=lambda x: x[1]) # 根据字典值的升序排序d_sorted_by_key[(&#x27;a&#x27;, 2), (&#x27;b&#x27;, 1), (&#x27;c&#x27;, 10)]d_sorted_by_value[(&#x27;b&#x27;, 1), (&#x27;a&#x27;, 2), (&#x27;c&#x27;, 10)] Go排序golang使用sort()函数，需要引入sort包。常规用法对数组或切片排序【sort.Sort()函数默认使用快速排序，不稳定，如果想要使用稳定排序，用sort.Stable()】。 1234nums := []int&#123;2, 31, 5, 6, 3&#125;sort.Sort(sort.IntSlice(nums)) //正序sort.Sort(sort.Reverse(sort.IntSlice(nums))) //逆序fmt.Println(nums) sort包里定义了一个接口类型，里面包含了三个函数，定义的类型只要实现了这个接口就可以用sort排序。内置已经实现了这个接口的类型是IntSlice、FloatSlice和StringSlince，针对int类型、float类型和字符串类型的切片进行排序。 1234567891011type Interface interface &#123; Len() int Less(i, j int) bool Swap(i, j int)&#125;func Sort(data Interface) &#123; n := data.Len() quickSort(data, 0, n, maxDepth(n)) //默认用快排算法排序&#125; 自定义排序的类型。 123456789101112131415161718192021222324type student struct &#123; name string grade string age int&#125;type studentslice []studentfunc (stu studentslice) Len() int &#123; return len(stu)&#125;func (stu studentslice) Swap(i, j int) &#123; stu[i], stu[j] = stu[j], stu[i]&#125;func (stu studentslice) Less(i, j int) bool &#123; return stu[i].age &lt; stu[j].age //正序&#125; stu := []student&#123;&#123;&quot;john&quot;, &quot;A&quot;, 15&#125;, &#123;&quot;jane&quot;, &quot;B&quot;, 12&#125;, &#123;&quot;dave&quot;, &quot;B&quot;, 10&#125;&#125; sort.Sort(studentslice(stu)) fmt.Println(stu) 123456789101112131415161718192021222324252627282930313233343536373839404142// 包含多个key的条件排序// 1、按照元组的第一个从小到大排序// 2、如果第一个相同 则按照元组第2个从大到小 排序package mainimport ( &quot;fmt&quot; &quot;sort&quot;)type mylis struct &#123; a int b int&#125;type mylissilce []mylisfunc main() &#123; mls := []mylis&#123;&#123;2, 3&#125;, &#123;4, 1&#125;, &#123;2, 8&#125;, &#123;2, 1&#125;, &#123;3, 4&#125;&#125; sort.Sort(mylissilce(mls)) fmt.Println(mls)&#125;func (s mylissilce) Len() int &#123; return len(s)&#125;func (s mylissilce) Swap(i, j int) &#123; s[i], s[j] = s[j], s[i]&#125;func (s mylissilce) Less(i, j int) bool &#123; if s[i].a == s[j].a &#123; return s[i].b &gt; s[j].b &#125; else &#123; return s[i].a &lt; s[j].a &#125;&#125; Python查找Python二分查找在bisect包中，使用bisect_left函数，bisect_left(list1, i)，二分查找i，返回list中的索引，如果只有一个，返回对应元素的索引，如果没有找到，就返回刚好比i大的元素对应的索引，如果找到多个，就返回最左边的元素的索引。 Go查找go的sort包也有二分查找的函数，基本用法： 123a := []int&#123;1, 2, 3, 4, 6, 7, 8&#125; x := 2 i := sort.SearchInts(a, x) SearchInts、SearchFloat64s、SearchStrings分别对int类型、float类型和字符串类型的slice进行查找，slice是升序排列。 自定义查找，需要重写比较函数。 12345a := []int&#123;1, 2, 3, 4, 4, 4, 6, 7, 8&#125;as := sort.Search(len(a), func(i int) bool &#123; return a[i] &gt;= 4 //升序的话，只能是大于或大于等于，返回第一个大于4的索引或大于等于4的索引&#125;) fmt.Println(as)","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://wht6.github.io/tags/Go/"},{"name":"Python","slug":"Python","permalink":"http://wht6.github.io/tags/Python/"}]},{"title":"MySQL隔离级别实现原理","slug":"mysql隔离级别实现原理","date":"2022-06-26T08:00:00.000Z","updated":"2022-09-23T13:22:51.029Z","comments":true,"path":"posts/51f6.html","link":"","permalink":"http://wht6.github.io/posts/51f6.html","excerpt":"","text":"MVCC在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。 当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。 回滚日志undo log对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。同时你会发现，即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的。 你一定会问，回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 read-view 的时候。 长事务 基于上面的说明，我们来讨论一下为什么建议你尽量不要使用长事务。 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。 除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库，这个我们会在后面讲锁的时候展开。 什么操作可能会导致长事务？ set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。 set autocommit=1，好处是自动提交减少了语句的交互次数，并且避免了长事务。缺点是不主动启动事务就不能明确知道每个语句是否处于事务中。建议方式：在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。 可重复读原理对于可重复读，查询只承认在事务启动前就已经提交完成的数据。 怎么实现的呢？首先介绍需要了解的概念。 一致性视图consistent read view：一致性视图和视图view没什么关系，视图view用查询语句定义的虚拟表，而一致性视图是InnoDB实现MVCC的一种手段，用于读已提交和可重复读两个隔离级别的实现。 事务ID：InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。每行数据都是有多个版本的，每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这行数据版本的事务 ID，记为 row trx_id。也就是说每行数据有多个事务ID，叫做 row trx_id，对应多个版本。 快照：快照是基于整库的，一个快照对应一个历史版本的数据库，但是快照并非真的将数据库完整的保留一份，而是根据当前版本和回滚日志计算出来的。 下面讲解一致性视图是如何实现可重复读的。 InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。而数据版本的可见性规则，就是基于数据的 row trx_id 和这个一致性视图的对比结果得到的。这个视图数组把所有的 row trx_id 分成了几种不同的情况。 已提交事务：在当前事务之前已经提交了。 未提交事务：当前事务启动的时候，这些事务已经启动了，只是还没提交。 未开始事务：当前事务启动的时候，这些事务还没开始，可能会在当前事务进行中的时候启动。 视图数组里面存放的是事务ID，事务ID的范围是低水位到高水位。这个视图数组就是一致性视图。 对于可重复读来说，一致性视图创建于事务启动的时候，之后这个视图数组就不会再改变了，也就是高水位和低水位不会在变动了。但是行数据对应的row trx_id是会变的，因为一行数据对应多个row trx_id，也可以将所有的row trx_id看成一个数组。 当前事务在进行读操作时，会查询这行数据的所有的row trx_id，与视图数组对比，找到可见的版本中最大的，然后根据undo log回滚到row trx_id对应的版本。 判断row trx_id是否可见的方法： 如果落在绿色部分，可见。 如果落在黄色部分，两种情况： ​ a. 是当前事务自己生成的，可见。 ​ b. 当前读，可见。 ​ c. 其他，不可见。 如果落在红色部分，不可见。 可重复读，事务开始后，一致性视图就不变了，所以不论在什么时候查询，看到的要么是之前已经提交的数据，要么是自己生成的数据，看到这行数据的结果始终都是一致的。 如果要对进行更新操作呢？ 更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。除了 update 语句外，select 语句如果加锁，也是当前读。当前读读到的是当前的最新数据。 如果一个事务更新了数据，另一个事务的当前读是否一定能读到最新数据呢？ 不一定。更新数据需要加写锁，一个事务如果修改了数据，但是没有释放锁，另一个当前读的事务就阻塞了，需要等待锁的释放。 读已提交原理对于读已提交，查询只承认在语句启动前就已经提交完成的数据。 读已提交也是用到了一致性视图，不同之处在于 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图； 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。 每次开始执行一个新的语句的时候会更新视图数组，在此之前所有提交的事务，都会变得可见。","categories":[{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://wht6.github.io/tags/MySQL/"}]},{"title":"MySQL运行中的问题解决","slug":"MySQL运行中的问题解决","date":"2022-06-24T06:00:00.000Z","updated":"2022-08-11T14:01:38.773Z","comments":true,"path":"posts/e60f.html","link":"","permalink":"http://wht6.github.io/posts/e60f.html","excerpt":"","text":"数据库在实际运行中，表是动态变化的，可能出现的问题如下： 日志更新问题： 数据库在更新时，会产生binlog、redo log、undo log。日志的产生是会对性能造成一定的影响的。 binlog：server层产生的逻辑日志（逻辑日志记录的数据的变化，而不是物理数据页的变化）。（数据库有任何的变化的时候（增删改），就会记录在binlog） undo log：InnoDB产生的逻辑日志，保证隔离性、原子性。 redo log：InnoDB产生的物理日志，保证持久化。 锁问题： 客户端执行SQL时，会产生各种行锁、表锁、元数据锁。 一个客户端产生的锁，会干扰其他客户端SQL的执行。 两个客户端之间可能产生死锁。 事务问题： 事务可能会造成查询到的数据与磁盘上的不一致，客户端可能因为脏读、幻读等而看不到已经更新的数据，并且事务可能会产生隐式锁。 MySQL的日志体系server层产生的binlog用来进行数据复制。InnoDB产生的redo log、undo log，用来实现事务ACID。 这三个日志都不是用来给管理员查看的，而是运行所必要的日志。 binlog叫归档日志，记录在专门的文件中，主要用来进行数据复制和数据传送，binlog完整记录了数据库每次的数据操作，可作为数据闪回手段。 undo log叫回滚日志，位于表空间的undo segment中，主要用于事务回滚和展示旧版本。对任何数据（包括缓存）的更新，都先写undo log。 1234-- 如果原来是a，下面为更新操作UPDATE name = &#x27;b&#x27;-- undo日志就类似于UPDATE name = &#x27;a&#x27; redo log叫重做日志，主要用于记录数据页的变化，属于物理日志。InnoDB遵循“日志优先”原则，即先记录更新日志，再做数据更新，并且只要日志记录了，数据一定会更新，所以记录redo log视为数据已经更新。redo log存储在4个1GB的文件中，当出现更新操作时，会将更新操作的redo日志写入这些文件中，然后将数据页加载到内存中更新数据，当更新的数据页被重新写入硬盘的时候，会删除redo log文件中的相应的redo log。 redo log文件的是循环写入和删除的，写入删除过程如图所示。 write pos是当前日志的写入点，check point是日志擦除点，当数据被更新到硬盘时擦除。黄色的表示已经在文件中记录了redo log但还没有进行数据更新，一旦数据更新，箭头就会顺时针旋转，绿色的表示redo log文件的剩余空间，当有新的数据更新的时候就会将redo log写入文件，箭头也会顺时针旋转。 当write pos追上check point时，事务无法提交，需要等待check point推进。 只要redo log不丢，数据就不会丢失。 如果其他人查询更新的数据，会先去内存中查最新的数据页。 InnoDB数据更新流程 server接收到客户端发过来的更新命令后，开始查询数据，将查询到是数据从磁盘读入到内存中，在内存中找出行数据并进行修改，然后写回滚日志undo log（比如更新是把A改成B，undo log就记录把B改成A），再去更新内存中的数据页（如果这会其他人查询数据库的话就能查到更新的数据了），然后将相应的重做日志redo log写入内存，这个事务进行到这里就可以准备提交了。如果后面还有更新操作，继续走上述流程，直到接收到COMMIT命令，正式提交事务，将binlog写入内存（binlog包含上面所有的更新操作，作用是为了同步备库），如果在提交事务之前有回滚的操作就执行undo log的内容。 整个过程叫做两阶段提交，第一阶段是更新数据和写undo log和redo log，第二阶段是提交事务。 可以看到更新期间redo log和undo log都是写入内存中的，redo log并未写入环形的磁盘文件中，redo log写入磁盘的方式涉及到刷盘的操作，控制redo log刷盘方式的参数是innodb_flush_log_at_trx_commit，参数值为0表示异步每秒刷盘（每隔1s将redo log写入磁盘），参数值为1表示每一个事务刷盘（每次提交事务的时候将redo log写入磁盘），参数值为N表示每N个事务刷盘，建议参数值设置为1。控制bin log刷盘方式的参数是sync_binlog，参数值为0表示自动控制刷盘，参数值为1表示每一个事务刷盘，参数值为N表示每N个事务刷盘，建议参数值设置为1。 如果在redo log刷盘前系统崩溃，会数据丢失，磁盘中数据未更新，redo log也没记录。 如果在redo log刷盘后系统崩溃，重启时会对redo log进行重放，重写内存中的数据页，重写binlog。 可以看出，只要redo log写入磁盘，数据就是安全的。 redo log在binlog之前的原因是redo log是保证数据写入的关键点，而binlog一旦写入无法撤回，因为binlog写入磁盘后是要同步到备库的。如果先写binlog了，而redo log还在内存中，那么如果这时候系统崩溃，主库上数据丢失，但是备库上有，数据就不同步了。 因为更新数据时，只更新了内存中的数据页，没有立即更新到磁盘，此时内存中的数据页与磁盘中的数据页不一致，称为脏页。 而刷脏就是将内存中数据页保存到磁盘。刷脏的同时会删除此页相关的redo log，并推进checkpoint。【这就是InnoDB的日志优先原则，先写日志（redo log刷盘），再写数据（刷脏）】 刷脏是数据库的主动操作，刷脏的原因是内存中脏页太多导致内存不足，或者是redo log环形磁盘文件写满了需要推进checkpoint，或者系统空闲提前刷脏以预防上述情况，或者MySQL关闭前保存数据。前两种情况属于是被迫刷脏，会导致系统卡顿，带来性能问题。 避免被迫刷脏的方法： 1、告知InnoDB服务器的硬盘性能（IOPS），从而使其设定合适的刷脏速度。【配置项：innodb_io_capacity】 2、配置合理的脏页比例上限，当脏页比例接近设定的上限，会加速刷脏页（建议保持默认值75，太高太低都不好）。 3、开启“顺便刷脏”策略。这个主要是针对机械磁盘，机械磁盘的特点是连续读写的性能高于不连续读写，所以刷页的时候尽量刷在磁盘上连续存储的页。（如果是SSD，默认设置不开启此策略） MySQL的锁按照粒度分，MySQL锁可以分为全局锁、表级锁、行锁。 全局锁会锁住所有表，整个库无法修改。全局锁命令，FTWRL（Flush table with read lock），此命令使整个库处于只读状态，主要用途是保证备份的一致性，一般只在备库使用。 表级锁分为表锁（数据锁）和元数据锁（MDL，metadata lock）（元数据包括表的结构、字段、数据类型、索引等）。表锁命令，lock tables xxx read/write，read是只读，write是禁止读写，表锁并不经常使用。事务在访问数据时，会自动给表加MDL读锁，事务在修改元数据时，会自动给表加MDL写锁。 行锁会锁住数据行，分为共享锁（又叫读锁和S锁）和独占锁（又叫写锁、X锁和排他锁）。S锁是自己要读，不让别人写，X锁是自己要写，不让别人读和写。只有S锁和S锁是兼容的，其他情况均不兼容，即多个事务都给某行数据加S锁是允许的。 从页中取出行数据之后是会给数据行加X锁，其他人无法读写这行数据，这里就能够放心大胆的修改数据了。如果在此之前这个数据行已经被加了S锁或X锁，那么这里事务会卡住，只能等待该数据行其他锁的释放才能继续。最后在COMMIT提交事务的时候会将前面加的X锁释放掉。 MySQL的事务事务的特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。 两阶段提交保证了事务的原子性，undo log用来回滚事务的更改。 锁和两阶段提交保证了事务的一致性，即数据修改前和修改后前后一致。 锁和undo log实现了事务的隔离性，即事务不能被其他事务所干扰。 redo log实现了事务的持久性，即事务一旦被提交，它对数据库中数据的改变就是永久的。 隔离性包括四个等级：读未提交（Read Uncommitted）、读提交（Read Committed）、可重复读（Repeatable Read）、串行化（Serializable）。 读未提交：并发的事务中，一个事务未提交的修改另一个事务也是可以看到的。实现：读写都不加锁，不隔离。特点：性能最好，但是等于没有事务，很少采用。 读提交：并发的事务中，事务A提交了之后，事务B才能看到A的修改（事务A未提交之前，事务B读到的是之前的历史数据快照）。实现：写的时候加X锁，提交的时候释放。特点：这是Oracle的默认隔离级别。 可重复读：并发事务中，事务A提交了之后，事务B还是看不到A的修改，直到事务B也提交才能看到A的修改。也就是一个事务进行期间，无论什么时候读，读到的数据都是一致的，直到提交事务之后，才能看到别的事务提交的修改。实现：写的时候加X锁，提交的时候释放。特点：这是MySQL的默认隔离级别。 串行化：不允许并发，对于一条数据，同时只能有一个事务进行写操作。实现：读加S锁，写加X锁，提交时释放。特点：事务隔离性最高，性能太差，很少采用。 1234-- 设置隔离级别set session TRANSACTION ISOLATION LEVEL serializable;-- 查询当前隔离级别select @@tx_isolation; 我们可以设置数据库的隔离级别，然后同时开启两个事务（begin开始事务，commit提交事务），来测试这四个等级出现的现象。 MVCC并发版本控制MVCC使得每个事务都可以读到特定的版本，InnoDB主要用undo log来实现MVCC。 undo log除了回滚事务之外还可以用来获取数据的历史版本(通过回滚就能推算出数据的历史版本)。 快照读（一致性非锁定读）：不锁定数据的情况下，读取数据的特定历史版本。版本由事务的具体需求确定，对于读已提交，读到的是每次SELECT时其他事务的提交结果，对于可重复读，读到的是本事务开始时其他事务的提交结果。 当前读（一致性锁定读）：直接读取当前最新版本，并加锁（update、delete、select for update加X锁，select for share mode加S锁）。如果当前版本已经被其他事务加锁，则本事务阻塞等待。对于串行化，所有的读都是当前读。对于读已提交和不可重复读，只有在真正修改的时候是当前读。 隔离问题脏读：读到了其他事务未提交的数据。 不可重复读：同样的查询读到的内容不一样。 幻读：同样的查询读到了更多（更少）的数据。 表中展示了四种隔离级别是否解决了三种隔离问题。 在可重复读级别，对于新插入的数据，由于无法回退到事务开始之前数据的历史版本，所以新插入的数据还是可以读到，所以可重复读通常情况是不解决幻读问题的，但是MySQL通过Next-Key锁可以部分解决幻读问题。 Next-Key锁是行锁+间隙锁，间隙锁的功能和行锁相同，只是针对间隙加锁。间隙锁不允许在间隙插入。 可重复读加锁时会同时锁住数据及其左右间隙。 Next-Key锁中Next-Key表示一个间隙+一条行记录，这是基本单位，Next-Key Lock会对查找过程中扫描过的范围都加锁，但是边界区域是否加锁根据不同查询有不同的情况。对于唯一索引等值查询，最右一个扫描到的不满足条件的行记录值不加锁。对于非唯一索引等值查询，最右一个扫描到的不满足条件的行记录值不加行锁，但是要加间隙锁。对于主键索引范围锁，最左侧行记录不加间隙锁，最右侧行记录加间隙锁和行锁。对于非唯一索引范围锁，最左侧行记录的间隙加锁和最右侧行记录的右间隙加锁。对于非索引字段查询，因为字段没有索引，走主键索引扫描，锁全表（包括所有的间隙）（锁全表会影响查询性能）。【整体的规则是在有可能会插入数据从而产生幻读的间隙加锁。】 长事务问题长事务是指运行时间过长的事务，其主要危害是因为行级锁长时间无法释放，导致其他事务等待。还可能导致死锁的产生，或因为MDL锁hold住大量事务，造成MySQL崩溃。 针对行级锁长时间无法释放导致其他事务等待的问题，我们可以设定合理的锁等待时间，当一个事务读取某个被其他事务锁住的行数据时，如果超出设定的锁等待时间就返回错误。 死锁情况的产生原因是两个事务都依赖对方的锁释放，但是两个事务的锁都长时间不释放。 解决方法是开启主动死锁检测（MySQL中默认开启），当MySQL检测到死锁时，会回滚代价较小的事务。 当有大量事务并发的时候，主动死锁检测会影响性能。 遇到锁不兼容时（只有读锁和读锁兼容），后面申请MDL锁的事务会形成一个队列 例如，事务A是SELECT语句加了MDL读锁，紧接着事务B是ALTER语句需要申请MDL写锁，此时事务B就进入一个队列，即使后面事务C是SELECT语句申请MDL读锁也需要放入队列中等待。 解决办法是alter语句之前，查看是否有长事务未提交。【查看长事务：information_schema库的innodb_trx表；查看锁：information_schema库的innodb_locks表；查看阻塞的事务：information_schema库的innodb_lock_waits表。MySQL 8.0中查看锁：performance_schema库的data_locks表；查看锁等待：performance_schema库的data_lock_waits表；查看MDL锁：performance_schema库的medadata_locks表；】 业务上的建议：①控制长事务，没有必要的情况下不开启事务。②数据修改（当前读）尽量放在事务后部，降低锁时间。","categories":[{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://wht6.github.io/tags/MySQL/"}]},{"title":"MySQL查询优化","slug":"MySQL查询优化","date":"2022-05-29T06:00:00.000Z","updated":"2022-08-09T13:53:04.058Z","comments":true,"path":"posts/d740.html","link":"","permalink":"http://wht6.github.io/posts/d740.html","excerpt":"","text":"索引覆盖索引覆盖：查询语句从执行到返回结果均使用同一个索引。索引覆盖可以有效的减少回表。 下面通过具体的例子解释什么是索引覆盖。 123456789101112131415-- 查看inventory的建表语句show create table inventory;-- 显示如下CREATE TABLE `inventory` ( `inventory_id` mediumint(8) unsigned NOT NULL AUTO_INCREMENT, `film_id` smallint(5) unsigned NOT NULL, `store_id` tinyint(3) unsigned NOT NULL, `last_update` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`inventory_id`), KEY `idx_fk_film_id` (`film_id`), KEY `idx_store_id_film_id` (`store_id`,`film_id`), CONSTRAINT `fk_inventory_film` FOREIGN KEY (`film_id`) REFERENCES `film` (`film_id`) ON UPDATE CASCADE, CONSTRAINT `fk_inventory_store` FOREIGN KEY (`store_id`) REFERENCES `store` (`store_id`) ON UPDATE CASCADE) ENGINE=InnoDB AUTO_INCREMENT=4582 DEFAULT CHARSET=utf8mb4 可以看到inventory表中存在一个联合索引store_id和film_id。 然后我们新建一个inventor_1表，去除联合索引，把两个外键也去除。 123456789101112CREATE TABLE `inventory_1` ( `inventory_id` mediumint(8) unsigned NOT NULL AUTO_INCREMENT, `film_id` smallint(5) unsigned NOT NULL, `store_id` tinyint(3) unsigned NOT NULL, `last_update` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`inventory_id`), KEY `idx_fk_film_id` (`film_id`)) ENGINE=InnoDB AUTO_INCREMENT=4582 DEFAULT CHARSET=utf8mb4;-- 把原表的数据插入进来insert into inventory_1 select * from inventory; 查看下面两个查询语句的执行计划： 123456789101112131415-- 查询1explain SELECT store_id,film_id FROM sakila.`inventory_1` where store_id=1;+----+-------------+-------------+------------+------+---------------+------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------------+------------+------+---------------+------+---------+------+------+----------+-------------+| 1 | SIMPLE | inventory_1 | NULL | ALL | NULL | NULL | NULL | NULL | 4581 | 10.00 | Using where |+----+-------------+-------------+------------+------+---------------+------+---------+------+------+----------+-------------+-- 查询2explain SELECT store_id,film_id FROM sakila.`inventory` where store_id=1;+----+-------------+-----------+------------+------+----------------------+----------------------+---------+-------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-----------+------------+------+----------------------+----------------------+---------+-------+------+----------+-------------+| 1 | SIMPLE | inventory | NULL | ref | idx_store_id_film_id | idx_store_id_film_id | 1 | const | 2270 | 100.00 | Using index |+----+-------------+-----------+------------+------+----------------------+----------------------+---------+-------+------+----------+-------------+ 查询1的key字段为NULL，表示没有使用索引，Extra字段是Using where，表示使用了where扫描。 查询1的key字段为idx_store_id_film_id，表示使用了联合索引，Extra字段是Using index，表示用到了索引覆盖。 因为查询字段store_id位于联合索引的左侧，省略了最右侧字段film_id，是符合联合索引的查询条件的，所有直接用联合索引就可以查到store_id和film_id，而不用回表。 12345678910111213141516-- 查询3explain SELECT inventory_id,store_id,film_id FROM sakila.`inventory` where store_id=1;+----+-------------+-----------+------------+------+----------------------+----------------------+---------+-------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-----------+------------+------+----------------------+----------------------+---------+-------+------+----------+-------------+| 1 | SIMPLE | inventory | NULL | ref | idx_store_id_film_id | idx_store_id_film_id | 1 | const | 2270 | 100.00 | Using index |+----+-------------+-----------+------------+------+----------------------+----------------------+---------+-------+------+----------+-------------+-- 查询4explain SELECT inventory_id,store_id,film_id,last_update FROM sakila.`inventory` where store_id=1;+----+-------------+-----------+------------+------+----------------------+----------------------+---------+-------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-----------+------------+------+----------------------+----------------------+---------+-------+------+----------+-------+| 1 | SIMPLE | inventory | NULL | ref | idx_store_id_film_id | idx_store_id_film_id | 1 | const | 2270 | 100.00 | NULL |+----+-------------+-----------+------------+------+----------------------+----------------------+---------+-------+------+----------+-------+ 查询3比查询2多查询了inventory_id字段，但是还是用到了索引覆盖，因为联合索引的辅助索引表除了包含索引字段也包含主键，主键是用来回表的，因此仅利用联合索引就可以查询到这三个字段，而不用回表。 查询4比查询3多查询了last_update字段，因为辅助索引表中查不到last_update字段，所以需要利用主键回表去查询主索引，因此查询4并未用到索引覆盖。查询效率大大降低。 从上面的例子可以看出，索引覆盖可以通过取消回表的操作，提升查询效率。只需要搜索一次B+树即可得到结果。可以通过优化SQL语句或优化联合索引来使用索引覆盖。 但是联合索引多了会占据更大的磁盘容量。 索引选择在表中存在多个索引的情况下，MySQL是如何确定使用哪一条索引的？ MySQL在选取索引时，会参考索引的基数（Cardinality），索引的基数即MySQL估算出来的索引字段的区分度，反映的是这个字段有多少中取值。MySQL估算基数的方法是选取几个页（Page）（即B+树的叶子节点）算出取值个数的平均值，再乘以页数，即为基数。 下面通过一个例子来理解基数。 1234567CREATE TABLE sakila.city_1(city VARCHAR(50) NOT NULL); INSERT INTO sakila.city_1 SELECT city FROM sakila.city; INSERT INTO sakila.city_1 SELECT city FROM sakila.city; INSERT INTO sakila.city_1 SELECT city FROM sakila.city; INSERT INTO sakila.city_1 SELECT city FROM sakila.city; INSERT INTO sakila.city_1 SELECT city FROM sakila.city; UPDATE sakila.city_1 set city=(SELECT city from sakila.city ORDER BY RAND() LIMIT 1); 建立一个city_1表，是city数据的5倍，并且将值的顺序打乱了。 123456789101112131415161718192021222324252627-- 查看表结构和数据desc city_1;select * from city_1;-- 添加前缀索引ALTER TABLE sakila.city_1 ADD key(city(1));ALTER TABLE sakila.city_1 ADD key(city(2));ALTER TABLE sakila.city_1 ADD key(city(3));ALTER TABLE sakila.city_1 ADD key(city(4));ALTER TABLE sakila.city_1 ADD key(city(5));ALTER TABLE sakila.city_1 ADD key(city(6));ALTER TABLE sakila.city_1 ADD key(city(7));ALTER TABLE sakila.city_1 ADD key(city(8));-- 查看表索引show index from city_1;+--------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+--------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| city_1 | 1 | city | 1 | city | A | 26 | 1 | NULL | | BTREE | | || city_1 | 1 | city_2 | 1 | city | A | 187 | 2 | NULL | | BTREE | | || city_1 | 1 | city_3 | 1 | city | A | 455 | 3 | NULL | | BTREE | | || city_1 | 1 | city_4 | 1 | city | A | 557 | 4 | NULL | | BTREE | | || city_1 | 1 | city_5 | 1 | city | A | 579 | 5 | NULL | | BTREE | | || city_1 | 1 | city_6 | 1 | city | A | 588 | 6 | NULL | | BTREE | | || city_1 | 1 | city_7 | 1 | city | A | 590 | 7 | NULL | | BTREE | | || city_1 | 1 | city_8 | 1 | city | A | 590 | 8 | NULL | | BTREE | | |+--------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+--------------- 可以看到有个Cardinality字段，表示基数的值。前缀长度为1的索引是26，说明该索引的区分度是26，即将数据分为26份。基数值越大，查询效率越高，占据的磁盘空间也相对越大。从上面的结果可以看出前缀长度为3或4的时候区分度增长较快，后面随着长度增加区分度就不怎么变了，所以选择长度3或4的前缀就好了，可以保证查询效率的同时节省空间。 当然MySQL会选择基数最大的索引作为实际查询中使用的索引。如果我们不想MySQL自动选择索引可以自己强制指定，使用force index。 因此MySQL是抽样估计的索引，并不一定准确，如果要重新估计索引（即重新计算索引的基数），使用analyze table。 count优化count函数用来用来统计结果集中不为NULL的数据个数。具体流程是首先存储引擎查询出结果集，然后server层逐个判断结果是否为NULL，不为NULL则加1。 12345 show index from customer; -- 主键是 customer_id ，三个辅助索引：store_id address_id last_name explain select count(first_name) from customer;-- key | Extra 都为NULL count(非索引字段)使用了全表扫描。既需要间接查询非索引字段，又需要判断字段是否为NULL，速度最慢。 12explain select count(last_name) from customer;-- key是idx_last_name， Extra是 Using index count(索引字段)使用了覆盖索引。走辅助索引，查询速度快，但是需要server层逐个判断字段是否为NULL。 但是使用count(非空索引字段)或count(主键)也要server层逐个判断字段是否为NULL，因为MySQL并未对此做出优化。 12explain select count(1) from customer;-- key是idx_fk_store_id ， Extra是 Using index count(1)只扫描索引树，不解析数据行，理论上更快。但server层逐个判断“1是否为NULL”。 12explain select count(*) from customer;-- key是idx_fk_store_id ， Extra是 Using index MyISAM引擎count(*)会直接返回数据库中记录的元数据——数据表行数，但是InnoDB引擎由于支持事务，所以不记录表行数，索引MySQL专门优化了count(*)函数，直接返回索引树中数据的个数，不会送到server层去判断字段是否为NULL。因此，count(*)的速度最快。 ORDER BY优化1select * from `film` where film_id&gt;80 order by title 首先根据where等条件查询，然后将查询结果放入sort_buffer（缓存需要被排序的数据内容），再对中间结果集按照order字段排序，最后回表生成完整的结果集。 第一种优化方法是给where查询字段加索引，改善where查询的速度。 对于sort_buffer，当中间结果表比较小的时候，会直接放入内存中。当中间结果表大于sort_buffer_size时，会放入硬盘中。所以第二种优化方法是当中间结果表比较大的时候，增大sort_buffer_size，使得中间结果表在内存中排序，以提高查询速度。 MySQL为了节省空间，如果数据行字段数大于max_length_for_sort_data，排序的时候只用主键字段和待排序字段生成中间表，排好序之后只需再根据主键回表就可以了。回表也会影响查询效率。第三种优化方法是适当调大max_length_for_sort_data，有时可以提高查询速度。 第四种方法是利用索引覆盖。中间结果表是不存在索引的，如果要利用索引来提高排序速率，就要使用索引覆盖。索引覆盖可以跳过中间结果集，直接输出中间结果。索引覆盖的条件是，输出字段，查询字段和排序字段都要有索引。 1select film_id,title from `film` order by title 上面这个SQL语句满足索引覆盖，如果要加where的话，必须保证where和order by走同一个索引才能满足索引覆盖。 12explain select film_id,title from `film` where title like &#x27;m%&#x27; order by title-- Extra是 Using index 上面这个SQL语句也是满足索引覆盖的。 RAND优化1SELECT title,description FROM `film` ORDER BY RAND() LIMIT 1; rand()函数会输出0到1之间的一个随机数种子。上面SQL语句的含义是从film表中随机选择一行数据，输出其中的title和description字段。 上面SQL语句的实际执行过程是：先创建一个临时表，临时表的字段是rand、title、description。再不断从原表中查询一行，调用RAND()，将结果和数据放入临时表，直到遍历完整个表得到最终临时表。然后针对这个临时表，将rand字段和行位置（主键）放入sort buffer。最后对sort buffer排序，取出第一个的行位置（主键），查询临时表的对应行。 123456-- 解决方案1-- 先新建2个变量，保存film_id的最大值和最小值select max(film_id),min(film_id) into @M,@N from film;-- 随机选取其中一个film_idset @X=floor((@M-@N+1)*rand()+@N);select title,description from film where film_id&gt;=@X limit 1; 上面的解决方案SQL语句太复杂，不易维护，只能作为一个临时的解决方案。 另一个可行的解决方案是通过业务逻辑获取表的总行数total，在total中随机选取一个数字r，并使用下面的SQL语句查询。 12select title,description from film limit r,1-- limit r,1表示从第r个开始选一个 联合索引优化123456789101112CREATE TABLE `inventory_3` ( `inventory_id` mediumint(8) unsigned NOT NULL AUTO_INCREMENT, `film_id` smallint(5) unsigned NOT NULL, `store_id` tinyint(3) unsigned NOT NULL, `last_update` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`inventory_id`), KEY `idx_store_id_film_id` (`store_id`,`film_id`)) ENGINE=InnoDB AUTO_INCREMENT=4582 DEFAULT CHARSET=utf8mb4;-- 把原表的数据插入进来insert into inventory_3 select * from inventory; 123SELECT * FROM `inventory_3` WHERE store_id in (1,2) and film_id=3;exlpain SELECT * FROM `inventory_3` WHERE store_id in (1,2) and film_id=3;-- Extra是 Using index condition 联合索引的排序方式是先对左侧索引排序，对于左侧索引字段值相同的数据行，再对右侧的索引排序，大致情况如下： 当store_id是1和2的时候，film_id的是乱序的，所以无法使用联合索引的排序结果来提升查询效率。但是如果store_id只有一个值的时候，就能用上联合索引的排序结果来提升查询效率。当store_id是1和2的时候，实际执行情况是什么，在MySQL5.6之前，会先走联合索引找到store_id为1和2的页中的主键，然后回表查询film_id，判断是否为3；在MySQL5.6之后，会直接走联合索引找到store_id为1和2的页中对应的film_id，这种操作叫做索引下推。 1SELECT film_ id FROM `inventory_3` WHERE film_id=3; 在MySQL8.0之前，这条SQL语句只能走全表扫描，因为缺少左侧字段而无法使用联合索引。 但是在MySQL8.0之后，对此进行了优化，会使用松散索引扫描的方法。具体是在联合索引表中先判断store_id为1的数据行中有没有film_id=3的，再判断store_id为2的数据行中有没有film_id=3的，以此类推，遍历完整张索引表。 索引失效优化1explain select * from film where film_id + 1 = 100; MySQL中，如果对索引字段做函数操作（film_id + 1）会破坏索引的排序规则，优化器会放弃索引。 1explain SELECT * FROM `rental` WHERE month(rental_date) = 5; 使用month函数后，无法使用索引。 解决方法如下： 12SELECT * FROM `rental` WHERE rental_date BETWEEN&#x27;2005-5-1&#x27;AND &#x27;2005-6-1&#x27;OR rental_date BETWEEN &#x27;2006-5-1&#x27;AND &#x27;2006-6-1&#x27;; 1234select * from t1 where f1=6;-- f1字段的类型为VARCHAR-- 就相当于select * from tl where CAST（f1 AS signed int）= 6; MySQL中若出现字符串与数字比较，会将字符串转换为数字。f1字段若为索引，因为进行了函数计算则索引失效。 1234select t2.*from t1，t2 where t1.f1=t2.f1 and tl.f2=6;-- t1表使用utf8mb4编码，t2表使用utf8编码。f1都是varchar类型，f2都是int类型-- 就相当于select t2.*from t1，t2 where t1.f1=CONVERT（t2.f1 USING utf8mb4）and tl.f2=6; MySQL中，utf8与utf8mb4字段比较时，会把utf8转为utf8mb4，所以在查询t1.f1=t2.f1时因为进行了函数计算则索引失效。 解决方法：将查询条件转换为索引字段的编码 1select t2.* from t1，t2 where CONVERT（t1.f1 USING utf8）= t2.f1 and t1.f2=6； 根据t1.f2索引查询的t1.f1转换为utf8再去和t2.f1比较。 1SELECT film_id,title,description FROM film ORDER BY title LIMIT 900,10; 上面SQL语句的执行顺序是先根据title对整个表排序，然后从第900个开始取出10条数据。排序偏移量大，并且会丢弃大量无用数据。 因此若要提升查询的速度，需要让它走索引覆盖。 12show index from film;-- 主键是film_id，辅助索引是title 因为description不是辅助索引，所以走不了索引覆盖。所以优化思路是先利用索引覆盖得到所需数据的主键，再利用得到的主键查询数据。 123-- 优化方案SELECT f.film_id,f.title,f.description FROM `film` f INNER JOIN (SELECT film_id FROM `film` ORDER BY title LIMIT 900,10) m ON f.film_id=m.film_id;-- INNER JOIN和JOIN是相同的","categories":[{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://wht6.github.io/tags/MySQL/"}]},{"title":"Git&GitLab原理及部署","slug":"GitLab原理及部署","date":"2022-05-24T06:00:00.000Z","updated":"2022-08-17T01:54:18.349Z","comments":true,"path":"posts/8a74.html","link":"","permalink":"http://wht6.github.io/posts/8a74.html","excerpt":"","text":"GitLab介绍GitGit 是一种在全球范围都广受欢迎的 分布式版本控制系统。 版本控制（Revision control）是一种在开发的过程中用于管理我们对文件、目录或工程等内容的修改历史，方便查看更改历史记录，备份以便恢复以前的版本的软件工程技术。简单说就是用于管理多人协同开发项目的技术。 版本控制工具具备的功能： 协同修改 多人并行不悖的修改服务器端的同一个文件。 数据备份 不仅保存目录和文件的当前状态，还能够保存每一个提交过的历史状态。 版本管理 在保存每一个版本的文件信息的时候要做到不保存重复数据，以节约存储空间，提高运行效率。这方面 SVN 采用的是增量式管理的方式，而 Git 采取了文件系统快照的方式。 权限控制 对团队中参与开发的人员进行权限控制。 对团队外开发者贡献的代码进行审核 【Git 独有】。 历史记录 查看修改人、修改时间、修改内容、日志信息。 将本地文件恢复到某一个历史状态。 分支管理 允许开发团队在工作过程中多条生产线同时推进任务，进一步提高效率。 版本控制工具可分为两类，一类是以SVN为代表的集中式版本控制工具，这类工具的工作原理是：它有一个中央服务器，所有开发人员都需要从服务器中将代码下载到本地，然后在本地进行开发，开发完成后再提交到服务器。所以集中式版本控制工具的最大缺点是它无法脱离服务器，如果服务器损坏，所有文件都将丢失。 另一类是以Git为代表的分布式版本控制工具，这类工具对于每个开发人员，它都能够直接在本地进行版本控制，也就是说，本地就有完整的提交历史，即一个完整的版本库。该类工具没有中央服务器的限制，大大减小了文件丢失的可能性。 Git安装： 1、安装到一个没有中文和没有空格的目录 2、Git的默认编辑器：建议使用Vim编辑器 参考链接文章：https://www.cnblogs.com/chenmingjun/p/10160410.html GitLabGitlab 是一个基于Git实现的在线代码仓库托管软件，你可以用Gitlab自己搭建一个类似于Github一样的系统平台，一般搭建Gitlab私服就是用在公司的内部。 GitHub、GitLab都是基于git开发的管理代码的软件。GitHub是公开的在线平台，是免费的代码托管中心，主要用于管理公共仓库。在局域网内，我们也可以通过GitLab自己搭建一个托管中心，用于管理私人仓库。 GitLab提供项目管理、wiki 文档、代码托管、issue跟踪、持续集成/交付等功能。 相关概念： Group（群组）：Group 是一组 Project 的集合，形式类似于文件夹。但其与 GitHub 中的 Organization 是不同的，因为 Organization 的再下一级只能是 Repository，也就是 GitLab 中的 Project，而 Group 下面是允许存在 Sub Group 的，这更符合企业较复杂的组织架构，权限也更好分配。 Project（项目）：Project 是 GitLab 的核心组件，是最基础的模块，除了保存代码以外，还是管理、交付和协作的核心构建块。 Issue（议题）：Issue 是 Project 的一部分，它也是整个 DevOps 工作流的起始。在 Issue 中可以记录需求、讨论实现、估算工作量、跟踪项目进度、分配工作等，通过不同的标签（label）来管理整个进度。注意：GitLab 中的 Issue 是有 Group 和 Project 两个维度的，可以跟踪不同维度的流程。 Merge Request（MR，合并请求）：MR 连接 Issue 和实际代码的变化，包括：设计、实现细节（code change）、讨论（code review）、审批、测试（CI Pipeline）和安全扫描，是 DevOps 流程中非常重要的一环。 Label（标签）：用于标记和追踪 Project 或 Group 的工作，并将 Issue 与不同的 initiatives 联系起来。 Epic（史诗）：一组拥有相同主题为位于不同 Group 和 Project 的相关 Issue。 Board（面板）：Project 和 Issue 的可视化列表、有助于管理积压的团队工作，确定项目优先级，并将 Issue 移动到 Group 或 Project 的特定阶段。 Milestone（里程碑）：一个 Sprint 或（多个）deliverable，帮助你把代码、Issue 和 MR 组织成一个整体。 Roadmap（路线图）：将各种 Epic 进行可视化展示，以非常清晰状态来展现所有 Epic 的状态和进度。 Git原理Git协作流程本地库与远程库的交互操作，可分为两种情况：团队内部协作和跨团队协作。团队内部协作先说说团队内容协作，假设有一个程序员张三，它手下有一个员工李四，这两个人如何进行团队协同开发呢？ 首先张三在自己本地进行开发，将项目的基本结构搭建好，此时张三将本地库的代码推送到远程库： 这时候托管中心就有了张三推送上去的远程库，此时李四只需要将远程库的代码下载到本地即可： 这样李四也在本地进行开发，开发完成后将本地库推送到远程库即可： 李四对远程库进行了修改之后，张三要想获得李四的修改内容，就需要对远程库进行一个拉取的操作，将远程库的内容拉取到本地库： 这样两个人便实现了协同开发，注意这里的李四并不能直接将本地库的代码推送到远程库，因为这个远程库是张三创建的，李四要想推送代码，就必须加入到张三的项目团队中。 跨团队协作再假设一种情况，有一个程序员老汪正在进行项目开发，发现自己遇到了一个难题，于是它找到了以前的师傅老魏，请求它帮忙，但是老魏不是这个项目团队的人，此时老汪将自己本地的代码推送到了远程库： 老魏要想参与到项目中，就需要将老汪推送的远程库作一个复制的操作： 此时代码托管中心就有了两份一模一样的远程库，只不过一份是老汪的，一份是老魏的，接着老魏将自己远程库的代码克隆到本地库： 老魏就可以进行开发了，问题解决后，老魏将本地库的代码推送到自己的远程库： 但这时候改变的仅仅是老魏的远程库，老汪的远程库并没有被修改，这时候老魏需要发起一个pull request，发起请求后，老汪就会收到老魏的请求和代码，经过老汪审核后，就可以合并到自己的远程库中： 此时老汪再对远程库进行拉取操作，就可以将老魏修改的代码获取到本地了。 Git的本地结构 Git相关机制数据完整性：文件在网络传输的过程中可能导致文件损坏，Git使用哈希加密算法（SHA-1算法）来计算文件的哈希值，通过比较传输前后加密结果是否一致来保证数据的完整性。 文件管理：Git把数据看作是小型文件系统的一组快照。每次提交更新时Git都会对当前的全部文件制作一个快照并保存这个快照的索引。为了高效，如果文件没有修改，Git不再重新存储该文件，而是只保留一个链接指向之前存储的文件。所以Git的工作方式可以称之为快照流。 GitFlow工作流Gitflow工作流通过为功能开发、发布准备和维护设立了独立的分支，让发布迭代过程更流畅。严格的分支模型也为大型项目提供了一些非常必要的结构。 主干分支master：主要负责管理正在运行的生产环境代码。永远保持与正在运行的生产环境完全一致。 开发分支develop：主要负责管理正在开发过程中的代码。一般情况下应该是最新的代码。 bug修理分支hotfix：主要负责管理生产环境下出现的紧急修复的代码。从主干分支分出，修理完毕并测试上线后，并回主干分支。并回后，视情况可以删除该分支。 准生产分支（预发布分支）release：较大的版本上线前，会从开发分支中分出准生产分支，进行最后阶段的集成测试。该版本上线后，会合并到主干分支。生产环境运行一段阶段较稳定后可以视情况删除。 功能分支feature：为了不影响较短周期的开发工作，一般把中长期开发模块，会从开发分支中独立出来。开发完成后会合并到开发分支。 GitLab原理GitLab服务构成分别是： Nginx：静态Web服务器 gitlab-shell：用于处理Git命令和修改authorized keys列表 gitlab-workhorse：轻量级的反向代理服务器(这个是个敏捷的反向代理，它会处理一些大的HTTP请求，比如文件的上传下载，其他的请求会反向代理给Gitlab Rails应用) logrotate：日志文件管理工具 postgresql：数据库 redis：缓存数据库 sidekiq：用于在后台执行队列的任务 unicorn：Gitlab Rails应用是托管在这个服务器上面的 GitLab Workhorse是一个敏捷的反向代理。它会处理一些大的HTTP请求，比如文件上传、文件下载、Git push/pull和Git包下载。其它请求会反向代理到GitLab Rails应用，即反向代理给后端的unicorn。 GitLab Server的两种访问方式1、SSH访问 当通过SSH访问GitLab Server时，GitLab Shell会： 限制执行预定义好的Git命令（git push, git pull, git annex） 调用GitLab Rails API 检查权限 执行pre-receive钩子（在GitLab企业版中叫做Git钩子） 执行你请求的动作 处理GitLab的post-receive动作 处理自定义的post-receive动作 2、http(s)访问 当通过http(s)访问GitLab Server时，工作流程取决于你是从Git仓库拉取(pull)代码还是向git仓库推送(push)代码。 如果你是从Git仓库拉取(pull)代码，GitLab Rails应用会全权负责处理用户鉴权和执行Git命令的工作； 如果你是向Git仓库推送(push)代码，GitLab Rails应用既不会进行用户鉴权也不会执行Git命令，它会把以下工作交由GitLab Shell进行处理： 调用GitLab Rails API 检查权限 执行pre-receive钩子（在GitLab企业版中叫做Git钩子） 执行你请求的动作 处理GitLab的post-receive动作 处理自定义的post-receive动作 项目组织结构使用 GitLab 进行项目管理，首先需要了解的就是如何合理的组织项目，不同于 GitHub 中的 organization 的下一级只能是 repo，GitLab 中的 Group 可以有 Sub Group 也就是子组的存在，这对于企业的组织架构来说更加灵活，可以非常方便的展示组织和项目之间的从属关系，和更精细的权限管理，再配合 Epic 和 Roadmap 清晰的了解项目当前的进度。 Workflow 最佳实践GitLab 推荐使用 Issue 并配合 Label 完成整个 DevOps 工作流，在体验上与 GitHub 上的操作类似，但在企业内部团队协作方面 GitLab 做的更加精细。以 Issue 为起点，通过添加和删除不同 Label 进行协作，不同的 Label 可以代表不同的团队、阶段、环境以及一些特定需求（如需要技术文档团队或营销团队接入）；在不同阶段不同的团队介入开发，完成后提交 MR（合并请求）并运行 CI Pipeline 和 review，通过不同环境的 CI 直到最终审核通过；接下来就是合并触发 CD Pipeline 完成发布并关闭 Issue。之后是监控和分析的接入，然后开启下一轮的 DevOps 工作流。 Git命令 git init：本地仓库初始化。（以当前路径为本地仓库的路径，自动新建一个.git目录）【.git目录中存放的是本地库相关的子目录和文件，不要删除，也不要胡乱修改。 】 设置签名 形式：【用户名：tom Email地址：goodMorning@atguigu.com】 作用：区分不同开发人员的身份。 辨析：这里设置的签名和登录远程库(代码托管中心)的账号、密码没有任何关系。 命令： 123# 项目级别/仓库级别：仅在当前本地库范围内有效git config user.name tom_progit config user.email goodMorning_pro@atguigu.com 设置的信息保存位置：./.git/config文件 123# 系统用户级别：登录当前操作系统的用户范围git config --global user.name tom_glbgit config --global user.email goodMorning_glb@atguigu.com 设置的信息保存位置：~/.gitconfig文件 级别优先级： 【就近原则：项目级别优先于系统用户级别，二者都有时采用项目级别的签名。】 如果只有系统用户级别的签名，就以系统用户级别的签名为准。 二者都没有不允许。 实际开发中我们设置的是系统用户级别较多。 git status ：查看工作区、暂存区状态。 git add [filename] ：将工作区的“新建/修改”添加到暂存区；git rm --cached [filename]，将文件从暂存区移除。 git commit -m &quot;commit message&quot; [filename] ：将暂存区的内容提交到本地仓库。 查看历史记录 git log ：多屏显示控制方式： 空格向下翻页 ，b 向上翻页 ，q 退出。 git log --pretty=oneline：以漂亮的格式显示单行历史记录。 git log --oneline：以简洁的格式显示单行历史记录。 git reflog：以指针的格式显示单行历史记录。HEAD@&#123;数字&#125;表示 移动到当前版本需要多少步。 版本的前进和后退 基于索引值操作(推荐使用)： git reset --hard [局部索引值] ，如git reset --hard c433284。 使用^符号：只能后退，git reset --hard HEAD^。【注：一个^表示后退一步，n个^表示后退n步】 使用~符号：只能后退，git reset --hard HEAD~n ，表示后退n步。 git reset的--hard选项在重置本地库版本的同时，也会重置暂存区（index file）和工作区（working tree），--mixed选项只重置本地库和暂存区而工作区不变，--soft选项只重置本地库而暂存区和工作区都保持不变。 删除文件并找回 git reset --hard [指针位置] ，如果删除操作已经提交到本地库：指针位置为局部索引值，如果删除操作尚未提交到本地库但是已经提交到本地暂存区：指针位置为HEAD。 【注】任何一个已经提交的版本操作，就会在本地版本库中有一个确定的记录，记录着该文件的操作，即便我们做的是提交删除的操作，那么该记录也是不可磨灭的。 Git只会增加版本，而不会把任何一个版本删除。 本地库 == 本地仓库 == 本地版本库。 比较文件差异 git diff [文件名] ：将工作区中的文件和暂存区中的文件进行比较。 git diff [本地库中某一历史版本] [文件名] ：将工作区中的文件和本地库历史记录进行比较。 分支管理 分支：在版本控制过程中，使用多条线同时推进多个任务。 分支的好处：1、同时并行推进多个功能开发，提高开发效率。 2、各个分支在开发过程中，如果某一个分支开发失败，不会对其他分支有任何影响。失败的分支删除重新开始即可。 创建分支：git branch [分支名]，查看分支：git branch -v，切换分支：git checkout [分支名]。 合并分支：第一步：切换到接受修改的分支（即被合并，增加新内容的分支）上，git checkout [被合并的分支名] ，第二步：执行merge命令，git merge [有新内容的分支名]，即合并其他分支到当前分支。 GitLab命令1234567891011121314151617181920212223gitlab-ctl start # 启动所有 gitlab 组件服务；gitlab-ctl stop # 停止所有 gitlab 组件服务；gitlab-ctl restart # 重启所有 gitlab 组件；gitlab-ctl status # 查看服务状态；gitlab-ctl reconfigure # 重新加载配置；vim /etc/gitlab/gitlab.rb # 修改默认的配置文件；gitlab-rake gitlab:check SANITIZE=true --trace # 检查gitlab；gitlab-ctl tail # 查看日志； GitLab部署环境：Centos 7 x86-64，软件版本：gitlab-ce 12.4.0。 首先下载rpm安装包。清华镜像 1234567891011121314151617# 安装依赖yum install -y curl policycoreutils openssh-server openssh-clientssystemctl start sshdsystemctl enable sshd# 安装Postfix邮箱，用邮箱地址记录用户信息yum install postfixsystemctl enable postfixvim /etc/postfix/main.cf # inet_interfaces = allsystemctl start postfix# 安装GitLabmkdir -p /service/toolscd /service/toolswget https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/gitlab-ce-12.4.0-ce.0.el7.x86_64.rpm --no-check-certificate# 安装yum install -y gitlab-ce-12.4.0-ce.0.el7.x86_64.rpm 修改配置文件中的url地址为本地服务器的地址。 123# vim /etc/gitlab/gitlab.rb# external_url &#x27;http://192.168.91.129&#x27;gitlab-ctl reconfigure # 重新加载配置，主要是启动一个LAMP环境 在GitLab启动后，可能会出现端口被其他程序占用的情况，此时需要我们手动修改默认端口。 12345678vim /etc/gitlab/gitlab.rb# 更改下面两个端口unicorn[&#x27;port&#x27;] = 8088nginx[&#x27;listen_port&#x27;] = 9900## 重新加载配置并重启gitlab-ctl reconfiguregitlab-ctl restart 由于我是在虚拟机中运行GitLab，内存分配的太少，因此还需要手动关闭无用的程序。 12gitlab-ctl stop pumagitlab-ctl stop sidekiq 在浏览器中输入，http://192.168.91.129:9900，打开web界面。 设置密码。用户名root登录。创建project，设定项目名称，选择pubic。 1234yum install git -y # 安装gitssh-keygencat .ssh/id_rsa.pub# 在web界面中，用户，设置，sshkeys，把公钥粘贴进去 测试。 12345678910git config --global user.name &quot;root&quot; git config --global user.email &quot;root@localhost&quot; git clone git@192.168.91.129:root/test.gitcd testtouch README.mdgit init #初始化git remote add origin git@localhost:root/test.git # 添加远程库git add README.md # 将文件加入到索引中git commit -m &quot;add README&quot; # 将文件提交到本地仓库git push -u origin master # 将文件同步到GitLab服务器上 参考资料： https://root-shady.gitbooks.io/git-gitlab/content/ https://cloud.tencent.com/developer/article/1884044 https://cloud.tencent.com/developer/article/1835955 https://blog.nowcoder.net/n/70547f11cb0247f98db672e62fcd483e https://www.cnblogs.com/jojoword/p/11147828.html https://help.aliyun.com/document_detail/52857.html https://cloud.tencent.com/developer/article/1388538","categories":[{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"GitLab","slug":"GitLab","permalink":"http://wht6.github.io/tags/GitLab/"},{"name":"DevOps","slug":"DevOps","permalink":"http://wht6.github.io/tags/DevOps/"}]},{"title":"Linux进程间通信IPC","slug":"Linux进程间通信IPC","date":"2022-05-21T08:00:00.000Z","updated":"2022-10-08T07:52:34.439Z","comments":true,"path":"posts/92f1.html","link":"","permalink":"http://wht6.github.io/posts/92f1.html","excerpt":"","text":"每个进程都拥有自己的用户地址空间，任何一个进程的全局变量在另一个进程中完全不可见，但是内核空间中每个进程都是共享的，所以进程之间要交换数据必须通过内核空间进行。进程间通信(IPC, InterProcess Communication)。 进程间通信机制共有七种，分别是匿名管道、命名管道、消息队列、共享内存、信号量、信号以及Scocket。各有不同的优缺点。 管道 匿名管道 管道的本质是一块内核缓冲区，由两个文件描述符引用，一个表示读端，一个表示写端，只能以先进先出的方式存取数据。管道是半双工，规定数据从管道的写端流入管道，从读端流出，当两个进程都终结的时候，管道也自动消失，管道的读端和写端默认都是阻塞的。 管道的数据一旦被读走，便不在管道中存在，不可反复读取，数据只能在一个方向上流动，若要实现双向流动，必须使用两个管道。 在Linux中，匿名管道一个常见的表现形式就是「|」这个竖线了，如常见的操作 1ps -ef | grep redis 对于匿名管道，它的通信范围是存在亲缘关系 的进程。因为管道没有实体，也就是没有管道文件，只能通过 fork 来复制父进程 fd 文件描述符，来达到通信的目的。 命名管道 匿名管道，由于没有名字，只能用于父子进程间的通信。为了克服这个缺陷，命名管道(FIFO)应运而生。命名管道与匿名管道之间的区别在于，命令管道提供了一个路径名与之相关联，从而以文件的形式存在于文件系统中。这样，即使与创建命名管道进程不存在父子关系的进程，只要可以访问该路径，就能够彼此通过 FIFO 相互通信，通过 FIFO 不相关的进程也能交换数据。 FIFO是Linux基础文件类型中的一种，文件类型为p FIFO文件在磁盘上没有数据块，仅仅用来标识内核中一条通道，进程打开这个文件进行read/write，实际上也是在读写内核缓冲区。 管道的缺点是半双工通信，通信效率低不适合进程间频繁交换数据，且缓冲区的大小有限，只能传输无格式的字节流。 消息队列 管道存取数据需要同时进行，否则就会阻塞。而使用消息队列进行数据传输时，A进程只需要把数据放到相应的消息队列即可返回；B进程在需要的时候只需要去消息队列中读取相关数据即可。同理，B进程给A进程发送消息也是如此。 消息队列是保存在内核中的消息链表，在发送数据时，会分成一个一个独立的数据单元，也就是消息体（数据块），消息体是用户自定义的数据类型，消息的发送方和接收方要约定好消息体的数据类型，所以每个消息体都是固定大小的存储块，不像管道是无格式的字节流数据。如果进程从消息队列中读取了消息体，内核就会把这个消息体删除。 消息队列生命周期随内核，如果没有释放消息队列或者没有关闭操作系统，消息队列会一直存在，而前面提到的匿名管道的生命周期，是随进程的创建而建立，随进程的结束而销毁。 消息队列的缺点是 1）不适合比较大数据的传输，因为在内核中每个消息体都有一个最大长度的限制，同时所有队列所包含的全部消息体的总长度也是有上限。 2）存在用户态与内核态之间的数据拷贝开销，因为进程写入数据到内核中的消息队列时，会发生从用户态拷贝数据到内核态的过程，同理另一进程读取内核中的消息数据时，会发生从内核态拷贝数据到用户态的过程。 共享内存 共享内存可以使得多个进程可以直接读写在同一块物理内存空间中，这是效率最高的进程间通信方式。 现代操作系统中，对于内存管理，采用的是虚拟内存技术，也就是每个进程都有自己的独立虚拟空间。不同进程的虚拟内存映射到不同的物理内存。所以，即使进程 A 和 进程 B 的虚拟地址是一样的，其实访问的是不同的物理内存地址，对于数据的增删查改互不影响。 共享内存的机制，就是各拿出一块虚拟地址空间来，映射到相同的物理内存中。这样这个进程写入的东西，另外一个进程马上就能看到了，都不需要拷贝来拷贝去，传来传去，大大提高了进程间通信的速度。 共享内存对比其他几种方式是效率最高的，因为无需进行多次复制，直接对内存操作，不过要注意一点，操作系统并不保证任何并发问题，我们需要借助信号量或其他方式来完成同步。 信号量 用了共享内存通信方式，带来新的问题，那就是如果多个进程同时修改同一个共享内存，很有可能就冲突了。例如两个进程都同时写一个地址，那先写的那个进程会发现内容被别人覆盖了。 为了防止多进程竞争共享资源，而造成的数据错乱，所以需要保护机制，使得共享的资源，在任意时刻只能被一个进程访问。正好，信号量就实现了这一保护机制。 信号量其实是一个计数器，主要用于实现进程间的互斥与同步，而不是用于存储进程间通信数据。 信号量表示资源的数量，控制信号量的方式有两种原子操作： 一个是 P 操作，这个操作会把信号量减去 -1，相减后如果信号量 &lt; 0，则表明资源已被占用，进程需阻塞等待；相减后如果信号量 &gt;= 0，则表明还有资源可使用，进程可正常继续执行。 另一个是 V 操作，这个操作会把信号量加上 1，相加后如果信号量 &lt;= 0，则表明当前有阻塞中的进程，于是会将该进程唤醒运行；相加后如果信号量 &gt; 0，则表明当前没有阻塞中的进程； P 操作是用在进入共享资源之前，V 操作是用在离开共享资源之后，这两个操作是必须成对出现的。 例如，你有 100 元钱，就可以将信号量设置为 100。其中 A 向你借 80 元，就会调用 P 操作，申请减去 80。如果同时 B 向你借 50 元，但是 B 的 P 操作比 A 晚，那就没有办法，只好等待 A 归还钱的时候，B 的 P 操作才能成功。之后，A 调用 V 操作，申请加上 30 元，也就是还给你 30 元，这个时候信号量有 50 元了，这时候 B 的 P 操作才能成功，才能借走这 50 元。 所谓原子操作（Atomic Operation），就是任何一块钱，都只能通过 P 操作借给一个人，不能同时借给两个人。也就是说，当 A 的 P 操作（借 80）和 B 的 P 操作（借 50），几乎同时到达的时候，不能因为大家都看到账户里有 100 就都成功，必须分个先来后到。 信号 上面说的进程间通信，都是常规状态下的工作模式。对于异常情况下的工作模式，就需要用「信号」的方式来通知进程。信号可以在应用进程和内核之间直接交互，内核也可以利用信号来通知用户空间的进程发生了哪些系统事件。 在Linux操作系统中，为了响应各种各样的事件，提供了很多种信号，分别代表不同的含义，如常见的SIGINT信号，表示终止该进程；SIGTSTP信号，表示停止该进程，但还未结束。 信号是进程间通信唯一的异步通信机制，因为可以在任何时候发送信号给某一个进程。一旦接收到信号，用户有以下三种处理方式： 执行默认操作。Linux 对每种信号都规定了默认操作，例如，上面列表中的 SIGTERM 信号，就是终止进程的意思。Core 的意思是 Core Dump，也即终止进程后，通过 Core Dump 将当前进程的运行状态保存在文件里面，方便程序员事后进行分析问题在哪里。 捕捉信号。可以为信号定义一个信号处理函数。当进程接收信号时，执行相应的信号处理函数； 忽略信号。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。有两个信号是应用进程无法捕捉和忽略的，即 SIGKILL 和 SEGSTOP，它们用于在任何时候中断或结束某一进程。 socket 前面提到的管道、消息队列、共享内存、信号量和信号都是在同一台主机上进行进程间通信，那要想跨网络与不同主机上的进程之间通信，就需要 Socket 通信了。实际上，Socket 通信不仅可以跨网络与不同主机的进程间通信，还可以在同主机上进程间通信。 我们来看看创建 socket 的系统调用： 1int socket(int domain, int type, int protocal) 三个参数分别代表： domain 参数用来指定协议族，比如 AF_INET 用于 IPV4、AF_INET6 用于 IPV6、AF_LOCAL/AF_UNIX 用于本机； type 参数用来指定通信特性，比如 SOCK_STREAM 表示的是字节流，对应 TCP、SOCK_DGRAM 表示的是数据报，对应 UDP、SOCK_RAW 表示的是原始套接字； protocal 参数原本是用来指定通信协议的，但现在基本废弃。因为协议已经通过前面两个参数指定完成，protocol 目前一般写成 0 即可； 根据创建 socket 类型的不同，通信的方式也就不同： 实现 TCP 字节流通信：socket 类型是 AF_INET 和 SOCK_STREAM； 实现 UDP 数据报通信：socket 类型是 AF_INET 和 SOCK_DGRAM； 实现本地进程间通信：「本地字节流 socket 」类型是 AF_LOCAL 和 SOCK_STREAM，「本地数据报 socket 」类型是 AF_LOCAL 和 SOCK_DGRAM。另外，AF_UNIX 和 AF_LOCAL 是等价的，所以 AF_UNIX 也属于本地 socket； 接下来，简单说一下这三种通信的编程模式。 1) 针对 TCP 协议通信的 socket 编程模型 服务端和客户端初始化 socket，得到文件描述符； 服务端调用 bind，将绑定在 IP 地址和端口; 服务端调用 listen，进行监听； 服务端调用 accept，等待客户端连接； 客户端调用 connect，向服务器端的地址和端口发起连接请求； 服务端 accept 返回用于传输的 socket 的文件描述符； 客户端调用 write 写入数据；服务端调用 read 读取数据； 客户端断开连接时，会调用 close，那么服务端 read 读取数据的时候，就会读取到了 EOF，待处理完数据后，服务端调用 close，表示连接关闭。 这里需要注意的是，服务端调用 accept 时，连接成功了会返回一个已完成连接的 socket，后续用来传输数据。 所以，监听的 socket 和真正用来传送数据的 socket，是「两个」 socket，一个叫作监听 socket，一个叫作已完成连接 socket。 成功连接建立之后，双方开始通过 read 和 write 函数来读写数据，就像往一个文件流里面写东西一样。 2) 针对 UDP 协议通信的 socket 编程模型 UDP 是没有连接的，所以不需要三次握手，也就不需要像 TCP 调用 listen 和 connect，但是 UDP 的交互仍然需要 IP 地址和端口号，因此也需要 bind。 对于 UDP 来说，不需要要维护连接，那么也就没有所谓的发送方和接收方，甚至都不存在客户端和服务端的概念，只要有一个 socket 多台机器就可以任意通信，因此每一个 UDP 的 socket 都需要 bind。 另外，每次通信时，调用 sendto 和 recvfrom，都要传入目标主机的 IP 地址和端口。 3) 针对本地进程间通信的 socket 编程模型 本地 socket 被用于在同一台主机上进程间通信的场景： 本地 socket 的编程接口和 IPv4 、IPv6 套接字编程接口是一致的，可以支持「字节流」和「数据报」两种协议； 本地 socket 的实现效率大大高于 IPv4 和 IPv6 的字节流、数据报 socket 实现； 对于本地字节流 socket，其 socket 类型是 AF_LOCAL 和 SOCK_STREAM。 对于本地数据报 socket，其 socket 类型是 AF_LOCAL 和 SOCK_DGRAM。 本地字节流 socket 和 本地数据报 socket 在 bind 的时候，不像 TCP 和 UDP 要绑定 IP 地址和端口，而是绑定一个本地文件，这也就是它们之间的最大区别。 参考链接： https://segmentfault.com/a/1190000040526944 https://juejin.cn/post/6844903845542232077 https://segmentfault.com/a/1190000040556872 https://www.jianshu.com/p/3ecf20200880","categories":[{"name":"Linux","slug":"Linux","permalink":"http://wht6.github.io/categories/Linux/"}],"tags":[{"name":"进程","slug":"进程","permalink":"http://wht6.github.io/tags/%E8%BF%9B%E7%A8%8B/"}]},{"title":"Docker创建深度学习环境","slug":"Docker创建深度学习环境","date":"2022-05-15T08:00:00.000Z","updated":"2022-08-10T12:41:56.603Z","comments":true,"path":"posts/a665.html","link":"","permalink":"http://wht6.github.io/posts/a665.html","excerpt":"","text":"这里使用的是Ubuntu的环境，并且机器上已经事先安装的显卡驱动。 安装Docker123456789101112131415161718192021222324252627282930313233# 删除dockerapt-get remove docker*apt-get remove --auto-remove docker# 更新软件列表apt update# 安装必要依赖apt install apt-transport-https ca-certificates curl gnupg-agent software-properties-common# 添加docker官方的gpg keycurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -# 添加docker官方的源add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot;# 安装Dockerapt updateapt install docker-ce docker-ce-cli containerd.io# 配置阿里的Docker镜像源cat &gt;&gt; /etc/docker/daemon.json &lt;&lt;EOF&#123;&quot;registry-mirrors&quot;: [&quot;https://xxx.mirror.aliyuncs.com&quot;]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker#安装nvidiaDockerdistribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\ &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.listapt-get updateapt-get install -y nvidia-docker2systemctl restart docker# 测试nvidia-dockerdocker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi 从docker 19版本之后，nvidia-docker成为了过去式。不需要单独去下nvidia-docker这个独立的docker应用程序，也就是说gpu docker所需要的Runtime被集成进docker中，使用的时候用—gpus参数来控制。 下载镜像可选择pytorch的官方镜像，也可选择nvidia的官方镜像。这里我选择的是nvidia的Ubuntu镜像。 dockerhub 1234# pytorch镜像docker pull pytorch/pytorch:1.9.0-cuda11.1-cudnn8-devel# nvidia镜像docker pull nvidia/cuda:11.3.0-cudnn8-devel-ubuntu18.04 build镜像准备Dockerfile新建Dockerfile，将build过程中使用的文件（Anaconda3-2021.05-Linux-x86_64.sh和sources.list）和Dockerfile放在同一个目录下。 apt源 anaconda 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162mkdir docker_filevim Dockerfile# 写入FROM nvidia/cuda:11.3.0-cudnn8-devel-ubuntu18.04ARG DEBIAN_FRONTEND=noninteractiveRUN mv /etc/apt/sources.list /etc/apt/sources.list.bakCOPY sources.list /etc/apt/sources.listRUN chmod a+x /etc/apt/sources.listRUN apt-get update &amp;&amp; apt-get install --assume-yes apt-utilsRUN apt-get install -y --no-install-recommends libglib2.0-0RUN apt-get install -y --no-install-recommends htop byobu gcc libsm6 libxext6 libxrender-dev lsb-coreRUN apt-get install -y --no-install-recommends \\ bzip2 \\ g++ \\ git \\ graphviz \\ libgl1-mesa-glx \\ libhdf5-dev \\ openmpi-bin \\ vim \\ libopencv-dev \\ libsnappy-dev \\ python-dev \\ python-pip \\ build-essential \\ wget &amp;&amp; \\ rm -rf /var/lib/apt/lists/*ADD Anaconda3-2021.05-Linux-x86_64.sh /root/anaconda.shRUN cd /root &amp;&amp; bash anaconda.sh -b -p ./anaconda3RUN bash -c &quot;source /root/anaconda3/etc/profile.d/conda.sh &amp;&amp; conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/ &amp;&amp; conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/ &amp;&amp; conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/ &amp;&amp; conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro/ &amp;&amp; conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ &amp;&amp; conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ &amp;&amp; conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r &amp;&amp; conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 &amp;&amp; conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ &amp;&amp; conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ &amp;&amp; conda config --remove channels defaults &amp;&amp; conda config --set show_channel_urls yes &amp;&amp; conda config --set ssl_verify false&quot;RUN ln -s /root/anaconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh &amp;&amp; \\ echo &quot;. /etc/profile.d/conda.sh &amp;&amp; conda activate base&quot; &gt;&gt; ~/.bashrcENV PATH /root/anaconda3/bin:$PATHRUN bash -c &quot;. /etc/profile.d/conda.sh &amp;&amp; conda activate base&quot;ENV LANG C.UTF-8WORKDIR /rootRUN mkdir workspaceWORKDIR workspaceRUN bash -c &quot;mkdir .pip &amp;&amp; touch ./.pip/pip.conf&quot;RUN echo &quot;[global]&quot; &gt;&gt; /root/workspace/.pip/pip.conf &amp;&amp; \\ echo &quot;index-url = https://pypi.tuna.tsinghua.edu.cn/simple&quot; &gt;&gt; /root/workspace/.pip/pip.confCMD [&quot;&quot;] ARG DEBIAN_FRONTEND=noninteractive，ARG 定义的变量只会存在于镜像构建过程，启动容器后并不保留这些变量，与之相反，ENV 定义的变量在启动容器后仍然保留。DEBIAN_FRONTEND=noninteractive表示无需向用户请求输入，所有操作都是非交互式的。 ENV PATH /root/anaconda3/bin:$PATH，设置环境变量。 build镜像12345678910111213141516171819## 构建docker build -t cuda11.3-conda-ubuntu18 .## 备份镜像docker save -o xxx.tar [镜像ID]## 删除所有构建过程中产生的游离镜像，如果存在子镜像问题，父镜像也要删除# 先删除所有无关容器（看情况）docker rm -f $(docker ps -aq)# 删除游离docker rmi $(docker images -q -f dangling=true)# 删除none镜像docker rmi $(docker images -a | grep &quot;none&quot; | awk &#x27;&#123;print $3&#125;&#x27;)# 无法删除，查找父镜像docker image inspect --format=&#x27;&#123;&#123;.RepoTags&#125;&#125; &#123;&#123;.Id&#125;&#125; &#123;&#123;.Parent&#125;&#125;&#x27; $(docker image ls -q --filter since=[none镜像ID])# 删除父镜像docker rmi -f [镜像ID]# 导入备份镜像docker load -i xxx.tar# 给镜像重命名docker tag [镜像ID] xxx:xxx 启动容器和测试启动容器 1docker run -it --privileged=true --name env1 --gpus=all --shm-size 15G -v /home/wanght/workspace:/root/workspace:rw [镜像ID] /bin/bash 测试： 123456# 查看显卡驱动nvidia-smi# 查看CUDA环境nvcc -V# 查看cudnn包cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR -A 2 测试python版本： 123import sys print (&quot;Python Version &#123;&#125;&quot;.format(str(sys.version).replace(&#x27;\\n&#x27;, &#x27;&#x27;))) 测试pytorch和cuda 12345678910111213141516171819202122import timeimport torchif __name__ == &#x27;__main__&#x27;: print(&#x27;torch版本：&#x27;+torch.__version__) print(&#x27;cuda是否可用：&#x27;+str(torch.cuda.is_available())) print(&#x27;cuda版本：&#x27;+str(torch.version.cuda)) print(&#x27;cuda数量:&#x27;+str(torch.cuda.device_count())) print(&#x27;GPU名称：&#x27;+str(torch.cuda.get_device_name())) print(&#x27;当前设备索引：&#x27;+str(torch.cuda.current_device())) device = torch.device(&quot;cuda:0&quot; if (torch.cuda.is_available()) else &quot;cpu&quot;) print(device) print(torch.rand(3, 3).cuda()) for i in range(1,100000): start = time.time() a = torch.FloatTensor(i*100,1000,1000) a = a.cuda() #a = a a = torch.matmul(a,a) end = time.time() - start print(end) 如果显示cuda可用，并且pytorch tensor可正常放到cuda上面去计算，那么就表明整个pytorch+cuda环境搭建成功，后面的for循环主要是为了测试GPU的显存是否能正常被pytorch调用。 安装Pytorch两种方法安装Pytorch，一种是conda安装，一种是pip安装。 conda安装的好处是直接安装在conda虚拟环境中，缺点是官方支持的包比较少。 pip安装的好处是支持更多的包。安装的缺点是每个conda虚拟环境都要配属于自己的pip，因为默认会全部安装到base环境。 对于Pytorch环境必须安装的包，conda需要安装pytorch、torchvision、torchaudio和cudatoolkit，pip需要安装torch+cu111、torchvision+cu111和torchaudio。包之间的版本必须对应上。需要一条命令同时安装这些包，否则可能会缺少依赖。 conda源相关配置： 123456789101112# 添加源conda config --append channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/# 移除源conda config --remove channels https ://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/# 去除ssl验证conda config --set ssl_verify false# 去除默认conda config --remove channels defaults# 显示urlconda config --set show_channel_urls yes# 查看源conda config --show-sources 在detectron2框架下试验CondInst模型： 因为detectron2提供编译好包，但是只支持某些特定版本。根据框架支持的版本和显卡型号选择对应版本的Pytorch和cuda。 因为conda不提供Pytorch1.9且cuda11.1的安装命令，所以选择pip安装。（在创建进行的时候，已经将conda源和pip源配好） pip安装Pytorch1.9+cuda11.1 1pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 安装编译好的detectron2。 12python -m pip install detectron2 -f \\ https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.9/index.html 从github下载CondInst模型的项目程序，并基于detectron2编译模型的项目程序。 1python setup.py build develop 安装opencv，图像处理必备。 1pip install opencv-python 先准备好模型权重，然后修改demo脚本并删除图形化展示部分或者配置相关参数来将处理后的图像保存。最后后准备样例图片并运行demo脚本。 12345python demo/demo.py \\ --config-file configs/CondInst/MS_R_101_3x.yaml \\ --input demo.jpg \\ --output &quot;demo&quot; \\ --opts MODEL.WEIGHTS CondInst_MS_R_101_3x.pth 提交镜像1docker commit -a &quot;wht&quot; -m &quot;pytorch1.9 cuda1.1 test ok&quot; [容器ID] cuda11_1_pytorch1_9_0:v1 训练模型准备数据集到指定的datasets目录。 1234OMP_NUM_THREADS=1 python tools/train_net.py \\ --config-file configs/CondInst/MS_R_50_1x.yaml \\ --num-gpus 1 \\ OUTPUT_DIR training_dir/CondInst_MS_R_50_1x 因为主机上只有一个gpu所以设定gpu的个数为1。 detectron2提供的imagenet预训练权重的下载链接不能用，需要手动下载，然后设定本地路径。 如果运行过程中显存不够，需要降低batch的大小。 这些配置都可以在config的yaml文件中找到。 参考资料： Forward Compatible Upgrade问题 cuda与驱动对应关系 重装显卡驱动-gpus用不了nvidia内核被占用而无法重装驱动 修改pip源虚拟conda与baseconda的pip冲突 https://github.com/aim-uofa/AdelaiDet/ docker共享内存不够问题 查看gpu信息","categories":[{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://wht6.github.io/tags/Docker/"}]},{"title":"iptables的原理与使用","slug":"iptables原理与使用","date":"2022-04-17T08:00:00.000Z","updated":"2022-09-25T02:44:17.461Z","comments":true,"path":"posts/60b.html","link":"","permalink":"http://wht6.github.io/posts/60b.html","excerpt":"","text":"iptables介绍iptables是Linux自带的一款优秀且开放源代码的基于包过滤的防火墙工具，它的功能十分强大，使用非常灵活，可以对流入和流出服务器的数据包进行很精细的控制。 iptables实际上它只是一个命令行工具，真正的内核防火墙是位于操作系统内核空间的netfilter。通过iptables命令制定规则并执行，即可使得netfilter在内核层对数据包作出相应的处理。netfilter属于“内核态”（Kernel Space，又称为内核空间）的防火墙功能体系；iptables属于“用户态”（User Space，又称为用户空间）的防火墙管理体系。 iptables原理iptables是采用数据包过滤机制工作的，所以它会对请求的数据包的包头数据进行分析，并根据我们预先设定的规则进行匹配来决定是否可以进入主机。 iptables内置了raw、mangle、nat和 filter四个规则表，规则表容纳各种规则链，规则链容纳各种防火墙规则，即“表中有链，链中有规则”。 规则表 说明 raw表 确定是否对该数据包进行状态跟踪。包含两个规则链，OUTPUT、PREROUTING mangle表 修改数据包内容，用来做流量整形的，给数据包设置标记。包含五个规则链，INPUT、OUTPUT、FORWARD、PREROUTING、POSTROUTING nat表 负责网络地址转换，用来修改数据包中的源、目标IP地址或端口。包含三个规则链，OUTPUT、PREROUTING、POSTROUTING。 filter表 负责过滤数据包，确定是否放行该数据包（过滤）。包含三个规则链，INPUT、FORWARD、OUTPUT 在iptables的四个规则表中，mangle表和raw表的应用相对较少。 规则链的含义 报文分为三个流向： 流入本机：PREROUTING —&gt; INPUT—&gt;用户空间进程 流出本机：用户空间进程—&gt;OUTPUT—&gt; POSTROUTING 转发：PREROUTING —&gt; FORWARD —&gt; POSTROUTING 内核中数据包的传输过程： （1）当一个数据包进入网卡时，数据包首先进入PREROUTING链，内核根据数据包目的IP判断是否需要转送出去 （2）如果数据包就是进入本机的，数据包就会到达INPUT链。经INPUT链检查后，数据包被发往本地进程。本地进程进行相应处理后发送响应数据包，数据包经过OUTPUT链，然后到达POSTROUTING链输出；如果数据包是要转发出去的，且内核允许转发，数据包就会向右移动，经过FORWARD链，然后到达POSTROUTING链输出。 规则链 说明 INPUT 处理入站数据包，匹配目标IP为本机的数据包 OUTPUT 处理出站数据包，一般不在此链上做配置 FORWARD 处理转发数据包，匹配流经本机的数据包 PREROUTING 在进行路由选择前处理数据包，用来修改目的地址，用来做DNAT。例如把内网中的80端口映射到路由器外网端口上 POSTROUTING链 在进行路由选择后处理数据包，用来修改源地址，用来做SNAT。例如内网通过路由器NAT转换功能实现内网主机通过一个公网IP地址上网 SNAT和DNAT都是起到了路由转发的功能，可以理解成一个NAT网关。SNAT就是网络地址转换实现多台内网主机公用一个公网IP来对外网进行访问的功能。而DNAT就是路由穿透实现内网主机透过NAT路由为外网开放服务端口的功能。 SNAT： 企业内部的主机A想访问互联网上的主机C，首先将请求数据包（源：ipA，目标：ipC）发送到防火墙所在主机B，B收到后将数据包源地址改为本机公网网卡的ip（源：ipB，目标：ipC），然后经互联网发送给C；C收到后将回应包（源：ipC，目标：ipB）转发给C的路由器，经互联网将回应包转发给B，B收到回应包后修改其目的地址，即回应包改为（源：ipC，目标：ipA）然后将数据包转发给A。DNAT： 互联网主机C想访问企业内部的web服务器A，但A的地址是私有地址，无法直接访问。此时，C可以访问防火墙的公网地址，C的请求数据包（源：ipC，目标：ipB）到达防火墙B后，在B的prerouting上将请求数据包的目标地址进行修改，并将数据包（源：ipC，目标：ipA）发送给A。A收到后进行回复发送响应包（源：ipA，目的ipC）到防火墙，防火墙收到后对数据包源地址进行修改，并将响应包（源：ipB，目标：ipC）给C。利用这种机制可以将企业内部的服务发布到互联网。 这里面其实还包括端口映射的功能，上面的描述只是针对网络层的描述。 匹配顺序规则表的匹配顺序：raw&gt;&gt;mangle&gt;&gt;nat&gt;&gt;filter。 规则链的匹配顺序： 1）入站数据（来自外界的数据包，且目标地址是防火墙本机）PREROUTING —&gt; INPUT —&gt; 本机的应用程序 2）出站数据（从防火墙本机向外部地址发送的数据包）本机的应用程序 —&gt; OUTPUT —&gt; POSTROUTING 3）转发数据（需要经过防火墙转发的数据包）PREROUTING —&gt; FORWARD —&gt; POSTROUTING 规则链内的匹配顺序： 1）自上向下按顺序依次进行检查，找到相匹配的规则即停止（LOG策略例外，表示记录相关日志） 2）若在该链内找不到相匹配的规则，则按该链的默认策略处理（未修改的状况下，默认策略为允许） iptables使用12345678910# 关闭防火墙systemctl stop firewalld.servicesystemctl disable firewalld.service#安装并开启iptables服务yum -y install iptables iptables-servicessystemctl start iptables.service#查看iptables版本iptables -V -L∶列出 （–list）指定链中所有的规则，若未指定链名，则列出表中的所有链 -n ∶ 使用数字形式（–numeric）显示输出结果，如显示 IP 地址而不是主机名 -v ∶ 显示详细信息，包括每条规则的匹配包数量和匹配字节数 123iptables -nvL # 查看全部链的信息iptables -nvL INPUT # 查看INPUT链的信息iptables -nL #去除-v（--verbose）较简单明了，但存在多个网卡时无法知道规则对应那个网卡 插入规则的方式： -A：在指定链的末尾追加（–append）一条新的规则 -I：在指定链的开头插入（–insert）一条新的规则，未指定序号时默认作为第一条规则 -R：修改、替换（–replace）指定链中的某一条规则，可指定规则序号或具体内容 -D：删除（–delete）指定链中的某一条规则，可指定规则序号或具体内容 常用的控制类型： 控制类型 作用 ACCEPT 允许数据包通过。DROP直接丢弃数据包，不给出任何回应信息 REJECT 拒绝数据包通过，会给数据发送端一个响应信息 SNAT 修改数据包的源地址 DNAT 修改数据包的目的地址 MASQUERADE 伪装成一个非固定公网IP地址 LOG 在/var/log/messages文件中记录日志信息，然后将数据包传递给下一条规则。LOG只是一种辅助动作，并没有真正处理数据包 匹配类型通用匹配直接使用，不依赖于其他条件或扩展，包括网络协议、IP地址、网络接口等条件。 1234567协议匹配：-p 协议名地址匹配：-s 源地址、-d 目的地址 #可以是IP、网段、域名、空（任何地址）接口匹配：-i 入站网卡、-o 出站网卡例：iptables -A FORWARD ! -p icmp -j ACCEPT iptables -A INPUT -s 192.168.80.11 -j DROPiptables -I INPUT -i ens33 -s 192.168.80.0/24 -j DROP 隐含匹配以特定的协议匹配作为前提，包括端口、TCP标记、ICMP类型等条件。 1.端口匹配 123456789--sport 源端口 --dport 目的端口 #可以是个别端口、端口范围--sport 1000 匹配源端口是1000的数据包--sport 1000:3000 匹配源端口是1000-3000的数据包--sport :3000 匹配源端口是3000及以下的数据包--sport 1000: 匹配源端口是1000及以上的数据包注意：--sport 和 --dport 必须配合 -p &lt;协议类型&gt; 使用例：iptables -A INPUT -p tcp --dport 20:21 -j ACCEPTiptables -I FORWARD -d 192.168.80.0/24 -p tcp --dport 24500:24600 -j DROP 2.TCP标记匹配 12--tcp-flags TCP标记iptables -I INPUT -i ens33 -p tcp --tcp-flags SYN,RST,ACK SYN -j ACCEPT #丢弃SYN请求包，放行其他包 3.ICMP类型匹配 12345678910--icmp-type ICMP类型 #可以是字符串、数字代码、、目标不可达“Echo-Request”（代码为 8）表示 请求“Echo-Reply”（代码为 0）表示 回显“Destination-Unreachable”（代码为 3）表示 目标不可达关于其它可用的 ICMP 协议类型，可以执行“iptables -p icmp -h”命令，查看帮助信息例：iptables -A INPUT -p icmp --icmp-type 8 -j DROP #禁止其它主机ping 本机iptables -A INPUT -p icmp --icmp-type 0 -j ACCEPT #允许本机ping其它主机iptables -A INPUT -p icmp --icmp-type 3 -j ACCEPT #当本机ping不通其它主机时提示目标不可达iptables -A INPUT -p icmp -j REJECT #此时其它主机需要配置关于icmp协议的控制类型为 REJECT 显示匹配要求以“-m 扩展模块”的形式明确指出类型，包括多端口、MAC地址、IP范围、数据包状态等条件。 1.多端口匹配 12345-m multiport --sports 源端口列表-m multiport --dports 目的端口列表例：iptables -A INPUT -p tcp -m multiport --dport 80,22,21,20,53 -j ACCEPTiptables -A INPUT -p udp -m multiport --dport 53 -j ACCEPT 2.IP范围匹配 12-m iprange --src-range IP范围iptables -A FORWARD -p udp -m iprange --src-range 192.168.80.100-192.168.80.200 -j DROP 3.状态匹配 1234567-m state --state 连接状态常见的连接状态：NEW ：与任何连接无关的，还没开始连接ESTABLISHED ：响应请求或者已建立连接的，连接态RELATED ：与已有连接有相关性的（如FTP 主被动模式的数据连接），衍生态，一般与ESTABLISHED 配合使用INVALID ：不能被识别属于哪个连接或没有任何状态iptables -A FORWARD -m state --state NEW -p tcp ! --syn -j DROP #禁止转发与正常 TCP 连接无关的非--syn 请求数据包（如伪造的网络攻击数据包） 参考链接： https://blog.csdn.net/beanewself/article/details/78317626 http://bencane.com/2012/09/17/iptables-linux-firewall-rules-for-a-basic-web-server/ http://www.manongjc.com/detail/25-oqjjkuteocznbct.html http://www.manongjc.com/detail/50-wsghzvsjpmmvvrm.html","categories":[{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"防火墙","slug":"防火墙","permalink":"http://wht6.github.io/tags/%E9%98%B2%E7%81%AB%E5%A2%99/"},{"name":"iptables","slug":"iptables","permalink":"http://wht6.github.io/tags/iptables/"}]},{"title":"迭代器和生成器","slug":"迭代器和生成器","date":"2022-04-13T08:00:00.000Z","updated":"2022-09-23T13:30:04.333Z","comments":true,"path":"posts/1b04.html","link":"","permalink":"http://wht6.github.io/posts/1b04.html","excerpt":"","text":"可迭代对象迭代就是更新换代，每次迭代都必须基于上一次的迭代成果，单纯的重复不是迭代。 12345678# 不属于迭代 while True: print(&#x27;嘿嘿嘿&#x27;) # 属于迭代 n = 0 while n &lt; 10: print(n) n += 1 判断可迭代对象 内置iter方法的都可以叫可迭代对象’读作：双下iter方法 int、float、bool 以及函数名都不是可迭代对象 字符串，列表，字典，元组，集合，文件都是可迭代对象，其中文件还是迭代器对象 迭代器对象 作用 迭代器对象为我们提供了一种不依赖索引的取值方式，因此我们可以对字典、集合这种无序类型循环取值。 如何判断迭代器对象 内置有iter和next方法的成为迭代器对象 可迭代对象与迭代器对象的关系 可迭代对象调用iter方法之后就会变成迭代器对象 迭代器对象调用iter方法无论多少次还是迭代器对象本身 迭代器对象迭代取值 1234567891011121314151617s1 = &#x27;abc&#x27; # s1本就是可迭代对象 s1 = s1.__iter__() # 将s1变为迭代器对象 print(s1.__next__()) # a print(s1.__next__()) # b print(s1.__next__()) # c print(s1.__next__()) # 取不到值，直接报错 l1 = [11, 22, 33, 44, 55, 66, 77, 88] # 需求:不使用for循环 依次打印出列表中所有的数据值 # 1.先将列表变成迭代器对象 res = l1.__iter__() # 2.定义一个计数器 count = 0 # 3.编写while循环 while count &lt; len(l1): print(res.__next__()) count += 1 迭代器反复使用 12345678910l1 = [1,2,3,4] print(l1.__iter__().__next__()) # 1 print(l1.__iter__().__next__()) # 1 print(l1.__iter__().__next__()) # 1# 每次都是产生了一个新的迭代器对象，所以每次输出都是一样的值 l1 = l1.__iter__() print(l1.__iter__().__next__()) # 1 print(l1.__iter__().__next__()) # 2 print(l1.__iter__().__next__()) # 3 # 每次使用的都是同一个迭代器对象 简写 12res = l1.__iter__() ===&gt; iter(l1)res.__next__() ===&gt; next(res) 迭代取值和索引取值的差异12345678910111213l1 = [1,2,3,4,5]#索引取值print(l1[0])print(l1[1])print(l1[2])print(l1[4])#迭代取值res = l1.__iter__()print(res.__next__())print(res.__next__())print(res.__next__())print(res.__next__()) 索引取值： 优势：可以随意反复地获取任意数据值(自由) 劣势：针对无序的容器类型无法取值(类型限制) 迭代取值： 优势：可以对无序的容器类型取值(通用) 劣势：取值只能一直向前，不能回退(不可逆) for循环的本质 12for 变量名 in 可迭代对象: for循环体代码 for会自动将in后面的数据调用iter()变成迭代器对象 之后每次循环调用next()取值 最后没有值next()会报错 for能够自动处理该错误 让循环正常结束 生成器对象 本质就是迭代器对象(内置iter 和 next方法)，只不过迭代器是python解释器提供给我们的(现成的)，生成器是我们自己定义的。 生成器的主要作用是节省代码，是一种不依赖索引的取值方式，主要可以节省数据类型占用的内存空间 生成器对象代码实现 1234567891011121314 def index(): print(&#x27;我跳出来了~&#x27;) yield 111 print(&#x27;我又跳进去了~&#x27;) yield 222 print(&#x27;我又跳出来了~&#x27;) yield 333, 222, 111print(index) # &lt;function index at 0x000001E30205F0D0&gt;res = index()print(res) # &lt;generator object index at 0x0000015127BFDC10&gt; 生成器对象res.__next__() # 我跳出来了~res.__next__() # 我又跳进去了~print(res.__next__()) # 我又跳出来了~ (333, 222, 111) 当函数体代码中有yield关键字，函数名第一次加括号调用不会执行函数体代码，而是由普通的函数变成了迭代器对象(生成器)。 yield可以在函数体代码中出现多次，每次调用next方法都会从上到下执行，直到遇到yield停留到此处。 yield后面如果有数据值，则会像return一样返回出去，如果有多个数据值，则会自动组织成元组返回。 编写生成器 实现range方法的功能 123456789101112131415161718192021222324252627# 1.先编写两个参数的情况def my_range(start_num, end_num): while start_num &lt; end_num: yield start_num start_num += 1for i in my_range(1, 10): print(i)# 2.再编写可能有一个参数的情况 def my_range(start_num, end_num = None): if not end_num: end_num = start_num start_num = 1 while start_num &lt; end_num: yield start_num start_num += 1for i in my_range(10): print(i) # 3.最后编写三个参数的情况def my_range(start_num, end_num = None, step = 1): if not end_num: end_num = start_num start_num = 1 while start_num &lt; end_num: yield start_num start_num += stepfor i in my_range(1,10,2): print(i) yield其他用法yield不仅能返回值，还可以接收外界传来的值 1234567891011121314def index(name,food=None): print(f&#x27;&#123;name&#125;准备干午饭!!!&#x27;) while True: food = yield print(f&#x27;&#123;name&#125;正在吃&#123;food&#125;&#x27;)res = index(&#x27;jason&#x27;)res.__next__()res.send(&#x27;豆皮&#x27;) # send方法可以将&#x27;豆皮&#x27;传值给yield(此时food = &#x27;豆皮&#x27;)，并自动调用__next__方法res.send(&#x27;鱼子酱&#x27;)#&gt;&gt;&gt;#jason准备干午饭!!!#jason正在吃豆皮#jason正在吃鱼子酱 参考链接： https://www.jianshu.com/p/9ad4428aa428 https://www.jianshu.com/p/9185ccbe9cd7 https://juejin.cn/post/7049520631272079374","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://wht6.github.io/tags/Python/"}]},{"title":"Python装饰器理解","slug":"Python装饰器理解","date":"2022-04-12T08:00:00.000Z","updated":"2022-09-23T02:16:01.810Z","comments":true,"path":"posts/652d.html","link":"","permalink":"http://wht6.github.io/posts/652d.html","excerpt":"","text":"1234567891011def log(func): def wrapper(*args, **kw): print(&#x27;call %s():&#x27; % func.__name__) return func(*args, **kw) return wrapper@logdef now(): print(&#x27;2022-4-11&#x27;) 123&gt;&gt;&gt; now()call now():2022-4-11 这里@log就相当于now = log(now)。 @就代表了装饰器符号，装饰器其实也就是一个函数，一个用来包装函数的函数，返回一个修改之后的函数对象，将其重新赋值原来的标识符，并永久丧失对原始函数对象的访问。 now作为参数传给log函数，log函数中定义了wrapper，在return wrapper处执行了wrapper函数。 相当于wrapper是类，return返回的wrapper是实例化之后的对象。 执行wrapper函数就会执行print，print用到了函数之外的参数func，这其实就是闭包。因为now没有传递任务参数，所以这里的*args, **kw其实没啥用。 最后返回func函数（对应的是now函数），就会执行func函数中的内容，即print(&#39;2022-4-11&#39;)。 下面是个复杂一点的： 1234567891011def log(text): def decorator(func): def wrapper(*args, **kw): print(&#x27;%s %s():&#x27; % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator@log(&#x27;execute&#x27;)def now(): print(&#x27;2022-4-11&#x27;) 123&gt;&gt;&gt; now()execute now():2022-4-11 这里@log就相当于now = log(now,&#39;execute&#39;)。 log函数中定义了decorator，在return decorator处执行了decorator函数。 执行decorator函数在return wrapper就会执行wrapper，wrapper中的print用到了函数外的参数，text和func，return func时执行func函数，即now函数。 因此，装饰器本身指向一个函数func_A，装饰器装饰的函数func_B会作为func_A的模块嵌入到func_A，但是我在调用func_B的时候，会直接去调用func_A。这就好像给原函数增加了一层外衣，在调用原函数的时候会连同“外衣”一起调用。我觉得这么做的原因可能是让外部操作不影响函数的内部逻辑，但又可以将这些外部操作与函数绑定在一起。 闭包： 在函数式语言中，当内嵌函数体内引用到体外的变量时，将会把定义时涉及到的引用环境和函数体打包成一个整体（闭包）返回。 所谓闭包，就是将组成函数的语句和这些语句的执行环境打包在一起时，得到的对象,一个闭包就是你调用了一个函数A，这个函数A返回了一个函数B给你。这个返回的函数B就叫做闭包。 1234567def func(name): def inner_func(age): print &#x27;name:&#x27;, name, &#x27;age:&#x27;, age return inner_funcbb = func(&#x27;the5fire&#x27;)bb(26) # &gt;&gt;&gt; name: the5fire age: 26 这里面调用func的时候就产生了一个闭包——inner_func,并且该闭包持有自由变量——name，因此这也意味着，当函数func的生命周期结束之后，name这个变量依然存在，因为它被闭包引用了，所以不会被回收。 简单的说，这种内部函数可以使用外部函数变量的行为，就叫闭包。","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://wht6.github.io/tags/Python/"}]},{"title":"pytorch环境配置","slug":"pytorch环境配置","date":"2022-04-11T08:00:00.000Z","updated":"2022-11-08T06:35:01.784Z","comments":true,"path":"posts/2635.html","link":"","permalink":"http://wht6.github.io/posts/2635.html","excerpt":"","text":"检查python版本 123import sys print (&quot;Python Version &#123;&#125;&quot;.format(str(sys.version).replace(&#x27;\\n&#x27;, &#x27;&#x27;))) 检查pytorch版本 1234567import timeimport torchif __name__ == &#x27;__main__&#x27;: print(&#x27;torch版本：&#x27;+torch.__version__) print(&#x27;cuda是否可用：&#x27;+str(torch.cuda.is_available())) print(&#x27;cuda版本：&#x27;+str(torch.version.cuda)) 卸载pytorch 先查看是否是pip安装，pip list 如果是： 123pip uninstall torchpip uninstall torchvisionpip uninstall torchaudio 如果不是： 12conda uninstall pytorchconda uninstall libtorch 非conda环境的python版本安装和切换 python安装 1apt install python3.7 python版本切换 检查切换列表是否可用(刚开始可能是不可用的) 1update-alternatives --list python 将想要使用的python版本添加到列表（末尾表示优先级，值越大，优先级越高，默认使用优先级最高的版本） 12update-alternatives --install /usr/bin/python python /usr/bin/python2.7 1update-alternatives --install /usr/bin/python python /usr/bin/python3.7 2 版本切换： 1update-alternatives --config python conda环境的python版本安装和切换 conda环境下一个虚拟环境就对应一个python版本，因此只需创建新的conda环境即可 创建conda环境并指定python版本 1conda create --name py37 python=3.7 切换conda环境 1conda activate py37 安装pytorch 1pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://wht6.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"实例分割","slug":"实例分割","permalink":"http://wht6.github.io/tags/%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2/"}]},{"title":"Nginx基础安装配置","slug":"Nginx基础安装配置","date":"2022-04-10T08:00:00.000Z","updated":"2022-04-13T02:30:41.270Z","comments":true,"path":"posts/379a.html","link":"","permalink":"http://wht6.github.io/posts/379a.html","excerpt":"","text":"Nginx简介什么是NginxNginx是一个开源、轻量级和高性能的 Web 服务器，也用作 HTTP、HTTPS、SMTP、IMAP、POP3 协议的反向代理服务器，另一方面，它也用作 IMAP、POP3 和 IMAP 的 HTTP 负载均衡器、HTTP 缓存和电子邮件代理。简言至，Nginx 是一种用于处理并发请求的软件。 Nginx 采用了模块化、事件驱动的架构设计，使用了异步非阻塞的事件处理机制处理请求，使得在高负载下也能提供更可靠的性能。 Nginx的架构进程模型Nginx 在启动后，在 unix 系统中会以 daemon （守护进程）的方式在后台运行，后台进程包含一个 master 进程和多个 worker 进程。 master 进程主要用来管理 worker 进程，包含：接收来自外界的信号，向各 worker 进程发送信号，监控 worker 进程的运行状态，当 worker 进程退出后(异常情况下)，会自动重新启动新的 worker 进程。 而基本的网络事件，则是放在 worker 进程中来处理了。 多个 worker 进程之间是对等的，他们同等竞争来自客户端的请求，各进程互相之间是独立的。一个请求，只可能在一个 worker 进程中处理，一个 worker 进程，不可能处理其它进程的请求。 worker 进程的个数是可以设置的，一般我们会设置与机器cpu核数一致，这里面的原因与 Nginx 的进程模型以及事件处理模型是分不开的。Nginx 的进程模型，可以由下图来表示： 在 Nginx 启动后，如果我们要操作 Nginx，要怎么做呢？从上文中我们可以看到，master 来管理 worker 进程，所以我们只需要与 master 进程通信就行了。 master 进程会接收来自外界发来的信号，再根据信号做不同的事情。所以我们要控制 Nginx，只需要通过 kill 向 master 进程发送信号就行了。比如kill -HUP pid，则是告诉 Nginx，从容地重启 Nginx，我们一般用这个信号来重启 Nginx，或重新加载配置，因为是从容地重启，因此服务是不中断的。 master 进程在接收到 HUP 信号后是怎么做的呢？首先 master 进程在接到信号后，会先重新加载配置文件，然后再启动新的 worker 进程，并向所有老的 worker 进程发送信号，告诉他们可以光荣退休了。新的 worker 在启动后，就开始接收新的请求，而老的 worker 在收到来自 master 的信号后，就不再接收新的请求，并且在当前进程中的所有未处理完的请求处理完成后，再退出。 当然，直接给 master 进程发送信号，这是比较老的操作方式，Nginx 在 0.8 版本之后，引入了一系列命令行参数，来方便我们管理。比如，./nginx -s reload，就是来重启 Nginx，./nginx -s stop，就是来停止 Nginx 的运行。 如何做到的呢？我们还是拿 reload 来说，我们看到，执行命令时，我们是启动一个新的 Nginx 进程，而新的 Nginx 进程在解析到 reload 参数后，就知道我们的目的是控制 Nginx 来重新加载配置文件了，它会向 master 进程发送信号，然后接下来的动作，就和我们直接向 master 进程发送信号一样了。 现在，我们知道了当我们在操作 Nginx 的时候，Nginx 内部做了些什么事情，那么，worker 进程又是如何处理请求的呢？我们前面有提到，worker 进程之间是平等的，每个进程，处理请求的机会也是一样的。当我们提供 80 端口的 http 服务时，一个连接请求过来，每个进程都有可能处理这个连接，怎么做到的呢？ 首先，每个 worker 进程都是从 master 进程 fork 过来，在 master 进程里面，先建立好需要 listen 的 socket（listenfd）之后，然后再 fork 出多个 worker 进程。所有 worker 进程的 listenfd 会在新连接到来时变得可读，为保证只有一个进程处理该连接，所有 worker 进程在注册 listenfd 读事件前抢 accept_mutex，抢到互斥锁的那个进程注册 listenfd 读事件，在读事件里调用 accept 接受该连接。当一个 worker 进程在 accept 这个连接之后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接，这样一个完整的请求就是这样的了。我们可以看到，一个请求，完全由 worker 进程来处理，而且只在一个 worker 进程中处理。 那么，Nginx 采用这种进程模型有什么好处呢？当然，好处肯定会很多了。首先，对于每个 worker 进程来说，独立的进程，不需要加锁，所以省掉了锁带来的开销，同时在编程以及问题查找时，也会方便很多。其次，采用独立的进程，可以让互相之间不会影响，一个进程退出后，其它进程还在工作，服务不会中断，master 进程则很快启动新的 worker 进程。当然，worker 进程的异常退出，肯定是程序有 bug 了，异常退出，会导致当前 worker 上的所有请求失败，不过不会影响到所有请求，所以降低了风险。当然，好处还有很多，大家可以慢慢体会。 事件处理有人可能要问了，Nginx 采用多 worker 的方式来处理请求，每个 worker 里面只有一个主线程，那能够处理的并发数很有限啊，多少个 worker 就能处理多少个并发，何来高并发呢？ 非也，这就是 Nginx 的高明之处，Nginx 采用了异步非阻塞的方式来处理请求，也就是说，Nginx 是可以同时处理成千上万个请求的。想想 apache 的常用工作方式（apache 也有异步非阻塞版本，但因其与自带某些模块冲突，所以不常用），每个请求会独占一个工作线程，当并发数上到几千时，就同时有几千的线程在处理请求了。这对操作系统来说，是个不小的挑战，线程带来的内存占用非常大，线程的上下文切换带来的 cpu 开销很大，自然性能就上不去了，而这些开销完全是没有意义的。 为什么 Nginx 可以采用异步非阻塞的方式来处理呢，或者异步非阻塞到底是怎么回事呢？ 我们先回到原点，看看一个请求的完整过程。首先，请求过来，要建立连接，然后再接收数据，接收数据后，再发送数据。具体到系统底层，就是读写事件，而当读写事件没有准备好时，必然不可操作，如果不用非阻塞的方式来调用，那就得阻塞调用了，事件没有准备好，那就只能等了，等事件准备好了，你再继续吧。阻塞调用会进入内核等待，cpu 就会让出去给别人用了，对单线程的 worker 来说，显然不合适，当网络事件越多时，大家都在等待呢，cpu 空闲下来没人用，cpu利用率自然上不去了，更别谈高并发了。 好吧，你说加进程数，这跟apache的线程模型有什么区别，注意，别增加无谓的上下文切换。所以，在 Nginx 里面，最忌讳阻塞的系统调用了。不要阻塞，那就非阻塞喽。非阻塞就是，事件没有准备好，马上返回 EAGAIN，告诉你，事件还没准备好呢，你慌什么，过会再来吧。好吧，你过一会，再来检查一下事件，直到事件准备好了为止，在这期间，你就可以先去做其它事情，然后再来看看事件好了没。虽然不阻塞了，但你得不时地过来检查一下事件的状态，你可以做更多的事情了，但带来的开销也是不小的。 CPU 上下文：CPU 寄存器和程序计数器就是 CPU 上下文，因为它们都是 CPU 在运行任何任务前，必须的依赖环境。CPU 寄存器是 CPU 内置的容量小、但速度极快的内存。程序计数器则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。 CPU 上下文切换：就是先把前一个任务的 CPU 上下文（也就是 CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。而这些保存下来的上下文，会存储在系统内核中，并在任务重新调度执行时再次加载进来。这样就能保证任务原来的状态不受影响，让任务看起来还是连续运行。 所以，才会有了异步非阻塞的事件处理机制，具体到系统调用就是像 select/poll/epoll/kqueue 这样的系统调用。它们提供了一种机制，让你可以同时监控多个事件，调用他们是阻塞的，但可以设置超时时间，在超时时间之内，如果有事件准备好了，就返回。这种机制正好解决了我们上面的两个问题，拿 epoll 为例(在后面的例子中，我们多以 epoll 为例子，以代表这一类函数)，当事件没准备好时，放到 epoll 里面，事件准备好了，我们就去读写，当读写返回 EAGAIN 时，我们将它再次加入到 epoll 里面。这样，只要有事件准备好了，我们就去处理它，只有当所有事件都没准备好时，才在 epoll 里面等着。这样，我们就可以并发处理大量的并发了，当然，这里的并发请求，是指未处理完的请求，线程只有一个，所以同时能处理的请求当然只有一个了，只是在请求间进行不断地切换而已，切换也是因为异步事件未准备好，而主动让出的。这里的切换是没有任何代价，你可以理解为循环处理多个准备好的事件，事实上就是这样的。 与多线程相比，这种事件处理方式是有很大的优势的，不需要创建线程，每个请求占用的内存也很少，没有上下文切换，事件处理非常的轻量级。并发数再多也不会导致无谓的资源浪费（上下文切换）。 我们之前说过，推荐设置 worker 的个数为 cpu 的核数，在这里就很容易理解了，更多的 worker 数，只会导致进程来竞争 cpu 资源了，从而带来不必要的上下文切换。而且，nginx为了更好的利用多核特性，提供了 cpu 亲缘性的绑定选项，我们可以将某一个进程绑定在某一个核上，这样就不会因为进程的切换带来 cache 的失效。像这种小的优化在 Nginx 中非常常见，同时也说明了 Nginx 作者的苦心孤诣。 信号与定时器现在，知道了 Nginx 为什么会选择这样的进程模型与事件模型了。对于一个基本的 Web 服务器来说，事件通常有三种类型，网络事件、信号、定时器。从上面的讲解中知道，网络事件通过异步非阻塞可以很好的解决掉。如何处理信号与定时器？ 首先，信号的处理。对 Nginx 来说，有一些特定的信号，代表着特定的意义。信号会中断掉程序当前的运行，在改变状态后，继续执行。如果是系统调用，则可能会导致系统调用的失败，需要重入。对于 Nginx 来说，如果nginx正在等待事件（epoll_wait 时），如果程序收到信号，在信号处理函数处理完后，epoll_wait 会返回错误，然后程序可再次进入 epoll_wait 调用。 另外，再来看看定时器。由于 epoll_wait 等函数在调用的时候是可以设置一个超时时间的，所以 Nginx 借助这个超时时间来实现定时器。nginx里面的定时器事件是放在一颗维护定时器的红黑树里面，每次在进入 epoll_wait前，先从该红黑树里面拿到所有定时器事件的最小时间，在计算出 epoll_wait 的超时时间后进入 epoll_wait。所以，当没有事件产生，也没有中断信号时，epoll_wait 会超时，也就是说，定时器事件到了。这时，nginx会检查所有的超时事件，将他们的状态设置为超时，然后再去处理网络事件。由此可以看出，当我们写 Nginx 代码时，在处理网络事件的回调函数时，通常做的第一个事情就是判断超时，然后再去处理网络事件。 Nginx安装编译安装Nginx准备工作这里我们使用nginx的mainline版本的1.8.0来进行编译安装，nginx各版本的官网下载地址：http://nginx.org/en/download.html 首先我们下载并解压nginx源码 12wget http://nginx.org/download/nginx-1.18.0.tar.gz tar -xvf nginx-1.18.0.tar.gz 在编译安装之前我们还需要先安装几个别的软件： GCC/G++编译器：GCC（GNU Compiler Collection）可用来编译C语言程序，如果你还需要使用C++来编写Nginx HTTP模块，这时还需要用到G++编译器了。 PCRE库：PCRE（Perl Compatible Regular Expressions，Perl兼容正则表达式）是由Philip Hazel开发的函数库，目前为很多软件所使用，该库支持正则表达式。实际上在nginx的很多高级配置中都会用到正则表达式，因此我们在编译Nginx时尽量先把PCRE库编译进Nginx。 zlib库：zlib库用于对HTTP包的内容做gzip格式的压缩，我们可以在nginx.conf里配置了gzip on，并指定对于某些类型（content-type）的HTTP响应使用gzip来进行压缩以减少网络传输量。 OpenSSL开发库：HTTPS必备，这个就不用解释了 上面提到的库我们都可以使用yum来进行安装： 1234yum install gcc gcc-c++ pcre pcre-devel zlib zlib-devel openssl openssl-devel# pcre-devel是使用PCRE做二次开发时所需要的开发库，包括头文件等，这也是编译Nginx所必须使用的。# 同理，zlib是直接使用的库，zlib-devel是二次开发所需要的库。 Nginx是高度自由化的Web服务器，它的功能是由许多模块来支持的。而这些模块可根据我们的使用需求来定制，如果某些模块不需要使用则完全不必理会它。同样，如果使用了某个模块，而这个模块使用了一些类似zlib或OpenSSL等的第三方库，那么就必须先安装这些软件。 编译安装我们进入nginx的目录，输入下面的指令可以查看各类的编译参数，或者在官网也可以看到： 1./configure --help 我们这里使用的参数是： 1./configure --sbin-path=/usr/local/nginx/nginx --conf-path=/usr/local/nginx/nginx.conf --pid-path=/usr/local/nginx/nginx.pid --with-http_ssl_module mask编译安装： 12makemake install 如无意外此时应该已经正常安装好了，我们到前面指定的安装目录看一下: 12cd /usr/local/nginx/ll 注意这个时候我们如果需要使用nginx需要指定这个安装目录，想要全局使用我们可以创建一个软链接： 1ln -s /usr/local/nginx/nginx /usr/sbin/nginx 添加模块同时，如果之后有需要用到的模块而在编译安装的时候忘了安装也没关系，我们可以继续编译添加新模块 首先我们需要查看已经编译的参数: 1234567nginx -Vnginx version: nginx/1.18.0built by gcc 4.8.5 20150623 (Red Hat 4.8.5-44) (GCC)built with OpenSSL 1.0.2k-fips 26 Jan 2017TLS SNI support enabledconfigure arguments: --sbin-path=/usr/local/nginx/nginx --conf-path=/usr/local/nginx/nginx.conf --pid-path=/usr/local/nginx/nginx.pid --with-http_ssl_module 在configure arguments: 这一栏里面我们就可以看到之前编译的时候的参数，对比上面的记录我们可以看到是一模一样的，然后我们会到之前下载的源码目录，然后添加上之前的编译参数，再添加新的模块。 1./configure --sbin-path=/usr/local/nginx/nginx --conf-path=/usr/local/nginx/nginx.conf --pid-path=/usr/local/nginx/nginx.pid --with-http_ssl_module --with-http_v2_module --with-mail --with-mail_ssl_module 比如这里我们添加了http_v2、mail、mail_ssl三个模块 如果想要添加第三方模块的话，只需要使用--add-module=然后加上第三方模块的路径即可。 1--add-module=/home/echo-nginx-module-0.61 最后我们的编译参数是： 123456789./configure \\--sbin-path=/usr/local/nginx/nginx \\--conf-path=/usr/local/nginx/nginx.conf \\--pid-path=/usr/local/nginx/nginx.pid \\--with-http_ssl_module \\--with-http_v2_module \\--with-mail \\--with-mail_ssl_module \\--add-module=/home/echo-nginx-module-0.61 接着我们使用make安装，再查看目录会发现原来的文件已经被替换成*.default了 1234567891011121314151617makemake installcd /usr/local/nginx/ # 查看安装目录llnginx -V # 查看模板是否安装nginx version: nginx/1.18.0built by gcc 4.8.5 20150623 (Red Hat 4.8.5-44) (GCC)built with OpenSSL 1.0.2k-fips 26 Jan 2017TLS SNI support enabledconfigure arguments: --sbin-path=/usr/local/nginx/nginx --conf-path=/usr/local/nginx/nginx.conf --pid-path=/usr/local/nginx/nginx.pid --with-http_ssl_module --with-http_v2_module --with-mail --with-mail_ssl_module --add-module=/home/echo-nginx-module-0.61# 测试Nginxnginxcurl 127.0.0.1 Yum安装Nginxcentos自带的repo中就有nginx，可以直接安装，但是版本比较旧，想要使用yum进行安装最新的稳定版本，我们需要自行配置yum仓库。 123456789101112131415[nginx-stable]name=nginx stable repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=1enabled=1gpgkey=https://nginx.org/keys/nginx_signing.keymodule_hotfixes=true[nginx-mainline]name=nginx mainline repobaseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/gpgcheck=1enabled=0gpgkey=https://nginx.org/keys/nginx_signing.keymodule_hotfixes=true 默认情况下mainline版本是不会启用的，因此我们如果需要安装mainline版本的nginx则需要手动启用这个repo。 12345yum -y install yum-utilsyum-config-manager --enable nginx-mainlineyum clean allyum repolistyum -y install nginx 安装的时候需要注意列出信息中的repo应该是我们刚刚新建的repo。 安装完成之后需要设置开机启动和防火墙放行80端口，如果使用https还需要放行443端口。 123456789systemctl enable nginx systemctl start nginx firewall-cmd --permanent --zone=public --add-port=80/tcpfirewall-cmd --permanent --zone=public --add-port=443/tcpfirewall-cmd --reload # 测试是否安装成功nginx -vcurl 127.0.0.1 Nginx配置配置文件nginx服务器的默认配置文件是/etc/nginx/nginx.conf。我们可以在这里对nginx的所有全局配置进行修改，包括线程数端口号等等，同时在默认情况下它也包括了/etc/nginx/conf.d/目录中的所有配置文件。 /etc/nginx/conf.d/包含的.conf配置文件主要用于单独定义某个http网页，从而使得整个配置目录文件的管理变得更加简洁而清晰。 /var/log/nginx目录是默认的log日志目录，主要有acces.log和error.log两个文件，前者负责记录每一个被访问的记录，后者负责记录访问中出现的错误。 nginx程序的指令： nginx -h：查看帮助文档 nginx -t：检查配置 nginx -s stop：关闭nginx nginx -s reload：重新加载配置文件后重启 格式： 123456789# 配置项格式[配置项名] [配置值1] [配置值2] ... ;# 配置块格式[配置块名]&#123;[配置项/块][配置项/块]...&#125; 一个配置项以英文分号;结束，中间的值使用空格隔开 块配置项由一个块配置项名和一对大括号组成。 块配置项可以嵌套，内层块直接继承外层块。 当内外层块中的配置发生冲突时，究竟是以内层块还是外层块的配置为准，取决于解析这个配置项的模块。 注释部分使用井号＃ 下面是nginx.conf中的关键字： user指的是以哪个用户来创建nginx的worker进程，master进程一般都是使用root用户启动，权限较大。 worker_processes则是nginx的worker进程数量，一般与CPU的核心数量一致。 error_log是日志的存放位置和输出等级，等级的取值范围是debug、info、notice、warn、error、crit、alert、emerg，从左至右级别依次增大。当设定为一个级别时，大于或等于该级别的日志都会被输出到记录文件中，小于该级别的日志则不会输出。这里默认设定的是warn级别，则warn、error、crit、alert、emerg级别的日志都会输出。 如果设定的日志级别是debug，则会输出所有的日志，这样数据量会很大，要确保存放日志的硬盘有足够的空间，同时，如果需要开启日志的debug功能，需要在编译安装的时候在configure时加入--with-debug配置项，如果不确定是否开启了debug功能，可以输入nginx -V查看所有的configure arguments。 pid是nginx的master进程的pid文件，理论上应该和查找的nginx进程中master进程的PID以及worker进程的PPID一致。 server块作用是配置的是虚拟主机，一个server块就相当于一个虚拟主机，包括主机名、端口、location块。（虚拟主机从用户角度看，和一台独立的硬件主机是完全一样的，该技术的产生是为了节省互联网服务器的硬件成本） location块作用基于nginx服务器接收到的请求字符串（例如server_name/uri-string），对虚拟主机名称（域名）之外的字符串（例如前面的/uri-string）进行匹配，对特定的请求进行处理。 静态页面配置使用vim对/etc/nginx/conf.d/default.conf进行修改，需要注意的是默认情况下/etc/nginx/conf.d/下面的配置文件只要是.conf即可生效，前面的名称并没有特殊限制，所以最好根据文件的实际用途进行命名方便记忆和管理。 在conf.d下，新建example.conf文件。并在/var/www/html下创建三个html文件：example.com.html，example.net.html，example.org.html。 123456789101112131415161718192021222324252627server &#123; listen 80; server_name example.com www.example.com; location / &#123; root /var/www/html; index example.com.html; &#125;&#125;server &#123; listen 80; server_name example.net www.example.net; location / &#123; root /var/www/html; index example.net.html; &#125;&#125;server &#123; listen 80 default_server; server_name example.org www.example.org; location / &#123; root /var/www/html; index example.org.html; &#125;&#125; 上面配置了三个server块，分别对应三组域名，三组域名都是指向本机的IP地址，同样都是监听的80端口，其中我们在第三个server块中指定了default_server参数，此时我们访问本机IP，返回的页面就是我们指定了default_server参数的这个页面。如果我们不指定default_server参数，按照匹配规则来匹配server，并返回对应的页面。 &nbsp; 参考资料： https://www.cainiaojc.com/nginx/nginx-introduction.html https://www.leafage.top/posts/detail/20C25YW6T http://www.manongjc.com/detail/12-spsvbhbesklpqqv.html https://tinychen.com/20200317-nginx-01-base-conf-static-web/ https://www.w3cschool.cn/nginx/ycn81k97.html","categories":[{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://wht6.github.io/tags/Nginx/"}]},{"title":"InnoDB数据表","slug":"InnoDB数据表","date":"2022-04-07T10:00:00.000Z","updated":"2022-10-08T07:25:03.462Z","comments":true,"path":"posts/da7e.html","link":"","permalink":"http://wht6.github.io/posts/da7e.html","excerpt":"","text":"索引组织表首先从定义上认识一下是主键和索引。 主键是关系表中记录的唯一标识。 索引是关系数据库中对某一列或多个列的值进行预排序的数据结构。通过使用索引，可以让数据库系统不必扫描整个表，而是直接定位到符合条件的记录，这样就大大加快了查询速度。 主键是一种约束，唯一索引是一种索引。 定义主键时会自动创建主键索引，主键索引是唯一索引的特定类型。主键索引不可未空，唯一索引可以为空。 之所以在创建主键的时候会自动创建主键索引，是因为索引可以提升查询的效率。 在 InnoDB存储引擎中,表都是根据主键顺序组织存放的,这种存储方式的表称为索引组织表( index organized table)。在 InnoDB存储引擎表中,每张表都有个主键( Primary Key),如果在创建表时没有显式地定义主键,则 InnoDB存储引擎会按如下方式选择或创建主键: 首先判断表中是否有非空的唯一索引( Unique NOT NULL),如果有,则该列即为主键。 如果不符合上述条件, InnoDB存储引擎自动创建一个6字节大小的指针。 当表中有多个非空唯一索引时, InnoDB存储引擎将选择建表时第一个定义的非空唯一索引为主键。 1create table z( a int not null, b int null, c int not null, d int not null, unique key(b), unique key(d), unique key(c) ); 创建表时由于没有显式地定义主键，选择第一个定义的唯一非空索引（d）为主键。 索引查找算法接着来了解索引查找算法的演化过程。 线性查找：复杂度O(N)，太慢了。 二分查找：复杂度O(logN)，每次可将数据量减小一半，但是，数据在磁盘中往往不是顺序存储的，会出现找不到中点的问题。 二叉查找树：很容易找到中点，但是可能会退化为线性。 平衡二叉树：保证不会退化为线性查找。因为数据库是存储在磁盘上的，但是一个节点只保存一条数据，而一条数据的数据量不足以占慢一个文件块，从而造成磁盘空间的浪费。并且当数据多的时候，树的高度非常高，而节点遍历在磁盘中的速度是很慢的。 B树：B树是线性数据结构与树的结合，节点内部是线性查找，节点外部树查找。使得单个节点可以保存更大的数据量，并且可以降低树的高度。并且单个数据的节点可以读取到内存中处理，及时是线性查找，速度也是很快的。但是B树的缺点是范围查找比较慢。 B+树：数据都存在于叶子节点中，树节点只是用于查找的指针，并且叶子节点之间用指针连接，形成一个有序的链表。B+树更适合数据库这种经常进行范围查找的场合。 InnoDB使用B+树作为索引的数据结构，B+树的高度一般为2-4层，每个节点可以放上万条数据，查找速度非常快。 生产环境中，需要尽量控制单表数据量的大小，建议控制在500万以内。B+树越高，磁盘IO越多，越影响查询效率。 InnoDB索引分为聚簇索引（主索引）和辅助索引。 主索引是将数据行与索引紧凑的存储在一起，即一个索引对应一个数据。主索引根据主键构造一个B+树，叶子节点存放主键值和行数据。根据主键我们很容易查询到主键所在行的数据。 辅助索引是根据索引字段构造一个B+树，叶子节点存放字段值和主键值。如果要通过辅助索引查找行数据，就需要回表，即先查找到主键值，在根据主键值去查找行数据。 图中是主索引对应的B+树。在InnoDB中，树节点叫做页（Page）。 图中可以看到InnoDB的主索引和数据都存放在叶子节点中。 辅助索引也是类似的B+树，只不过树节点判断的是辅助索引字段，叶子节点存放的是字段值和主键值。回表指的是通过辅助索引查询到了这一行的主键，然后再根据主键回去查主索引对应的B+树，找到对应行中的其他数据。 InnoDB的逻辑存储结构 表空间（tablespace）指的是数据表在硬盘上的存储空间。默认情况下，所有表的数据都存放在共享表空间，但是也可以将每个表的数据放在独占表空间（ibd文件）。 段（segment）包括数据段（存放B+树的叶子节点）和索引段（存放B+树的非叶子节点）。在InnoDB中，段由存储引擎自动管理。 区（extent），一个区包括64个页（page），每个页的默认大小是16KB。 页是InnoDB中磁盘读写的最小逻辑单位，一个数据页就是一个B+树节点，页的大小充分考虑了机械硬盘和SSD的最小读取单元（512B和4KB）。 行是数据真正存储的地方。 行溢出机制： 受限于页的容量，数据字段的大小也无法无限的增长，当数据字段过大时，InnoDB会使用行溢出机制。 数据字段过大导致一个页中的行记录数减少（退化成平衡二叉树），会降低检索的效率，所以行溢出机制会把超长字段取出放入单独开辟的数据页，这个单独开辟的数据页叫BLOB页。 下图是行溢出时InnoDB的逻辑存储结构： 一旦行溢出，不但要维护表，还用维护BLOB页，必然影响查询效率。 在生产环境中，为了避免行溢出： 把BLOB或是TEXT列分离到单独的扩展表中； 禁止在数据库中存储图片，文件等大的二进制数据。 InnoDB的行记录格式InnoDB的老旧的行记录格式：Redundant和Compact，最新的行记录格式：Dynamic和Compressed。 Redundant格式从左到右依次是： 字段偏移列表：记录每个字段的相对位置 header：列数量、字段偏移表的单位、下一行记录的指针等信息。 RowID（没有主键时作为隐藏主键）、TxID（事务ID）、RollPointer（回滚指针）。 Col1到ColN：列字段，分为未溢出和溢出两种情况。溢出时，Col存放字段的前768B数据和BLOB页指针。 Compact格式在Redundant格式基础上将字段偏移列表压缩为变长字段长度表和NULL标志位。 Dynamic格式在Compact格式基础上将Col的存储改为，当小于40B时表示未溢出，当溢出时，Col中直接存放BLOB页指针。Dynamic格式是现在最常用的格式。 Compressed格式在Dynamic格式基础上将数据行进行压缩（利用zlib算法）以进一步节省行所占的空间，但是对CPU压力较大。 行记录进化的核心需求是节约行记录空间，增加每个页的数据行数，提高查询效率。 为了增加每个页的数据行数，在生产环境中： 尽量做到冷热数据分离，减小表的宽度。冷热数据分离是将经常查的字段和不经常查的字段放到两张表中，使得热表的行数据尽量小，以提高查询效率。 优先选择符合存储需要的最小数据类型。 索引的左侧用法联合索引 联合索引也是辅助索引，联合索引是将两个或以上字段的联合作为一个索引。联合索引首选排序字段是最左侧字段，所以联合索引能加速“最左前缀”的查询。一个查询可以只使用索引中的一部份，但只能是最左侧部分。例如索引是a,b,c。 可以支持a | a,b| a,b,c 3种组合进行查找，但不支持b,c进行查找 字符串的前缀索引 如果字符串过长，可以考虑使用前缀索引节约空间。当我们用一个字符串字段作为辅助索引，会建立B+树，因为字段值是存储在B+树中，所以字符串过长会导致B+树占用很大空间。前缀索引只将字符串的前几个字符作为索引来构建B+树就能节省空间。比如，邮箱字段，后缀基本一样，就可以用前缀索引来节省空间。 如果前缀区分度太小，可以考虑两种变通方法：①倒序存储（适用于前几个字符很相似的情况），②新建hash字段（利用hash算法相字符串转换为hash值）。 12alter table user add index index2(email(6))-- 字符前6位作为索引 like模糊查询 like %关键字%和like %关键字这两种模糊查询会使索引失效，like 关键字%才可以使用索引。 InnoDB约束数据的方法Primary Key和Unique Key，主键（唯一且不为空）和唯一索引（唯一）。唯一约束在插入时的性能开销较大。 Foreign Key，外键。外键可以对数据的正确性实现约束。外键在数据修复时可能出问题，外键也会降低性能。 Default和NOT NULL，默认值和不为空。如果一个索引字段存在空值，其检索的效率就会降低。使用NOT NULL要在MySQL中开启严格模式innodb_strict_mode。 触发器。在插入或修改数据的时候，使用触发器来校验数据。但是触发器容易干扰业务，实际使用很少。 视图的使用使用视图可以创建不存在的虚拟表，视图的原理是预设一个SELECT语句，SELECT语句的查询结果作为虚拟表的数据。 视图算法： MERGE，将视图SQL合并到主查询SQL，即合并成一个SQL语句去查询基础表。 TEMPTABLE，将视图作为临时表（中间结果）来处理，即先查询基础表得到临时表，在查询临时表。 因为MERGE没有中间结果，一步到位，所以其性能优于TEMPTABLE。 12345CREATEALGORITHM&#x3D;MERGEVIEW &#96;bigpay&#96;ASSELECT * FROM payment WHERE amount &gt; 2; MERGE算法在有些情况是无法使用的，当查询语句包括聚集函数、DISTINCT、GROUP BY、HAVING、UNION和UNION ALL、子查询时，不支持MERGE算法。 尽量使用MERGE算法，而且需要避免无法使用MERGE的SQL。","categories":[{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"InnoDB","slug":"InnoDB","permalink":"http://wht6.github.io/tags/InnoDB/"},{"name":"MySQL","slug":"MySQL","permalink":"http://wht6.github.io/tags/MySQL/"}]},{"title":"Ansible实践","slug":"Ansible实践","date":"2022-04-05T09:00:00.000Z","updated":"2022-08-15T11:42:20.053Z","comments":true,"path":"posts/6d87.html","link":"","permalink":"http://wht6.github.io/posts/6d87.html","excerpt":"","text":"Docker集群搭建由于即没有主机又没有云上环境，只能用Docker的容器来模拟一个集群。 下载镜像 下载主控节点的镜像 1docker pull chusiang/ansible-jupyter:latest 下载被控节点的镜像 1docker pull chusiang/ansible-managed-node:latest 启动容器 启动主控节点容器 1docker run -p 8888:8888 -d --name manager e30 注：这里都是使用镜像id的简写 测试主控节点是否可用 先docker ps -a查看容器状态是否为Up。 然后访问http://localhost:8888，打开jupyter notebook。 新建一个文本，执行以下命令： 12!ansible localhost -m ping!ansible localhost -m setup 注：ctl+enter执行代码块。执行 Shell 指令需要在前面加上!。 输出SUCCESS或CHANGED证明节点可用。 启动被控节点容器 123docker run --name server1 -d -P b7cedocker run --name server2 -d -P b7cedocker run --name server3 -d -P b7ce 测试被控节点是否可用 先docker ps -a查看容器状态是否为Up。 修改主机的inventory文件/etc/ansible/hosts。 1234[container]server1 ansible_ssh_host=127.0.0.1 ansible_ssh_port=49153 ansible_ssh_pass=dockerserver2 ansible_ssh_host=127.0.0.1 ansible_ssh_port=49154 ansible_ssh_pass=dockerserver3 ansible_ssh_host=127.0.0.1 ansible_ssh_port=49155 ansible_ssh_pass=docker 三个容器的端口通过docker ps -a查看。 执行下面的命令： 1ansible container -m command -a &#x27;echo Hello World.&#x27; 都输出SUCCESS或CHANGED证明节点可用。 配置主控节点配置主控节点的Ansible，修改ansible.cfg 1234567[defaults]inventory = inventoryremote_user = dockerprivate_key_file = ~/.ssh/id_rsahost_key_checking = Falseretry_files_save_path = ./ansible-retry 修改主控节点Ansible的inventory。在这里添加了一个主机组，这个主机组中包含了三个主机，指定了三个主机的ip以及ssh的端口。其实这里还可以配置每个主机的登录用户名和登录密码，但是因为配置了密钥登录，所以这里不用配置密码，用户名可以在执行命令或者剧本的时候指定。（用户名密码登录没有密钥登录安全，建议使用密钥登录） 1234[myserver]server1 ansible_ssh_host=192.168.91.129 ansible_ssh_port=49153server2 ansible_ssh_host=192.168.91.129 ansible_ssh_port=49154server3 ansible_ssh_host=192.168.91.129 ansible_ssh_port=49155 新建一个文本，执行以下命令： 123!ansible myserver -m ping!ansible myserver -m setup!ansible all -m command -a &#x27;sudo cat /etc/sudoers&#x27; --become 输出SUCCESS或CHANGED证明主控节点和被控节点连接成功。 Ansible的两种操作方式Ad-Hoc方式Ad-Hoc方式即使用简短的指令来操作被控节点，又叫指令操作模式。 Ansible命令的基本语法格式 12ansible &lt;pattern_goes_here&gt; -m &lt;module_name&gt; -a &lt;arguments&gt;#ansible 匹配模式 -m 模块 -a &#x27;需要执行的内容&#x27; 匹配模式：即哪些机器生效 (可以是某一台, 或某一组, 或all)。 详细说明： 12345678910111213ansible &lt;host-pattern&gt; [options]常用选项：-m name, --module-name=name: 指定执行使用的模块-a MODULE_ARGS，--args=MODULE_ARGS:传递参数给模块--version:显示版本-b,--become:升级为sudo权限-h,--help:显示使用帮助-u username:指定远程主机运行此命令的用户-s, --sudo: 相当于linux系统下的sudo命令-v:查看执行的详细过程(-vv、-vvvv更详细)-k,--ask-pass:提示输入ssh连接密码，默认使用key验证-K,--ask-become-pass:提示执行输入sudo的密码 PlayBook方式PlayBook（剧本）方式即把所有任务写好然后一次性执行。Ad-Hoc和PlayBook的关系类似于Shell命令和Shell脚本的关系。PlayBook也是Ansible强大的体现。 PlayBook是用Yaml格式，简单易读。 Yaml的语法格式： 缩进: YAML使用一个固定的缩进风格表示层级结构,每个缩进由两个空格组成, 不能使用tabs;冒号: 以冒号结尾的除外，其他所有冒号后面所有必须有空格;短横线: 表示列表项，使用一个短横杠加一个空格。多个项使用同样的缩进级别作为同一列表; 在一份 Playbook 中，可以有多个 Play、多个 Task 和多个 Module。 Play ：通常为某个特定的目的，例如： Setup a official website with Drupal (使用Drupal 创建官网) Restart the API service (重开 API 服务) Task ：是要实行 Play 这个目地所需做的每个步骤，例如： Install the Nginx (安裝 Nginx) Kill the djnago process (强制停止 django 的进程) Module ：Ansible 所提供的各种操作方法，例如： apt: name=vim state=present (使用 apt 套件安装 vim) command: /sbin/shutdown -r now (使用 shutdown 的指令重新开机) PlayBook语法特性 以 —- (三个减号)开始，必须顶行写； 次行开始写Playbook的内容，但是一般要求写明该playbook的功能； 严格缩进，并且不能用Tab键缩进； 缩进级别必须是一致的，同样的缩进代表同样的级别，程序判别配置的级别是通过缩进结合换行来实现的； K/V的值可同行写，也可换行写。同行使用 :分隔，换行写需要以 - 分隔； PlayBook的基础组件 hosts：运行执行任务（task）的目标主机 remote_user：在远程主机上执行任务的用户 tasks：任务列表 handlers：任务，与tasks不同的是只有在接受到通知时才会被触发 templates：使用模板语言的文本文件，使用jinja2语法。 variables：变量，变量替换{{ variable_name }} 举例： 12345678910111213141516---- hosts: control-node # hosts，已经在inventory文件中定义好了，可以是单个主机或主机组 remote_user: root # remote_user用户身份 vars: # variables变量 - pkg: httpd tasks: # task任务列表 - name: &quot;install httpd package.&quot; # 任务名 yum: name=&#123;&#123; pkg &#125;&#125; state=installed # 模块 &#123;&#123; pkg &#125;&#125;表示变量的引用 - name: &quot;copy httpd configure file to remote host.&quot; # 任务名 copy: src=/root/conf/httpd.conf dest=/etc/httpd/conf/httpd.conf # 模块 notify: restart httpd #当这个任务执行状态发生改变时，触发handlers执行. - name: &quot;boot httpd service.&quot; # 任务名 service: name=httpd state=started # 模块 handlers: # handlers任务，与tasks是同一级别 - name: restart httpd # 任务名 service: name=httpd state=restarted # 模块 PlayBook的命令12345678910111213ansible-playbook [options] &lt;filename.yml&gt; [options]: 选项-C, --check:模拟执行，不会真正在机器上执行(查看执行会产生什么变化)，仅仅测试。-D, --diff:当更新的文件数及内容较少时，该选项可显示这些文件不同的地方-h, --help:打开帮助文档-i INVENTORY, --inventory-file=INVENTORY:指定要读取的Inventory清单文件--list-hosts:列出执行匹配到的主机，但并不会执行任何动作。--list-tags:列出所有可用的tags--list-tasks:列出所有即将被执行的任务-v, --verbose:执行详细输出-u REMOTE_USER, --user=REMOTE_USER:指定远程主机以USERNAME运行命令-s, --sudo:相当于Linux系统下的sudo命令 Ansible的常用基础模块命令模块ping模块主要用于判断远程客户端是否在线，这个ping并不是真的去发送ICMP包去测试网络是否联通，而是测试ssh是否可以远程登录。 1ansible 主机组名 -m ping command模块该模块为ansible的默认模块，支持所有远程权限范围内的命令，不支持管道符号|。 1ansible 主机组名 -m command -a &#x27;command&#x27; shell模块和command模块一样，且支持管道符号|。 1ansible 主机组名 -m shell -a &#x27;command&#x27; 文件模块copy模块从ansible主机上拷贝文件或目录到被管理机器上，类似scp功能。 123456789101112ansible 主机组名 -m copy -a &#x27;src=/root/test.sh dest=/root/ owner=root group=root mode=0755&#x27;参数说明:src ansible主机上的文件或目录dest 指定被管理机器的目录owner 指定文件或目录传过去的所有者group 指定文件或目录传过去的所属组mode 指定文件或目录传过去的权限其中：如果src是文件，dest是目录，则src会被拷贝至dest目录下，如果dest是文件，则会被src替换如果src是目录，dest也必须为目录，若dest不存在，则dest会被当成目录创建 file模块管理文件和目录以及属性。 123456789101112131415161718创建目录：ansible 主机组名 -m file -a &#x27;name=/data mode=644 state=directory&#x27;创建普通文件：ansible 主机组名 -m file -a &#x27;dest=/data/test.txt mode=777 state=touch&#x27;删除文件或目录：ansible 主机组名 -m file -a &#x27;path=/data/abc state=absent&#x27;创建一个软连接：ansible 主机组名 -m file -a &#x27;src=/data/test.txt dest=/root/test state=link&#x27;参数说明：name(dest|path) 指定目录或文件名mode 指定创建后文件或目录的权限state 指定类型: directory（如果目录不存在则创建该目录）、touch（创建普通文件）、absent（删除文件或目录）、link（创建软链接）owner 指定所属用户 group 指定所属组 script模块在被管理的机器上执行管理机器上的脚本，脚本不会被拷贝到被管理主机上。 1ansible 主机组名 -m script -a &#x27;/root/test.sh&#x27; fetch模块从被管理主机上拷贝文件到ansible主机，和copy模块用法类似。 12345ansible 主机组名 -m fetch -a &#x27;src=/root/test.sh dest=/root/&#x27;参数说明；src 被管理主机上的文件，必须是文件dest ansible主机存放的路径 安装模块yum模块管理被管理机器上的rpm包。 12345ansible 主机组名 -m yum -a &#x27;name=mysql state=installed&#x27;参数说明：name 指定要安装的rpm包名称state 指定类型:installed(present)：安装软件包、absent(removed)：卸载软件包 定时模块cron模块1234567ansible 主机组名 -m cron -a &quot;name=&#x27;check dirs&#x27; hour=&#x27;5,2&#x27; job=&#x27;ls -alh &gt; /dev/null&#x27;&quot;参数说明：name 指定时间任务名称（加备注）hour 指定时间任务周期(minute,hour,day,month,weekday)job 指定任务内容state 指定状态:absent(present):取消指定时间任务 挂载模块mount模块实现被管理机的分区挂载(添加至/etc/fstab文件)。 1234567ansible 主机组名 -m cron -a &quot;name=&#x27;check dirs&#x27; hour=&#x27;5,2&#x27; job=&#x27;ls -alh &gt; /dev/null&#x27;&quot;参数说明：name 指定时间任务名称（加备注）hour 指定时间任务周期(minute,hour,day,month,weekday)job 指定任务内容state 指定状态:absent(present):取消指定时间任务 服务模块service模块管理被管理机器上的服务（启动、停止、重启等）。 123456789ansible 主机组名 -m service -a &quot;name=nginx state=stopped&quot; 参数说明：name 指定服务名state 指定需要执行的操作stopped 停止started 启动restarted 重启reloaded 平滑重启 用户模块user模块管理被管理机器上的用户。 1234567891011ansible 主机组名 -m user -a &quot;name=ninmu shell=/bin/sh home=/home/munin group=8888 uid=8888 comment=&#x27;useradd ninmu&#x27;&quot;参数说明：name 指定用户名shell 指定shell类型home 指定用户家目录group 指定用户所属组（前提是指定的组已经存在）uid 指定用户uidcomment 指定用户描述项remove 删除用户时是否删除家目录（remove=yes）state 指定状态:absent:删除用户 解压缩模块unarchive模块将ansible主机上的归档文件或压缩文件释放至指定被管理机上。 12345678910111213将ansible主机上的压缩文件释放至被管理机：ansible 主机组名 -m unarchive -a &#x27;src=/root/test.tar.gz dest=/root/&#x27;释放被管理机器上的压缩文件：ansible 主机组名 -m unarchive -a &#x27;src=/root/local.tar.gz dest=/usr/src/ remote_src=yes&#x27;参数说明：src 指定要解压的压缩或归档文件名dest 指定要将文件释放到哪里owner 指定释放后文件所属用户group 指定释放后文件所属组mode 指定释放后文件权限remote_src 需要释放的文件在被管理机器端时使用(remote_src=yes) 更多其他模块参考：https://devdocs.io/ansible/ 使用Ansible实现批量文件上传和执行目的将被控端的Shell脚本文件上传到所有服务器中，并执行。 使用Ad-Hoc方式准备文件随便写一个简单的脚本dir10.sh，功能是循环创建10个文件夹。 12345678910vi dir10.sh# 写入以下内容#!/bin/bashnum_list=$(seq 10)for i in $num_list do name_string=&#x27;dir&#x27;$i&#x27;&#x27; mkdir $name_string done 上传文件执行命令 1!ansible myserver -m copy -a &#x27;src=/home/dir10.sh dest=/home/ owner=root group=root mode=0755&#x27; --become 下面是执行结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051server2 | CHANGED =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot; &#125;, &quot;changed&quot;: true, &quot;checksum&quot;: &quot;f86137fe86c8ea407b833790f9eedfcd5ec1bfbc&quot;, &quot;dest&quot;: &quot;/home/dir10.sh&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;md5sum&quot;: &quot;cea49f738ef1ee0c338d8f53f5f6b3ad&quot;, &quot;mode&quot;: &quot;0755&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;size&quot;: 117, &quot;src&quot;: &quot;/home/docker/.ansible/tmp/ansible-tmp-1649165589.2369564-501-255128743381511/source&quot;, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 0&#125;server1 | CHANGED =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot; &#125;, &quot;changed&quot;: true, &quot;checksum&quot;: &quot;f86137fe86c8ea407b833790f9eedfcd5ec1bfbc&quot;, &quot;dest&quot;: &quot;/home/dir10.sh&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;md5sum&quot;: &quot;cea49f738ef1ee0c338d8f53f5f6b3ad&quot;, &quot;mode&quot;: &quot;0755&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;size&quot;: 117, &quot;src&quot;: &quot;/home/docker/.ansible/tmp/ansible-tmp-1649165589.4284437-500-39731174249426/source&quot;, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 0&#125;server3 | CHANGED =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot; &#125;, &quot;changed&quot;: true, &quot;checksum&quot;: &quot;f86137fe86c8ea407b833790f9eedfcd5ec1bfbc&quot;, &quot;dest&quot;: &quot;/home/dir10.sh&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;md5sum&quot;: &quot;cea49f738ef1ee0c338d8f53f5f6b3ad&quot;, &quot;mode&quot;: &quot;0755&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;size&quot;: 117, &quot;src&quot;: &quot;/home/docker/.ansible/tmp/ansible-tmp-1649165589.4540727-503-56808426460747/source&quot;, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 0&#125; 进入容器验证文件上传成功 1234567docker exec -it server1 /bin/bashdocker exec -it server2 /bin/bashdocker exec -it server3 /bin/bash$ cd /home$ ls 执行文件1!ansible myserver -m command -a &#x27;bash /home/dir10.sh&#x27; --become 下面是执行结果： 123server3 | CHANGED | rc=0 &gt;&gt;server1 | CHANGED | rc=0 &gt;&gt;server2 | CHANGED | rc=0 &gt;&gt; 进入容器验证文件执行成功： 12345678docker exec -it server1 /bin/bashdocker exec -it server2 /bin/bashdocker exec -it server3 /bin/bash$ cd /home$ cd /docker$ ls 使用PlayBook方式为了区分两种方式，我将文件上传到server的/root下，然后执行，通过编写PlayBook可以将两个任务一起安装顺序执行。 设置密钥验证因为docker用户访问不了/root，所有需要将remote_user改为root。这样私钥的路径也需要更改为root下的私钥。这里我不使用镜像中自带的密钥，自己重新生成新的密钥对。 1ssh-keygen -t rsa 将公钥发送到三个被控端server： 123ssh-copy-id -i /root/.ssh/id_rsa.pub root@192.168.91.129 -p 49153ssh-copy-id -i /root/.ssh/id_rsa.pub root@192.168.91.129 -p 49154ssh-copy-id -i /root/.ssh/id_rsa.pub root@192.168.91.129 -p 49155 非对称密钥加密原理： 消息发送方A在本地构建密钥对，公钥和私钥； 消息发送方A将产生的公钥发送给消息接收方B； B向A发送数据时，通过公钥进行加密，A接收到数据后通过私钥进行解密，完成一次通信； 反之，A向B发送数据时，通过私钥对数据进行加密，B接收到数据后通过公钥进行解密。 重新配置ansible.cfg。 12345inventory = inventoryremote_user = rootprivate_key_file = /root/.ssh/id_rsahost_key_checking = Falseretry_files_save_path = ./ansible-retry 测试是否联通： 12!ansible myserver -m ping -o# -o的作用是简洁输出 编写PlayBookPlayBook包含两个任务，上传拷贝任务和执行脚本任务。 123456789101112131415---- name: my playbook v1. hosts: myserver remote_user: root become: yes tasks: - name: &quot;copy dir&quot; copy: src: &quot;/home/dir10.sh&quot; dest: &quot;/root/&quot; owner: root group: root mode: 0755 - name: &quot;run sh&quot; command: bash /root/dir10.sh 一键执行先验证PlayBook不执行。 1!ansible-playbook -C mkdir10.yaml 验证成功，结果如下： 123456789101112131415161718192021PLAY [my playbook v1.] *********************************************************TASK [Gathering Facts] *********************************************************ok: [server2]ok: [server3]ok: [server1]TASK [copy dir] ****************************************************************changed: [server3]changed: [server2]changed: [server1]TASK [run sh] ******************************************************************skipping: [server1]skipping: [server2]skipping: [server3]PLAY RECAP *********************************************************************server1 : ok=2 changed=1 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 server2 : ok=2 changed=1 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 server3 : ok=2 changed=1 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 执行PlayBook。 1!ansible-playbook -v mkdir10.yaml 执行成功，结果如下： 1234567891011121314151617181920212223Using /home/ansible.cfg as config filePLAY [my playbook v1.] *********************************************************TASK [Gathering Facts] *********************************************************ok: [server1]ok: [server2]ok: [server3]TASK [copy dir] ****************************************************************changed: [server3] =&gt; &#123;&quot;changed&quot;: true, &quot;checksum&quot;: &quot;f86137fe86c8ea407b833790f9eedfcd5ec1bfbc&quot;, &quot;dest&quot;: &quot;/root/dir10.sh&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;md5sum&quot;: &quot;cea49f738ef1ee0c338d8f53f5f6b3ad&quot;, &quot;mode&quot;: &quot;0755&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;size&quot;: 117, &quot;src&quot;: &quot;/root/.ansible/tmp/ansible-tmp-1649210480.499055-1796-72406851272976/source&quot;, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 0&#125;changed: [server1] =&gt; &#123;&quot;changed&quot;: true, &quot;checksum&quot;: &quot;f86137fe86c8ea407b833790f9eedfcd5ec1bfbc&quot;, &quot;dest&quot;: &quot;/root/dir10.sh&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;md5sum&quot;: &quot;cea49f738ef1ee0c338d8f53f5f6b3ad&quot;, &quot;mode&quot;: &quot;0755&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;size&quot;: 117, &quot;src&quot;: &quot;/root/.ansible/tmp/ansible-tmp-1649210480.4550297-1793-199689558249035/source&quot;, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 0&#125;changed: [server2] =&gt; &#123;&quot;changed&quot;: true, &quot;checksum&quot;: &quot;f86137fe86c8ea407b833790f9eedfcd5ec1bfbc&quot;, &quot;dest&quot;: &quot;/root/dir10.sh&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;md5sum&quot;: &quot;cea49f738ef1ee0c338d8f53f5f6b3ad&quot;, &quot;mode&quot;: &quot;0755&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;size&quot;: 117, &quot;src&quot;: &quot;/root/.ansible/tmp/ansible-tmp-1649210480.4890015-1795-121241766203217/source&quot;, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 0&#125;TASK [run sh] ******************************************************************changed: [server2] =&gt; &#123;&quot;changed&quot;: true, &quot;cmd&quot;: [&quot;bash&quot;, &quot;/root/dir10.sh&quot;], &quot;delta&quot;: &quot;0:00:02.579533&quot;, &quot;end&quot;: &quot;2022-04-06 02:01:25.443510&quot;, &quot;rc&quot;: 0, &quot;start&quot;: &quot;2022-04-06 02:01:22.863977&quot;, &quot;stderr&quot;: &quot;&quot;, &quot;stderr_lines&quot;: [], &quot;stdout&quot;: &quot;&quot;, &quot;stdout_lines&quot;: []&#125;changed: [server3] =&gt; &#123;&quot;changed&quot;: true, &quot;cmd&quot;: [&quot;bash&quot;, &quot;/root/dir10.sh&quot;], &quot;delta&quot;: &quot;0:00:02.669678&quot;, &quot;end&quot;: &quot;2022-04-06 02:01:25.540936&quot;, &quot;rc&quot;: 0, &quot;start&quot;: &quot;2022-04-06 02:01:22.871258&quot;, &quot;stderr&quot;: &quot;&quot;, &quot;stderr_lines&quot;: [], &quot;stdout&quot;: &quot;&quot;, &quot;stdout_lines&quot;: []&#125;changed: [server1] =&gt; &#123;&quot;changed&quot;: true, &quot;cmd&quot;: [&quot;bash&quot;, &quot;/root/dir10.sh&quot;], &quot;delta&quot;: &quot;0:00:02.621975&quot;, &quot;end&quot;: &quot;2022-04-06 02:01:25.491630&quot;, &quot;rc&quot;: 0, &quot;start&quot;: &quot;2022-04-06 02:01:22.869655&quot;, &quot;stderr&quot;: &quot;&quot;, &quot;stderr_lines&quot;: [], &quot;stdout&quot;: &quot;&quot;, &quot;stdout_lines&quot;: []&#125;PLAY RECAP *********************************************************************server1 : ok=3 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 server2 : ok=3 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 server3 : ok=3 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 进入容器验证文件执行成功： 1234567docker exec -it server1 /bin/bashdocker exec -it server2 /bin/bashdocker exec -it server3 /bin/bash$ cd /root$ ls &nbsp; 参考链接： https://docs.ansible.com/ansible/latest/index.html https://www.w3cschool.cn/automate_with_ansible/automate_with_ansible-db6727oq.html https://cloud.tencent.com/developer/article/1660856 https://juejin.cn/post/6844903631066513421 http://www.manongjc.com/detail/26-tdonzgunxqptyyk.html https://www.cnblogs.com/Confusedren/p/11149843.html http://www.manongjc.com/detail/25-igwutsgqyzkkarn.html http://www.manongjc.com/detail/21-rncagpndgyrhvwf.html","categories":[{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"http://wht6.github.io/tags/Ansible/"}]},{"title":"初识Ansible","slug":"初识Ansible","date":"2022-04-04T09:00:00.000Z","updated":"2022-08-13T01:46:12.499Z","comments":true,"path":"posts/42ee.html","link":"","permalink":"http://wht6.github.io/posts/42ee.html","excerpt":"","text":"01|Ansible介绍什么是Ansible？Ansible 是一个基于python 开发的IaC工具，也叫自动化运维工具，类似的工具还有Chef、Saltstack、Puppet等。 基础架构即代码（IaC）是通过代码（而非手动流程）来管理和置备基础架构的方法。 Ansible的特性： 基于ssh远程连接服务，ssh相当于ansible客户端。所以不用帮每台机器 (instance) 预载 agent ，只要机器上有 SSH 和 Python，就能够对其进行配置和管理。 Ansible通过调用相关模块，完成指定任务，并且支持任何语言编写的自定义模块。 通过编写剧本（playbook），可根据需要一次执行完剧本中的所有任务或某些任务。 Ansible是怎么运作的？Ansible通过 inventory 档案来定义有哪些 Managed node (被控端)，并借由 SSH 和 Python 进行沟通。 当 Control Machine (主控端) 可以用 SSH 连上 Managed node，且被连上的机器里有预载 Python 时，Ansible 就可以运作了。 Control Machine 指的是我们主要会在上面操作 Ansible 的机器，即主控端。它可以是我们平时用的电脑、手机 或机房里的某一台机器。 Managed node 则是被 Ansible 操纵的机器，即被控端，生产环境中通常是Server。 如何安装Ansible？首先机器上预装Python 2.5+和SSH。 Ubuntu： 123apt-get install -y python-software-properties software-properties-commonadd-apt-repository -y ppa:ansible/ansible; sudo apt-get updateapt-get install -y ansible CentOS: 12yum install -y epel-releaseyum install -y ansible Pip: 12pip install -U pippip install ansible 02|Ansible配置ansible配置/etc/ansible/ansible.cfg是Ansible的配置文件。 123456789inventory = /etc/ansible/hosts #inventory文件的位置library = /usr/share/ansible #指向存放Ansible模块的目录，支持多个目录方式，只要用冒号（：）隔开就可以forks = 5 #并发连接数，默认为5sudo_user = root #设置默认执行命令的用户remote_user = root #被控端的用户名。注意如果不指定, /usr/bin/ansible默认使用当前用户名称remote_port = 22 #指定连接被管节点的管理端口，默认为22端口，建议修改，能够更加安全host_key_checking = False #设置是否检查SSH主机的密钥，值为True/False。关闭后第一次连接不会提示配置实例timeout = 60 #设置SSH连接的超时时间，单位为秒log_path = /var/log/ansible.log #指定一个存储ansible日志的文件（默认不记录日志） inventory配置inventory 是什么？ 可以把inventory当成是一份主机列表，我们可通过它对定义每个 Managed Node 的代号、IP 位址、连接信息和群组。 /etc/ansible/hosts是Ansible的inventory文件。 12345678# [groupname]表示群组# ansible_ssh_host表示远端SSH主机地址# ansible_ssh_port表示远端主机SSH端口# ansible_ssh_user表示远端SSH的用户名# ansible_ssh_private_key_file表示本机SSH私钥路径# ansible_ssh_pass表示远端SSH密码（建议改用私钥）# ansible_connection表示以何种模式连接远程主机，比如:local, ssh 或者 paramiko。默认值是smart，表示当本地ssh支持持久连接(controlpersist)时采用ssh连接，否则采用python的paramiko ssh连接。 本机测试ansible向/etc/ansible/ansible.cfg写入 123456[defaults]inventory = /etc/ansible/hosts library = /usr/share/ansible sudo_user = root remote_user = root log_path = /var/log/ansible.log 向/etc/ansible/hosts写入 1234$ cat &gt;&gt; /etc/ansible/hosts &lt;&lt;EOF&gt;[local]&gt;localhost ansible_connection=local&gt;EOF 通过Ansible操控本机localhost打印hello world： 12ansible localhost -m command -a &#x27;echo Hello World.&#x27;# localhost表示Ansible要对本机进行操控，-m指定模块，command模块的作用是“远程执行shell命令” &nbsp; 参考链接： http://www.manongjc.com/detail/23-ysipdjdqrpbsccr.html https://juejin.cn/post/6844903631066513421 https://segmentfault.com/a/1190000038230424","categories":[{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"http://wht6.github.io/tags/Ansible/"}]},{"title":"Sed命令详解","slug":"Sed命令详解","date":"2022-04-02T08:00:00.000Z","updated":"2022-04-24T00:17:29.508Z","comments":true,"path":"posts/5117.html","link":"","permalink":"http://wht6.github.io/posts/5117.html","excerpt":"","text":"01|sed介绍什么是sed？sed是一种非交互式的编辑器，使用者只能在命令行编辑命令来对文本进行修改，默认情况下，所有的输出行都被打印到屏幕上。 sed的特点 在sed命令处理数据之前，需要预先提供一组规则，sed命令会按照这个规则来编辑数据。 每次仅读取一行数据，根据提供的规则命令对这一行数据匹配并修改完成后才会读取下一行数据。 sed默认不会直接修改源文件的数据，而是会将数据复制到缓冲区中，在缓冲区中修改数据 sed的命令格式1sed [选项] ‘command’ [文件名] 02|sed使用替换格式：s/pattern/replacement/flags pattern 指的是需要替换的内容，replacement 指的是要替换的新内容。flags为标记。 flags 标记 功能 n 1~512 之间的数字，表示指定要替换的字符串出现第几次时才进行替换，例如，一行中有 3 个 A，但用户只想替换第二个 A，这是就用到这个标记； g 对数据中所有匹配到的内容进行替换，如果没有 g，则只会在第一次匹配成功时做替换操作。例如，一行数据中有 3 个 A，则只会替换第一个 A； p 会打印与替换命令中指定的模式匹配的行。此标记通常与 -n 选项一起使用。 w file 将缓冲区中的内容写到指定的 file 文件中； 普通用法： 1234567891011121314sed &#x27;s/test/trial/2&#x27; data.txt# 使用数字 2 作为标记的结果就是，sed 编辑器只替换每行中第 2 次出现的匹配模式。sed &#x27;s/test/trial/g&#x27; data.txt# 替换所有匹配的字符串sed -n &#x27;s/test/trial/p&#x27; data.txt# -n 选项会禁止 sed 输出，但 p 标记会输出修改过的行，将二者匹配使用的效果就是只输出被替换命令修改过的行sed &#x27;s/test/trial/w test.txt&#x27; data.txt# w 标记会将匹配后的结果保存到指定文件中sed &#x27;s/\\/bin\\/bash/\\/bin\\/csh/&#x27; /etc/passwd# 遇到特殊字符转义 正则表达式用法： s/[正则表达式]/replacement/，通过正则表达式查找需要替换的内容 123456sed &#x27;s/^west/north/&#x27; ceshi.txt# 找到以west开头的行，并将west替换成northsed &#x27;s/[0-9][0-9]$/&amp;.5/&#x27; ceshi.txt# &amp;表示保存查找串以便在替换串中引用# 在两位数后面加上.5 定位行默认情况下，sed 命令会作用于文本数据的所有行。 如果只想将命令作用于特定行或某些行，则必须写明 address 部分。即 12345[address]command或[address]&#123; 多个command &#125; address的表示的方法有以下 2 种： 以数字形式指定行区间； 用文本模式指定具体行区间。 以数字形式指定行区间： 123456789sed &#x27;2s/dog/cat/&#x27; data.txt# 只修改地址指定的第二行的文本sed &#x27;2,3s/dog/cat/&#x27; data.txt# 修改某个地址区间sed &#x27;2,$s/dog/cat/&#x27; data.txt# $表示最后一行 用文本模式指定具体行区间： 普通用法：/[文本]/ 12345sed &#x27;/demo/s/bash/csh/&#x27; /etc/passwd# 相当于先grep demo /etc/passwd，在执行替换操作sed -n &#x27;/west/,/east/p&#x27; ceshi.txt# 打印west所在行与east所在行之间的行 正则表达式用法：/[正则表达式]/或\\c[正则表达式]c（c可以是任意字符） 123456sed -r &#x27;/^My/s/food/rice/&#x27; data.txt# 所有My开头的行，执行替换操作# 使用正则表达是加-rsed -r &#x27;/(^#|^$)/d&#x27; nginx.conf# 删除#开头的行和空行 删除行格式：[address]d 因为默认删除所有行，所以通常指定address 12345sed &#x27;3d&#x27; data6.txt# 删除第3行sed -r &#x27;/(^#|^$)/d&#x27; nginx.conf# 删除#开头的行和空行 插入行格式： [address]i\\，行前插入。 [address]a\\，行后插入。 12345sed &#x27;2i\\then....&#x27; data.txt# 第二行前面插入sed &#x27;2a\\then....&#x27; data.txt# 第二行后面插入 打印（查看）格式：[address]p，打印指定行 默认会打印所有输入行，通常结合-n来抑制输入行，只打印输出行。 1234sed -n &#x27;3p&#x27; data.txt# 只打印第三行sed -n &#x27;/north/p&#x27; data.txt# 打印north所在的行 修改行格式：[address]c [文本]，将指定行修改为文本 12sed &#x27;3c ---&#x27; data.txt# 将第3行修改为--- 获取下一行通常会与其他命令结合使用 格式：[address]&#123;n;command&#125;，操作指定行的下一行 12sed &#x27;/god/&#123;n;d&#125;&#x27; data.txt# 将god所在行的下一行删除 多个单字符替换格式：[address]y/ABC/abc/，将A替换成a，B替换成b，c替换成c 将输出写入文件方法一： 格式：[address]w filename，将输出指定行，并将其写入指定文件filename 方法二： 格式：-i，直接修改原文件，且不在屏幕上输出。 将文件1的内容插入格式：[address]r file1，将file1的内容插入 12sed &#x27;$r data1.txt&#x27; data2.txt# 将data1的内容插入data2的末尾 实现多重命令方法一： 通过-e连接多条命令，多个 command 之间，是按照在命令中的先后顺序来执行的。 12sed -n -e ‘1,2p’ -e ‘4p’ mysed.txt# 先打印1,2行，再打印4行 方法二： 先多条命令写入文件，每行一条命令，再通过-f从文件中读取命令 123456# 文件sed.txt1ds/oh/ho/w sedr.txtsed -f sed.txt data.txt 03| 生产环境实践 删除文本中所有行开头的空格 1sed &#x27;s/^[ ]*//&#x27; data.txt 删除文中所有行开头的数字 1sed &#x27;s/^[0-9][0-9]*//g&#x27; sed.txt 删除文字两边的单引号 12sed &quot;s/&#x27;//g&quot;# 有特殊字符或转义字符时需要用双引号括起来 &nbsp; 参考链接： https://www.runoob.com/linux/linux-comm-sed.html https://www.jianshu.com/p/3270b94cdc38 http://www.manongjc.com/detail/25-srmitkdnxlpsuzk.html","categories":[{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://wht6.github.io/tags/Shell/"}]},{"title":"Shell编程实践","slug":"Shell实践模板","date":"2022-03-29T08:00:00.000Z","updated":"2022-09-16T06:34:32.417Z","comments":true,"path":"posts/1cf9.html","link":"","permalink":"http://wht6.github.io/posts/1cf9.html","excerpt":"","text":"文本数据统计 命令统计Nginx日志access.log（目录是/var/usr/log）中访问量最多的3个ip 1awk &#x27;&#123;print $1&#125;&#x27; /var/log/nginx/access.log |sort | uniq -c |sort -nr -k1 | head -n 3 拆解： awk &#39;&#123;print $1&#125;&#39; /var/log/nginx/access.log : 提取日志中的第一列（即ip） sort：排序（目的是将一样的行放到一起） uniq -c：删除重复的行，-c是在行首加上重复行的次数 sort -nr -k1：-nr按数字从大到小逆序排序；-k1按照第一列排序（此处未指定分隔符，默认对整行排序） head -n 3：查看前3行 统计主机tcp的链接数量，并按链接状态分类 1netstat -ant | awk &#x27;/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;&#x27; 拆解： netstat -ant：列出所有tcp端口详情，数字代替名称 /^tcp/：匹配tcp开头的行 $NF：一行的最后一个字段（最后一个字段是State，即链接状态） S[$NF]：数组名为S，数组的索引是$NF，因为$NF不是数字，所以这个数组其实是个字典 ++S[$NF]：给索引为$NF的数组值加1 END &#123;for(a in S) print a, S[a]&#125;：for循环依次打印数组的索引和对应的值 用户登录时显示最近两天登录的用户名。在.bashrc中添加如下命令。 123456echo &quot;Recently user login:&quot;s1=$(last | grep &quot; $(date +%d) &quot; | awk &#x27;&#123;print $1&#125;&#x27;)lastd=$[$(date +%d)-1]s2=$(last | grep &quot; $lastd &quot; | awk &#x27;&#123;print $1&#125;&#x27;)s3=&quot;$s1\\n$s2&quot;echo -e $s3 | sed &#x27;s/ /\\n/g&#x27; | sort | uniq 拆解： last：查看最近用户登录情况。 date +%d：显示当前的“日”。 awk &#39;&#123;print $1&#125;&#39;：打印第一列（用户名）。 $[$(date +%d)-1]：数学运算（“日”-1）。 sed &#39;s/ /\\n/g&#39;：将空格替换为换行符。 | sort | uniq：排序加去重。 截取文本中的域名，并统计重复域名出现的次数 1）使用cut命令 1cat b.txt | cut -d &quot;/&quot; -f 3 | sort | uniq -c | sort -nr 拆解： cut -d &quot;/&quot; -f 3：-d指定分隔符，-f选中第三列。 sort：排序（目的是将一样的行放到一起） uniq -c：删除重复的行，-c是在行首加上重复行的次数。 sort -nr：按照数值，从大到小逆序排序。 2）使用sed命令 1sed -e &#x27;s/http:\\/\\///&#x27; -e &#x27;s/\\/.*//&#x27; b.txt | sort | uniq -c | sort -nr 拆解： -e &#39;s/http:\\/\\///&#39;：-e多次编辑无需管道符，将http://替换为空。 -e &#39;s/\\/.*//&#39;：-e多次编辑无需管道符，将/及后面任意多个字符替换为空。 3）使用awk命令 12awk -F/ &#x27;&#123;print $3&#125;&#x27; b.txt | sort | uniq -c | sort -nr# -F指定分隔符 统计当前服务器正在连接的IP地址，并按照连接数排序。 1netstat -an | grep ESTABLISHED | awk &#x27;&#123;print $5&#125;&#x27; | cut -d &quot;:&quot; -f 1 | sort | uniq -c | sort -nr 拆解： awk &#39;&#123;print $5&#125;&#39;：中间空格数不确定，cut不好指定分隔符，所以用awk。 对文件中的第一列数字，求和 12awk &#x27;&#123;sum += $1&#125;END&#123;print sum&#125;&#x27; a.txt# sum为新建变量，$1表示第一列，END后处理 在每行后面添加一个字符 12sed &#x27;s/$/ABC/&#x27; a.txt# $表示行末尾符号 文件操作 将/usr/local/bak下大于1M的文件移到/tmp下 123456#! /bin/bashcd /usr/local/bakfor filename in `ls -l | awk &#x27;$5&gt;1024*1024&#123;print $9&#125;&#x27;` do mv $filename /tmp done 拆解： ls -l | awk &#39;$5&gt;1024*1024&#123;print $9&#125;&#39;：显示文件详情；从所有行中找出第5个字段大于1024*1024的行，并提取第9个字段。 注：文件详情：[类型和权限] [子目录个数/文件链接数] [属主] [属组] [文件大小] [月] [日] [时间] [文件名] mv $filename /tmp ： 移动文件 使用循环在/xxx目录下创建10个文件，要求文件名由6个随机字母或数字加一个固定的字符串组成。 123456789101112#!/bin/bashif[ ! -d /xxx ]then mkdir /xxxficd /xxxfor ((i=0;i&lt;10;i++))do filename=$(tr -dc &#x27;A-Za-z0-9&#x27; &lt; /dev/urandom | head -c 6) touch &quot;$filename&quot;_gg.txtdone 拆解： ! -d /xxx：判断/xxx目录是否存在。 tr -dc &#39;A-Za-z0-9&#39;：-d删除字符集中的字符，-c是取字符集的反，作用是提取字符集中存在的字符。 &lt; /dev/urandom：输入重定向，/dev/urandom是一个随机字符串生成设备文件，可以生成随机字符串。 head -c 6：取出开头的6个字符，-c字符。 生成1000以内的随机数 1echo $(($RANDOM%1000)) 拆解： $RANDOM：这是一个系统变量，可随机生成0-32767的数字。 进程相关 关闭所有与redis相关的进程 1234cmd: ps -ef|grep -i redis |grep -v grep &gt; /root/redispid.txt cmd: awk &#x27;&#123;print $2&#125;&#x27; redispid.txt |xargs kill -9 拆解： ps -ef：以标准格式查看进程。 grep -i redis：查找输出带有redis的行，不区分大小写。 grep -v grep：不显示带有grep的行。 &gt; /root/redispid.txt：写入文件 awk &#39;&#123;print $2&#125;&#39; redispid.txt：获取数据的第二列（PID）。 |xargs kill -9：将参数通过管道传递给kill -9。 内存 每隔1分钟监控⼀次内存,当你的可⽤内存低于100m,发邮件报警,要求显示剩余内存 1234567891011#!/usr/bin/bashHostName=$(hostname)_$(hostname -i)Date=$(date +%F)while true;do Free=$(free -m|awk &#x27;/^Mem/&#123;print $NF&#125;&#x27;) if [ $Free -le 100 ];then echo &quot;$Date: $HostName Mem Is &lt; $&#123;Free&#125;MB&quot; | mailx -v -s &quot;admin1&quot; xxxx@qq.com fi sleep 60done 拆解： date +%F：完整日期格式（当前日期）。 free -m：查看内存，以Mb为单位。 awk &#39;/^Mem/&#123;print $NF&#125;&#39;：查找Mem开头的行，打印最后一个字段。 [ $Free -le 100 ]：比较大小。 用户登录时显示显存的占用。在/etc/profile.d目录下创建一个shell脚本。 1234567891011#!/bin/bashmenuse=$(nvidia-smi --query | grep &quot;Used G&quot; | awk &#x27;&#123;print $5&#125;&#x27;)if [ $menuse ];then if (($menuse &gt; 10000)) then echo &quot;GPU正在被使用，显存占用：$menuse MB!&quot; echo &quot;GPU正在被使用，显存占用：$menuse MB!&quot; echo &quot;GPU正在被使用，显存占用：$menuse MB!&quot; fifi 拆解： nvidia-smi --query：打印GPU详情。 包管理 找出报错中缺少的包名，并依次安装 12345678#!/bin/bashi=1for package in $(cat warning.txt | grep &quot;dpkg: warning: files list file for package &quot; | grep -Eo &quot;&#x27;[^&#x27;]*&#x27;&quot; | sed &quot;s/&#x27;//g&quot;)；do echo &quot;No.$&#123;i&#125; ==================start intall $&#123;package&#125;===================&quot; apt-get install --reinstall &quot;$package&quot; -y; i=`expr $i + 1`done 拆解： grep -Eo &quot;&#39;[^&#39;]*&#39;&quot;：-E表示支持正则表达式，-o表示只显示被模式匹配到的字符串，[^&#39;]*标识任意个不包含单引号的字符。 sed &quot;s/&#39;//g&quot;：将单引号替换为空（涉及到特殊字符用双引号括起来） 权限 检查系统中有无新增的SUID和SGID文件。 首先，提前备份系统自带的SUID和SGID文件的文件名列表。 1find / -perm -4000 -o -perm -2000 &gt;&gt; /tmp/sugid.list 拆解： /表示在根下查找 -perm表示匹配权限-4000 -2000 -1000分别表示SUID SGID SBIT -o表示or，-a表示and。 检测脚本： 123456789101112131415#!/bin/bashfind / -perm -4000 -o -perm -2000 &gt; /tmp/sugid.checkfor file in `cat /tmp/sugid.check`do grep $file /tmp/sugid.list &gt; /dev/null if [ &quot;$?&quot; != &quot;0&quot;] then echo &quot;$file isn&#x27;t in the list!!&quot; exit 0 fidonerm -rf /tmp/sugid.checkecho &quot;no new suid or sgid file was added.&quot; 拆解： grep $file /tmp/sugid.list表示在文件中查询file变量的值 $?表示上一指令的返回值，成功是0，不成功是1 检测 检查数组中的网站是否访问正常。 12345678910111213141516171819202122232425#!/bin/bashweb=(http://www.baidu.comhttp://www.souhu.comhttp://www.wht.com)for i in $&#123;web[*]&#125;do code=$(curl -o /dev/null -s --connect-timeout 5 -w &#x27;%&#123;http_code&#125;&#x27; $i | grep -E &quot;200|302&quot;) if [ &quot;$code&quot;!=&quot;&quot;] then echo &quot;$i is OK!&quot; &gt;&gt; /root/ok.log else sleep 10 code=$(curl -o /dev/null -s --connect-timeout 5 -w &#x27;%&#123;http_code&#125;&#x27; $i | grep -E &quot;200|302&quot;) if [ &quot;$code&quot;!=&quot;&quot;] then echo &quot;$i is OK!&quot; &gt;&gt; /root/ok.log else echo &quot;$i is ERROR!&quot; &gt;&gt; /root/error.log fi fidone 拆解： curl -o /dev/null -s：-o是输出，把输出放入回收站，-s是静默模式，不输出错误信息和进度信息。 --connect-timeout 5 -w &#39;%&#123;http_code&#125;&#39;：超时时间是5s，-w表示追加内容，追加变量http_code的值。 grep -E：查找，-E表示支持正则。","categories":[{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://wht6.github.io/tags/Shell/"}]},{"title":"Shell脚本复习","slug":"shell脚本复习","date":"2022-03-23T08:00:00.000Z","updated":"2022-08-09T04:47:35.188Z","comments":true,"path":"posts/a2bb.html","link":"","permalink":"http://wht6.github.io/posts/a2bb.html","excerpt":"","text":"基础复习代码规范 脚本第一行 123456#!/usr/bin/env bash # 通过/usr/bin/env运行程序，用户不需要去寻找程序在系统中的位置,，只要程序在你的$PATH中；#!/usr/bin/bash# 指定程序在系统中的具体位置的方式usr/bin/bash，在某些情况下更安全，因为它限制了代码注入的可能；#!/bin/bash# 在一些系统上/usr/bin/bash可能没有，而/bin/bash则一定存在的，所以/bin/bash是显示指定的优先选择。 基本信息描述 12345# Author：作者# Desc：描述# Usage：用法# Update：更新时间# Release：发布版本 基本数据类型Bash中基本数据类型只有字符串类型，连数值类型都没有(declare -i可强制声明数值类型)。 变量赋值和引用变量12345a=3echo $aa=&#x27;hello world&#x27;echo $a 变量替换和命令替换12a=&quot;hello&quot;echo $a world 取得变量a的值hello，并将它替换到命令行的$a处. 123echo `id root`echo $(id root)rm -rf $(ls) 命令的输出结果替换到$()或反引号位置处。 算术运算$[]或$(())或let命令可以做简单的整数算术运算。 let是单独的命令，不能写在其它命令行中。 123a=3let a=a+1echo $a $[]和$(())可以写在命令行内部，Shell在解析命令行的时候，会对它们做算术运算，然后将运算结果替换到命令行中。 1234567a=33echo $[a+3]echo $((a+3))a=333echo $[$a+3]echo $(($a+3)) 如果要进行小数运算需要借助计算工具bc或者利用awk来计算。 123a=1.1b=1.2c=`echo &quot;scale=3;$a + $b&quot;|bc` 退出状态码每个命令执行后都会有对应的进程退出状态码，用来表示该进程是否是正常退出。 所以，在命令行中，在Shell脚本中，经常会使用特殊变量$?判断最近一个前台命令是否正常退出。 通常情况下，如果$?的值： 为0，表示进程成功执行，即正常退出 非0，表示进程未成功执行，即非正常退出 但非0退出状态码并不一定表示错误，也可能是正常逻辑的退出 另外，在Shell脚本中，所有条件判断(比如if语句、while语句)都以0退出状态码表示True，以非0退出状态码为False。 重定向在Linux系统中，每个程序默认都会打开三个文件描述符(file descriptor,fd)： fd=0：标准输入，表示程序默认从哪里读取数据 fd=1：标准输出，表示程序默认将数据输出到哪里 fd=2：标准错误，表示程序默认将错误信息输出到哪里 文件描述符，说白了就是系统为了跟踪打开的文件而分配给它的一个数字，这个数字和文件有对应关系：从文件描述符读取数据，即表示从对应的文件中读取数据，向文件描述符写数据，即表示向对应文件中写入数据。 Linux中万物皆文件，文件描述符也是文件。默认： fd=0的标准输入是/dev/stdin文件 fd=1的标准输出是/dev/stdout文件 fd=2的标准错误是/dev/stderr文件 这些文件默认又是各个终端的软链接文件： 12345678910$ ls -l /dev/std*lrwxrwxrwx 1 root root 15 Jan 8 20:26 /dev/stderr -&gt; /proc/self/fd/2lrwxrwxrwx 1 root root 15 Jan 8 20:26 /dev/stdin -&gt; /proc/self/fd/0lrwxrwxrwx 1 root root 15 Jan 8 20:26 /dev/stdout -&gt; /proc/self/fd/1$ ls -l /proc/self/fd/lrwx------ 1 root root 64 Jan 16 10:40 0 -&gt; /dev/pts/0lrwx------ 1 root root 64 Jan 16 10:40 1 -&gt; /dev/pts/0lrwx------ 1 root root 64 Jan 16 10:40 2 -&gt; /dev/pts/0lr-x------ 1 root root 64 Jan 16 10:40 3 -&gt; /proc/75220/fd 所以，默认情况下读写数据都是终端。 改变文件描述符对应的目标，可以改变数据的流向。比如标准输出fd=1默认流向是终端设备，若将其改为/tmp/a.log，便能让数据写入/tmp/a.log文件中而不再是终端设备中。 在Shell中，这种改变文件描述符目标的行为称为重定向，即重新确定数据的流向。 其实，文件描述符有很多类操作，包括fd的重定向、fd的分配(open，即打开文件)、fd复制(duplicate)、fd的移动(move)、fd的关闭(close)。现在只介绍基础重定向操作。 Shell中，基础重定向操作有以下几种方式： [n]&gt;file：覆盖式输出重定向，输出到fd=n的数据改变流向输出到file文件中，file不存在则创建，file存在则先清空再写入数据 省略n时&gt;file，等价于1&gt;file，即标准输出覆盖重定向到file文件中 [n]&gt;&gt;file：追加式输出重定向，输出到fd=n的数据改变流向输出到file文件的尾部，file不存在则创建，file存在则直接追加在文件尾部 省略n时&gt;&gt;file，等价于1&gt;&gt;file，即标准输出追加重定向到file文件中 [n]&lt;file：输入重定向，以读取模式打开file文件并分配fd=n，file不存在则报错 省略n时&lt;file，等价于0&lt;file，即直接从file中读数据 通常程序都只从fd=0中读数据，所以当n不等于0时，需要多做一步操作3&lt;file &lt;&amp;3 &amp;&gt;file：这是特殊的重定向方式，表示将标准错误和标准输出都重定向到file文件中，等价于&gt;file 2&gt;&amp;1 &amp;&gt;&gt;file：这是特殊的重定向方式，表示将标准错误和标准输出都追加到file文件中，等价于&gt;&gt;file 2&gt;&amp;1另外，经常用于输出的一个特殊目标文件是/dev/null，它是空设备，可以直接丢掉所有写入它的数据。 一个经常用的技巧是清空文件的方式： 12$ cat /dev/null &gt;file$ &gt;file cat1cat -n /etc/fstab cat命令开始执行后，会识别-n选项，该选项会让cat输出时同时输出行号，cat同时还会识别/etc/fstab参数，cat会读取参数指定的文件然后输出。 如果没有指定cat的文件参数，则cat默认会从标准输入中读取数据。默认的标准输入是终端，所以在没有改变标准输入的流向时，会从终端读取数据，也就是用户输入什么字符，就读取什么字符，然后输出什么字符： 123456$ cathello # 在终端输入hello # 在终端输出world # 在终端输入world # 在终端输出^C 但用户可以改变标准输入的来源。比如： 1$ cat &lt;/etc/fstab 表示将标准输入来源改为/etc/fstab文件，于是cat会从/etc/fstab中读取数据。 cat与重定向结合使用可以将将stdin标准输入的内容重定向到指定的文件(若文件不存在，则创建)，且当stdin中含有EOF时完成写入。 1234567891011cat &lt;&lt; EOF &gt; cattest.txthelloEOF# 等价于cat &gt; cattest.txt &lt;&lt; EOFhelloEOF# 也可以使用追加cat &gt;&gt; cattest.txt &lt;&lt;EOFworldEOF read命令read命令是bash内置命令，用来从标准输入中读取数据。比如可以交互式读取用户在终端的输入，读取管道数据，读取标准输入重定向数据，等等。 读取文件中数据的方式： 按字符数读取 按分隔符读取 按行读取 一次性读完所有数据 按字节数读取(read命令不支持) 1234567891011121314151617181920212223242526272829read [-a aname] [-d delim] [-n nchars] [-N nchars] [-p prompt] [-t timeout] [-u fd] [name …]-a arr读取数据保存到数组arr中，arr数组已存在则先清空-d delim指定读取数据时的分隔符，不指定时默认为换行符。当指定为空字符串时，则默认为\\0。当指定为文件中不存在的符号，则直接读整个文件-n X每次读取X个字符，但如果读满X字符前遇到了分隔符，则也停止本次读取-N X每次读取X个字符，即使遇到了分隔符也不停止-p prompt交互式提示用户在终端输入的信息，默认不输出换行符-s用户在终端中的输入不会显示出来。在提示用户输入密码时应设置该选项-r使得读取到的反斜线不进行转义，而是作为一个普通反斜线字符 -t timeout超时时间，如果在超时时间内还未读满数据，则读取失败，可指定为小数时间-u fd从文件描述符中读取数据 例子： 1234567891011#!/usr/bin/bashclearread -p &quot;Login: &quot; accecho -n -e &quot;Password： &quot;read -s -t50 -n6 pwechoecho &quot;account: $acc password: $pw &quot; 注意：如果没有提供变量名，那么读取的数据将存放到环境变量 REPLY 中。 变量分类 本地变量：用户私有变量，只有本用户可以使用，保存在家目录的.bash_profile、.bashrc文件中。 全局变量：所有用户都可以使用，保存在/etc/profile、/etc/bashrc文件中。定义全局变量加export。 用户自定义变量：用户自定义，比如脚本中的变量。 普通数组普通数组：只能使用整数作为数组索引(元素的索引)定义方式： 1数组名称=(元素1 元素2 元素3 ...) 读出方式： 1$&#123;数组名[索引]&#125; 赋值方式： 一次赋一个值 1234变量名=变量值array[0]=v1array[1]=v2array[3]=v3 一次附多个值 12345array=(var1 var2 var3 var4)array1=(`cat /etc/passwd`) # 将文件中每一行赋值给array1数组array2=(`ls /root`)array3=(harry amy jack &quot;Miss zhang&quot;)array4=(1 2 3 4 &quot;hello world&quot; [10]=linux) 访问数组元素1234567$&#123;array[i]&#125; i表示元素的索引# 使用@ 或 * 可以获取数组中的所有元素：echo $&#123;array[*]&#125; # 获取数组里的所有元素echo $&#123;#array[*]&#125; # 获取数组里所有元素个数echo $&#123;!array[@]&#125; # 获取数组元素的索引索引echo $&#123;array[@]:1:2&#125; # 访问指定的元素；1代表从索引为1的元素开始获取；2代表获取后面几个元素 关联数组关联数组使用首先需要申明该数组为关联数组，声明方式： declare -A 数组名称 赋值方式： 声明关联数组 1$ declare -A asso_array1 一次赋一个值 12asso_array1[linux]=oneasso_array1[java]=two 一次附多个值 1declare -A asso_array2=([name1]=harry [name2]=jack [name3]=amy [name4]=&quot;Miss zhang&quot;) bash debug 使用bash的-x选项 1$ bash -x example_script.sh 调试部份的脚本，调用set -x，结束的时候调用set +x。 12345#!/bin/bashecho &quot;Hello $USER,&quot;set -xecho &quot;Today is $(date %Y-%m-%d)&quot;set +x 判断 文件存在与否 12345678-e 是否存在 不管是文件还是目录，只要存在，条件就成立-f 是否为普通文件-d 是否为目录-S socket-p pipe-c character-b block-L 软link 例子：(创建文件夹) 123456if [ ! -d /tmp/abc ] then mkdir -v /tmp/abc echo &quot;123&quot; echo &quot;create /tmp/abc ok &quot;fi 文件权限 123456-r 当前用户对其是否可读-w 当前用户对其是否可写-x 当前用户对其是否可执行-u 是否有suid-g 是否sgid-k 是否有t位 两个文件比较 123file1 -nt file2 比较file1是否比file2新 file1 -ot file2 比较file1是否比file2旧file1 -ef file2 比较是否为同一个文件，或者用于判断硬连接，是否指向同一个inode 字符串比较 1234-z 是否为空字符串 字符串长度为0，就成立-n 是否为非空字符串 只要字符串非空，就是成立string1 == string2 是否相等string1 != string2 不等 if语句1234if [ condition ] 假如 条件为真 then 那么 commands 执行commands代码块fi 结束 123456if [ condition ] then 条件为真 commands1 真 要执行代码块else 条件为假 commands2 假 要执行的代码块fi 结束 1234567891011if [ condition 1 ] 满足第一个条件 then 真 command1 执行command1代码块elif [ condition 2 ] 满足第二个条件 then 真 commands2 执行command2代码块 .......else 如果条件都不满足 commandsX 执行commandX代码块fi 结束判断 高级用法 12345678910111213141516# 条件符号使用双圆括号，可以在条件中植入数学表达式 if (())if (( (5+5-5)*5/5 &gt; 10 )) then echo &quot;yes&quot;else echo &quot;no&quot;fi# 使用双方括号,可以在条件中使用通配符for var in ab ac rx bx rvv vt do if [[ &quot;$var&quot; == r* ]] then echo &quot;$var&quot; fidone for循环123456789101112for variable_name in &#123;list&#125; do command command … done或者for variable in a b c do command command done c风格 123456789101112for(( expr1;expr2;expr3 )) do command command … done for (( i=1;i&lt;=5;i++)) do echo $i done break [n]，退出整个循环，包括for、while、until和select语句。其中数值n表示退出的循环层次。 continue [n]，退出当前循环进入下一次循环。n表示继续执行向外退出n层的循环。默认n=1，表示继续当前层的下一循环，n=2表示继续上一层的下一循环。 while循环1234while [ 表达式 ] # 条件为真就进入循环；条件为假就退出循环 do command... done case语句1234567891011121314case $var in 定义变量;var代表是变量名pattern 1) 模式1;用 | 分割多个模式，相当于or command1 需要执行的语句 ;; 两个分号代表命令结束pattern 2) command2 ;;pattern 3) command3 ;;*) default，不满足以上模式，默认执行*)下面的语句 command4 ;;esac esac表示case语句结束 函数1234567891011121314151617181920语法一:函数名 () &#123; 代码块 return N &#125;语法二：function 函数名 &#123; 代码块 return N &#125; 函数名 #调用函数 函数中return说明：1.return可以结束一个函数，类似于循环控制语句break(结束当前循环，执行循环体后面的代码)2.return默认返回函数中最后一个命令的退出状态，也可以给定参数值，该参数值的范围是0-256之间。3.如果没有return命令，函数将返回最后一个Shell的退出值。shell的函数一般都不写return。 Shell脚本中的$字符 特殊标志符 含义 $0 当前脚本的文件名 $n 传递给脚本或函数的参数。n 是一个数字，表示第几个参数。例如，第一个参数是$1，第二个参数是$2 $# 传递给脚本或函数的参数个数 $* 传递给脚本或函数的所有参数 $@ 传递给脚本或函数的所有参数 $? 上个命令的退出状态 $$ 当前Shell进程ID $() 与 `(反引号) 一样用来命令替换使用 $&#123;&#125; 引用变量划分出边界 12345678#! /bin/bashecho &quot;file name $0&quot;echo &quot;first param $1&quot;echo &quot;pid $$&quot;echo &quot;total num of param $#&quot;echo &quot;last status $?&quot;echo &quot;$@&quot;echo &quot;$*&quot; 12345678910/1.sh hello mayuan # 执行脚本并传入两个参数# 输出结果file name ./1.shfirst param hellopid 142total num of param 2last status 0hello mayuanhello mayuan 文件操作sedsed是linux中提供的一个外部命令,它是一个行(流)编辑器，非交互式的对文件内容进行增删改查的操作。 文本编辑器: 编辑对象是文件；行编辑器：编辑对象是文件中的行。前者一次处理一个文本，而后者是一次处理一个文本中的一行。 sed 命令语法： sed [options] ‘{command}[flags]’ [filename] 123456789101112131415161718192021222324252627282930#命令选项-e script 将脚本中指定的命令添加到处理输入时执行的命令中 多条件，一行中要有多个操作-f script 将文件中指定的命令添加到处理输入时执行的命令中-n 抑制自动输出-i 编辑文件内容-i.bak 修改时同时创建.bak备份文件。-r 使用扩展的正则表达式! 取反 （跟在模式条件后与shell有所区别）#command 对文件干什么sed 常用内部命令a 在匹配后面添加i 在匹配前面添加d 删除s 查找替换 字符串c 更改y 转换 N D P p 打印#flags数字 表示新文本替换的模式g： 表示用新文本替换现有文本的全部实例p： 表示打印原始的内容w filename: 将替换的结果写入文件 命令： 123456789101112131415161718192021222324252627282930313233sed &#x27;a\\append data &quot;haha&quot;&#x27; data1 # 在data1的每行后追加一行新数据内容: append data &quot;haha&quot;sed &#x27;2a\\append data &quot;haha&quot;&#x27; data1# 在第二行后新开一行追加数据: append data &quot;haha&quot;sed &#x27;/3 the/a\\append data &quot;haha&quot;&#x27; data1# 找到包含&quot;3 the&quot;的行，在其后新开一行追加内容: append data &quot;haha&quot;# //开启匹配模式 /要匹配的字符串/ 更常用sed &#x27;2,4i\\insert data &quot;haha&quot;&#x27; data1# 在第二到四行每行前新开一行插入数据: insert data &quot;haha&quot;sed &#x27;/3 the/i\\insert data &quot;haha&quot;&#x27; data1# 找到包含&quot;3 the&quot;的行，在其前新开一行插入内容: insert data &quot;haha&quot;sed &#x27;/3 the/d&#x27; data1# 删除文件data1中包含字符串&quot;3 the&quot;的行sed &#x27;2s/dog/cat/&#x27; data1# 将data1中第二行的dog替换为catsed &#x27;/3 the/s/dog/cat/&#x27; data1# 将包含字符串&quot;3 the&quot;的行中的dog替换为catsed &#x27;2c\\change data &quot;haha&quot;&#x27; data1# 将data1文件第二行的内容更改为: change data &quot;haha&quot;sed &#x27;2,4c\\change data &quot;haha&quot;&#x27; data1# 将data1文件中的第二、三、四行的内容删除，更改为一行：change data &quot;haha&quot;sed &#x27;/3 the/c\\change data &quot;data&quot;&#x27; data1# 将data1文件中包含&quot;3 the&quot;的行内容更改为: change data &quot;haha&quot;sed &#x27;y/abc/ABC/&#x27; data1# 将data1中的a b c字符转换为对应的 A B C字符sed &#x27;2,4p&#x27; data1# 打印data1文件第二、三、四行内容 标志位 123456789101112sed &#x27;s/dog/cat/2&#x27; data2# 替换一行中的第二处dog为catsed &#x27;s/dog/cat/g&#x27; data2# 将data1文件中的所有dog替换为catsed &#x27;3s/dog/cat/w text&#x27; data2# 将修改的内容存入text文件中sed &#x27;w data2.bak&#x27; data2# 备份data2的数据sed &#x27;3s/dog/cat/p&#x27; data2# 打印文本内容，类似于-p命令选项 选项 12345678910111213141516171819202122232425sed -n &#x27;2,$p&#x27; data1 # 打印data1文件的第二行到最后一行内容 $是最后的意思sed -r &#x27;/(^#|#|^$)/d&#x27; nginx.conf # 删除所有注释和空行sed -n -r &#x27;/^(root)(.*)(bash)$/p&#x27; /etc/password# 打印root开头，bash结尾，中间任意的行sed -e &#x27;s/brown/green/;s/dog/cat/&#x27; data1# 将brown替换为green dog替换为cat# 在命令行中使用多个命令 用-evim abc ### abc的内容# s/brown/green/ # s/dog/cat/# s/fox/elephant/###sed -f abc data1 # 从文件读取编辑器命令 用-f ，适用于日常重复执行的场景# 如果需要修改文件内容可以直接使用-i命令选项sed -i.bak &#x27;s/brown/green/&#x27; data1# -i是一个不可逆的操作，一旦修改，如果想复原就很困难，几乎不可能，所以建议大家在操作的时候可以备份一下源文件。 其他： 12345# 配合管道echo &quot;xxx is yyy&quot; | sed &#x27;s/yyy/zzz/&#x27;# 统计行数sed -n &#x27;$=&#x27; data1 awkawk可以对数据进行筛选和处理。awk是一种可以处理数据、产生格式化报表的语言，功能十分强大。awk 认为文件中的每一行是一条记录 记录与记录的分隔符为换行符,每一列是一个字段 字段与字段的分隔符默认是一个或多个空格或tab制表符。 awk的工作方式是读取数据，将每一行数据视为一条记录（record）每条记录以字段分隔符分成若干字段，然后输出各个字段的值。 awk语法 awk [options] ‘[BEGIN]{program}[END]’ [FILENAME] 12345678910111213常用命令选项-F fs 指定描绘一行中数据字段的文件分隔符 默认为空格-f file 指定读取程序的文件名-v var=value 定义awk程序中使用的变量和默认值注意：awk 程序由左大括号和右大括号定义。 程序命令必须放置在两个大括号之间。由于awk命令行假定程序是单文本字符串，所以必须将程序包括在单引号内。1）程序必须放在大括号内2）程序必须要用单引号引起来awk程序运行优先级是: 1)BEGIN: 在开始处理数据流之前执行，可选项 2)program: 如何处理数据流，必选项 3)END: 处理完数据流后执行，可选项 基本用法 awk对字段(列)的提取 字段提取:提取一个文本中的一列数据并打印输出 字段相关内置变量 $0 表示整行文本 $1 表示文本行中的第一个数据字段 $2 表示文本行中的第二个数据字段 $N 表示文本行中的第N个数据字段 $NF 表示文本行中的最后一个数据字段（NF是总字段数，即列数） 123456awk &#x27;&#123;print $0&#125;&#x27; test # 读入test每行数据并把每行数据打印出来awk &#x27;&#123;print $6&#125;&#x27; test# 打印test第六个字段（列）awk &#x27;&#123;print $NF&#125;&#x27; test# 打印test最后一列 awk对记录(行)的提取 记录提取：提取一个文本中的一行并打印输出 记录的提取方法有两种：a、通过行号 b、通过正则匹配 NR: 指定行号 number row 12345678awk &#x27;NR==3&#123;print $0&#125;&#x27; test # 提取test第三行数据 指定行号为3awk &#x27;$1==&quot;3&quot;&#123;print $0&#125;&#x27; test# 指定行的第一个字段精确匹配字符串为3df -hT|awk &#x27;/\\/$/&#123;print$(NF-1)&#125;&#x27;# 两个/之间是正则表达式，这里是找到以/结尾的行，并打印倒数第二列# 整条命令的作用是查看根目录的磁盘使用率 选项 -F: 指定字段与字段的分隔符 12awk -F &#x27;:&#x27; &#x27;&#123;print $1,$3,$NF&#125;&#x27; /etc/passwdawk -F &#x27;:&#x27; &#x27;&#123;print &quot;Account: &quot; $1,&quot;UID: &quot; $3,&quot;DESC: &quot; $5&#125;&#x27; /etc/passwd #可以加入字符串 awk的优先级 关于awk程序的执行优先级，BEGIN是优先级最高的代码块，是在执行PROGRAM之前执行的，不需要提供数据源，因为不涉及到任何数据的处理，也不依赖与PROGRAM代码块；PROGRAM是对数据流干什么，是必选代码块，也是默认代码块。所以在执行时必须提供数据源；END是处理完数据流后的操作，如果需要执行END代码块，就必须需要PROGRAM的支持，单个无法执行。 BEGIN：处理数据源之前干什么 不需要数据源就可以执行 PROGRAM： 对数据源干什么 【默认必须有】 需要数据源 END：处理完数据源后干什么 需要program 需要数据源 12345678awk &#x27;BEGIN&#123;print &quot;hello&quot;&#125;&#123;print $0&#125;END&#123;print &quot;bye&quot;&#125;&#x27; test# 不需要数据源，可以直接执行awk &#x27;BEGIN&#123;print &quot;hello world&quot;&#125;&#x27;# 没有提供数据流，所以无法执行成功# awk &#x27;&#123;print &quot;hello world&quot;&#125;&#x27;# awk &#x27;END&#123;print &quot;hello world&quot;&#125;&#x27; 高级用法awk还可以定义变量，定义数组，运算和流程控制。 计算内存使用率 1head -2 /proc/meminfo | awk &#x27;NR==1&#123;t=$2&#125;NR==2&#123;f=$2;print (t-f)*100/t &quot;%&quot;&#125;&#x27; 变量和数组 123awk &#x27;BEGIN&#123;name=&quot;xxx&quot;;print name&#125;&#x27;awk &#x27;BEGIN&#123;array[0]=100;array[1]=200;print array[0],array[1]&#125;&#x27; 运算 1234awk &#x27;BEGIN&#123;print &quot;a&quot; &gt;= &quot;b&quot; &#125;&#x27;awk &#x27;BEGIN&#123;print 100/3 &#125;&#x27;awk &#x27;BEGIN&#123;print 100&gt;=2 &amp;&amp; 100&gt;=3 &#125;&#x27;awk -v &#x27;count=0&#x27; &#x27;BEGIN&#123;count++;print count&#125;&#x27; 匹配 12awk -F: &#x27;$1=&quot;root&quot; &#123;print $0&#125;&#x27; /etc/passwd # 精确匹配awk -F: &#x27;$1 ~ &quot;^ro&quot; &#123;print $0&#125;&#x27; /etc/passwd # 模糊匹配 awk环境变量 变量 描述 FIELDWIDTHS 以空格分隔的数字列表，用空格定义每个数据字段的精确宽度 FS 输入字段分隔符号，数据源的字段分隔符，等价于-F OFS 输出字段分隔符号 RS 输入记录分隔符 ORS 输出记录分隔符号 NF 一条记录（行）的字段的数目，即列数 1234567891011121314# FIELDWIDTHS:重定义列宽并打印，注意不可以使用$0打印所有，因为$0是打印本行全内容，不会打印你定义的字段awk &#x27;BEGIN&#123;FIELDWIDTHS=&quot;5 2 8&quot;&#125;NR==1&#123;print $1,$2,$3&#125;&#x27; /etc/passwd# FS:指定数据源中字段分隔符，类似命令选项-Fawk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125;NR==1&#123;print $1,$3,$NF&#125;&#x27; /etc/passwd# OFS:指定输出到屏幕后字段的分隔符awk &#x27;BEGIN&#123;FS=&quot;:&quot;;OFS=&quot;-&quot;&#125;NR==1&#123;print $1,$3,$NF&#125;&#x27; /etc/passwd# RS:指定记录(行)的分隔符awk &#x27;BEGIN&#123;RS=&quot;&quot;&#125;&#123;print $1,$2,$3&#125;&#x27; test# ORS:输出到屏幕后记录的分隔符，默认为回车awk &#x27;BEGIN&#123;RS=&quot;&quot;;ORS=&quot;*&quot;&#125;&#123;print $1,$2,$3&#125;&#x27; test 流程控制 1234567891011121314151617181920awk &#x27;&#123;if($1&gt;5)print $0&#125;&#x27; num# 单if语句 打印$1大于5的行awk &#x27;&#123;if($1&gt;5)print $1/2;else print $1*2&#125;&#x27; num# if...else语句 假如$1大于5则除以2输出，否则乘以2输出awk &#x27;&#123;sum=0;for (i=1;i&lt;4;i++)&#123;sum+=$i&#125;print sum&#125;&#x27; num2# for循环 将一行中的数据都加起来 $1+$2+$3# 如果看的不明白可以看下面格式awk &#x27;&#123;&gt; sum=0&gt; for (i=1;i&lt;4;i++) &#123;&gt; sum+=$i&gt; &#125;&gt; print sum&gt; &#125;&#x27; num2awk &#x27;&#123;sum=0;i=1;while(i&lt;4)&#123;sum+=$i;i++&#125;print sum&#125;&#x27; num2# while循环 将文件中的每行的数值累加# do while和break也可以使用 小技巧 12345678# 打印test文本中的行数awk &#x27;END&#123;print NR&#125;&#x27; test# 打印test文本最后一行的内容awk &#x27;END&#123;print $0&#125;&#x27; test# 打印test文本中的列数awk &#x27;END&#123;print NF&#125;&#x27; test 常用命令wcwc命令用于计算字数。 -c或—bytes或—chars 只显示Bytes数。 -l或—lines 显示行数。 -w或—words 只显示字数。 sortsort命令用于将文本文件内容加以排序。 1234567891011121314151617181920sort filename # sort不加任何参数默认是对文件的行进行排序，以ASCII 码的次序排列，，并将结果输出到标准输出。$ cat testfile # testfile文件原有排序 test 30 Hello 95 Linux 85$ sort testfile # 重排结果 Hello 95 Linux 85 test 30sort -n qc.log # 按数字排序：默认为升序。sort -r qc.log # 按倒序排序。sort -nr qc.log # 按数字倒序排序。-t # 指定列的分隔符-k # 按指定的列排序sort -t&quot; &quot; -k2 qc3.log # 以空格划分列，并按照第二列排序 uniquniq 命令用于检查及删除文本文件中重复出现的行列，一般与 sort 命令结合使用。 12uniq testfile # 删除重复行(重复的行只保留一行)uniq -c testfile # 检查文件并删除文件中重复出现的行，并在行首显示该行重复出现的次数 headhead 用来查看文件的开头部分的内容，默认head命令打印其相应文件的开头10行。 1234head -n 5 test.log # 显示 test.log 文件的开头 5 行head -n -10 test.log # 输出文件除了最后 10 行的全部内容head -c 50 test.log # 显示test.log 文件前 50 个字节head -c -50 test.log # 文件的除了最后 50 个字节以外的内容 tailtail用于显示指定文件末尾内容。 12tail -n 3 log1 # 显示log1文件最后3行内容tail -n +3 log1 # 从第3行开始显示log1文件内容 tailf常用来追踪日志。 1tailf logfile # 动态跟踪日志文件logfile，最初的时候打印文件的最后10行内容。 常用工具lsoflsof 是 List Open File 的缩写, 它主要用来获取被进程打开文件的信息，我们都知道，在Linux中，一切皆文件，lsof命令可以查看所有已经打开了的文件，比如: 普通文件，目录，特殊的块文件，管道，socket套接字，设备，Unix域套接字等等，同时，它还可以结合 grep 以及 ps 命令进行更多的高级搜索。 不带任何参数执行 lsof 命令会输出当前所有活跃进程打开的所有文件 -u，列出指定用户已经打开的文件 -i，列出所有打开了的网络文件 -i 4，列出所有已经打开了的 ipv4 网络文件 -i 6，列出所有已经打开了的 ipv6 网络文件 -i:端口号，获得所有在指定端口号上打开的文件 -i TCP/UDP 列出使用了TCP 或 UDP 协议的文件 -p，列出指定进程ID打开的文件 curlcurl 是一个工具，利用URL规则在命令行下工作的文件传输工具，可以说是一款很强大的http命令行工具。它支持文件的上传和下载，是综合传输工具，但按传统，习惯称url为下载工具。可支持的协议有（DICT、FILE、FTP、FTPS、GOPHER、HTTP、HTTPS、IMAP、IMAPS、LDAP、LDAPS、POP3、POP3S、RTMP、RTSP、SCP、SFTP、SMTP、SMTPS、TELNET和TFTP）。 1234567891011121314curl http://www.linux.com# 直接下载html页面，内容会直接显示在终端curl http://www.linux.com &gt;&gt; linux.html# 将内容重定向到文件中curl -o linux.html http://www.linux.com# 或者使用-o 输出到文件中curl -O http://www.linux.com/hello.sh# -O 要注意这里后面的url要具体到某个文件，不然抓不下来-s # 静默模式，不输出错误信息和进度信息-S # 当与 -s 一起使用时，如果curl失败，curl将显示一条错误消息-i # 在输出的内容中包含HTTP 头信息-I # (HTTP/FTP/FILE)只获取HTTP头文件;在FTP或FILE 文件上使用时，curl只显示文件大小和最后修改时间-# # 将curl进度显示为一个简单的进度条；而不是标准的、具有更多信息的进度表 curl也可以发送Post请求，上传表单。 12345678# -d参数用于发送 POST 请求的数据体curl -d&#x27;login=emma＆password=123&#x27;-X POST https://google.com/login# 等价于curl -d &#x27;login=emma&#x27; -d &#x27;password=123&#x27; -X POST https://google.com/login# 使用-d参数以后，会自动将请求转为 POST 方法，因此可以省略-X POST# -d参数也可以从本地文件读取数据curl -d &#x27;@data.txt&#x27; https://google.com/login netstatnetstat命令是一个监控TCP/IP网络的非常有用的工具，它可以显示路由表、实际的网络连接以及每一个网络接口设备的状态信息。 字段 含义 Proto 协议 Recv-Q 接收队列（本地缓冲） Send-Q 发送队列（本地缓冲） Local Address 本地IP:本地端口 Foreign Address 远程IP:远程端口 State 链接状态 PID 进程PID号 Program name 程序名字 链接状态 含义 LISTEN 侦听来自远程TCP端口的连接请求 SYN_SENT 在发送SYN连接请求后等待确认 SYN_RECV 已收到SYN连接请求，发送SYNACK后等待确认 ESTABLISHED 连接建立 FIN_WAIT1 主动关闭(active close)端发出FIN关闭请求 CLOSE_WAIT 被动关闭(passive close)端TCP接到FIN后，就发出FINACK以回应FIN请求 FIN_WAIT2 主动关闭端接到FINACK后，就进入了 FIN-WAIT-2 LAST_ACK 被动关闭端一段时间后，接收到文件结束符的应用程序将调用CLOSE关闭连接，发出FIN关闭请求 TIME_WAIT 在主动关闭端接收到FIN后，发送FINACK包，并进入TIME-WAIT状态 CLOSING 比较少见。Both sockets are shut down but we still don’t have all our data sent. CLOSED 被动关闭端在接受到ACK包后，就进入了closed的状态。TIME-WAIT状态结束，在主动关闭端进入closed状态 UNKNOWN 未知的Socket状态 列出所有端口情况 1234netstat -a # 列出所有端口详情netstat -at # 列出所有TCP端口netstat -au # 列出所有UDP端口netstat -an # 数字代替名称 列出所有处于监听状态的 Sockets 1234netstat -l # 只显示监听端口netstat -lt # 显示监听TCP端口netstat -lu # 显示监听UDP端口netstat -lx # 显示监听UNIX端口 显示每个协议的统计信息 123netstat -s # 显示所有端口的统计信息netstat -st # 显示所有TCP的统计信息netstat -su # 显示所有UDP的统计信息 显示 PID 和进程名称 1netstat -p 显示核心路由信息 12netstat -rnetstat -rn # 显示数字格式，不查询主机名称 查看端口和服务 12netstat -antp | grep sshnetstat -antp | grep 22 scpscp是基于ssh的远程文件复制工具。 1234567891011# 本地复制到远程scp /home/space/music/1.mp3 root@192.168.1.132:/home/root/others/music scp /home/space/music/1.mp3 root@192.168.1.132:/home/root/others/music/001.mp3scp -r /home/space/music/ root@192.168.1.132:/home/root/others/ # 从远程复制到本地scp root@192.168.1.132:/home/root/others/music /home/space/music/1.mp3 scp -r root@192.168.1.132:/home/root/others/ /home/space/music/# 指定端口scp -P 4588 root@192.168.1.132:/usr/local/sin.sh /home/administrator 其他库函数库函数就是/etc/rc.d/init.d/functions文件中定义的系统函数（/etc/init.d/是/etc/rc.d/init.d的软链接）。 显示函数 success：显示绿色的OK，表示成功 failure：显示红色的FAILED，表示失败 passed：显示黄色的PASSED，表示pass该任务 warning：显示黄色的warning，表示警告 confirm：提示(Y)es/(N)o/(C)ontinue? [Y]并判断、传递输入的值 is_true：$1的布尔值代表为真时，返回状态码0，否则返回1；包括t/y/yes/true，不区分大小写 is_false：$1的布尔值代表为假时，返回状态码0，否则返回1；包括f/n/no/false，不区分大小写 action：根据进程退出状态码自行判断是执行success还是failure 进程函数 checkpid：检查/proc下是否有给定pid对应的目录，给定多个pid时，只要存在一个目录都返回状态码0 __pids_var_run：检查pid是否存在，并保存到变量pid中，同时返回几种进程状态码 __pids_pidof：获取进程pid pidfileofproc：获取进程pid，但只能获取/var/run下的pid文件中的值 pidofproc：获取进程pid，可获取任意给定pidfile或默认/var/run下pidfile中的值 status：检查给定进程的运行状态 daemon：启动一个服务程序，启动前还检查进程是否已在运行 killproc：杀掉给定的服务进程 在脚本中使用引入库函数的方式是在脚本中加入. /etc/rc.d/init.d/functions或. /etc/init.d/functions 正则表达式正则表达式是一种文本匹配模式，提供一些特殊字符生成匹配字符串的公式来从文本中匹配出想要的数据。 Linux中支持正则表达式的工具：locate |find| vim| grep| sed |awk。 定位符使用技巧：同时锚定开头和结尾，做精确匹配；单一锚定开头或结尾或者不锚定的，做模糊匹配。 定位符 说明 ^ 锚定开头 ^a 以a开头 默认锚定一个字符 $ 锚定结尾 a$ 以a结尾 默认锚定一个字符 grep -e或egrep，表示支持正则表达式（默认不支持）。 123$ egrep &quot;^ac$&quot; file # 匹配a开头且c结尾的串 精确匹配$ egrep &quot;^a&quot; file # 匹配a开头的串 模糊匹配$ egrep &quot;c$&quot; file # 匹配c结尾的串 模糊匹配 匹配符 说明 . 匹配除回车以外的任意一个字符 ( ) 字符串分组 [ ] 定义字符类，匹配括号中的一个字符 [ ^ ] 表示否定括号中出现字符类中的字符,取反。 \\ 转义字符 \\ 或 123$ egrep &quot;^a[a-z0-9]c$&quot; file # 以a开头c结尾 中间是a-z,0-9 长度为三个字节的字符串$ egrep &quot;^a[^a-z0-9]c$&quot; file # 以a开头c结尾 中间不包含a-z,0-9 长度为三个字节的字符串$ egrep &quot;^a.(b|c)$&quot; file # 以a开头b或c结尾 中间是任意 长度为三个字节的字符串 限定符 说明 * 某个字符之后加星号表示该字符不出现或出现多次 a (ab) ？ 与星号相似，但略有变化，表示该字符出现一次或不出现 + 与星号相似，表示其前面字符出现一次或多次，但必须出现一次 {n,m} 某个字符之后出现，表示该字符最少n次，最多m次 {m} 正好出现了m次 12345$ egrep &quot;^ab*c$&quot; file # 以a开头 c结尾 中间是有b或者没有b 长度不限的字符串$ egrep &quot;^ab?c$&quot; file # 以a开头 c结尾 中间只出现一次b或者没有b的字符串$ egrep &quot;^ab+c$&quot; file # 以a开头 c结尾 中间是有b且至少出现一次 长度不限的字符串$ egrep &quot;^ab&#123;2,4&#125;c$&quot; file # 以a开头 c结尾 中间是有b且至少出现两次最多出现四次 长度不限的字符串$ egrep &quot;^ab&#123;3&#125;c$&quot; file # 以a开头 c结尾 中间是有b且正好出现三次的字符串 posix字符 posix字符一次只匹配一个范围中的一个字节 POSIX字符 说明 [:alnum:] 匹配任意字母字符0-9 a-z A-Z [:alpha:] 匹配任意字母，大写或小写 [:digit:] 数字 0-9 [:graph:] 非空字符( 非空格控制字符) [:lower:] 小写字符a-z [:upper:] 大写字符A-Z [:cntrl:] 控制字符 [:print:] 非空字符( 包括空格) [:punct:] 标点符号 [:blank:] 空格和TAB字符 [:xdigit:] 16 进制数字 [:space:] 所有空白字符( 新行、空格、制表符) 注意[[ ]]双中括号的意思: 第一个中括号是匹配符[]匹配中括号中的任意一个字符，第二个[]是格式 如[:digit:]。 12$ egrep &quot;^a[[:alnum:]]c$&quot; file # 以a开头c结尾 中间a-zA-Z0-9任意字符 长度为三个字节的字符串$ egrep &quot;^a[[:blank:]]c$&quot; file # 以a开头c结尾 中间是空格或者TAB符字符 长度为三个字节的字符串 案例：ip匹配 1egrep &#x27;(^([1-9]|1[0-9]|1[1-9]&#123;2&#125;|2[0-4][0-9]|25[0-5])\\.)(([0-9]|1[0-9]|1[1-9]&#123;2&#125;|2[0-4][0-9]|25[0-5])\\.)&#123;2&#125;([1-9]|1[0-9]|1[1-9]&#123;2&#125;|2[0-5][0-9]|25[0-4])$&#x27; -- color ip.txt &nbsp; 参考资料： shell中的括号","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"http://wht6.github.io/tags/Shell/"}]},{"title":"LVS+Keepalived架构理解","slug":"LVS-Keepalived架构理解","date":"2022-03-19T12:00:00.000Z","updated":"2022-04-06T06:21:12.292Z","comments":true,"path":"posts/8db7.html","link":"","permalink":"http://wht6.github.io/posts/8db7.html","excerpt":"","text":"架构简介在LVS+Keepalived架构中，LVS起到负载均衡的作用，Keepalived保证服务的高可用。LVS按照一定的调度算法，把不同客户端请求转发到不同的后端真实的服务器（RS，Real Server），平衡各个后端服务器的负载。keepalived 为LVS中负载均衡器（LB，LoadBalancer）提供一个冗余备份，当主LB故障时，随时切换到从LB。keepalived 还会对RS做健康检查，发现不健康的 RS，就把它从LVS集群中剔除。&nbsp; Keepalived在此之前，先理解什么是虚拟IP（VIP）。&nbsp; 虚拟IP虚拟IP，就是一个未分配给真实主机的IP，也就是说对外提供服务器的主机除了有一个真实IP外还有一个虚IP，使用这两个IP中的任意一个都可以连接到这台主机。虚拟IP一般用作达到HA(High Availability)的目的,比如让所有项目中数据库链接一项配置的都是这个虚IP，当主服务器发生故障无法对外提供服务时，动态将这个虚IP切换到备用服务器。&nbsp;因为ip地址只是一个逻辑地址，在以太网中MAC地址才是真正用来进行数据传输的物理地址，每台主机中都有一个ARP高速缓存，存储同一个网络内的IP地址与MAC地址的对应关 系，以太网中的主机发送数据时会先从这个缓存中查询目标IP对应的MAC地址，会向这个MAC地址发送数据。操作系统会自动维护这个缓存。&nbsp;虚拟IP实际上就是临时绑定在物理网卡上的别名，如eth0:x ，x为0-255的任意数字，你可以在一块网卡上绑定多个别名。这个VIP可以看作是你上网的QQ网名、昵称、外号等。在实际生产环境中，需要在DNS配置中把网站域名地址解析到这个VIP地址，由这个VIP对用户提供服务。这样做的好处就是当提供服务的服务器宕机以后，在接管的服务器上会直接自动配置上同样的VIP提供服务。&nbsp;Linux系统给网卡配置VIP的方法常见的有两种，即别名IP（alias ip）以及辅助IP（secondary ip address）。ip alias 和 secondary ip address 是两种不同的实现方式，用来在 Linux 系统中给同一个物理网卡增加多个ip地址。提示： heartbeat 和 keepalived 在启动时就是分别利用上面命令来配置VIP的。在停止时利用下面的命令来删除VIP。以上两种方式配置VIP，在高可用环境中的作用是一样的，没什么区别，只是由于当时的系统环境等历史原因，选择的配置命令方式不同。heartbeat3 版本起，不在使用别名，而是使用辅助IP提供服务，而 keepalived 软件一直都是使用的辅助IP技术。注意：别名IP将被遗弃，用辅助IP替代&nbsp; Keepalived原理Keepalived是基于VRRP协议的一款高可用软件。Keepailived有一台主服务器和多台备份服务器，在主服务器和备份服务器上面部署相同的服务配置，使用一个虚拟IP地址对外提供服务，当主服务器出现故障时，虚拟IP地址会自动漂移到备份服务器。&nbsp;VRRP（Virtual Router Redundancy Protocol，虚拟路由器冗余协议），VRRP是为了解决静态路由的高可用。VRRP的基本架构虚拟路由器由多个路由器组成，每个路由器都有各自的IP和共同的VRID(0-255)，其中一个VRRP路由器通过竞选成为MASTER，占有VIP，对外提供路由服务，其他成为BACKUP，MASTER以IP组播（组播地址：224.0.0.18）形式发送VRRP协议包，与BACKUP保持心跳连接，若MASTER不可用（或BACKUP接收不到VRRP协议包），则BACKUP通过竞选产生新的MASTER并继续对外提供路由服务，从而实现高可用。&nbsp;Keepalived起初就是为LVS设计的，因此通常搭配LVS实现高可用。Keepalived的实现就是它自身启动为一个服务，工作在多个LVS主机节点上，当前活动的节点叫做Master，备用节点叫做Backup，Master会不停的向Backup节点通告自己的心跳，这种通告是基于VRRP协议的。Backup节点一旦接收不到Master的通告信息，从Backup中选择一个新的Master，将LVS的VIP和ipvs的规则在该节点上生效，从而替代Master节点。&nbsp;Keepalived启动后以后会有一个主进程Master，它会生成还有2个子进程，一个是VRRP Stack负责VRRP（也就是VRRP协议的实现）、一个是Checkers负责IPVS的后端的应用服务器的健康检查，当检测失败就会调用IPVS规则删除后端服务器的IP地址，检测成功了再加回来。当检测后端有失败的情况可以使用SMTP通知管理员。另外VRRP如果检测到另外一个Keepalive失败也可以通过SMTP通知管理员。&nbsp; LVSLVS概念LVS（Linux Virtual Server）即Linux虚拟服务器，是一个负载均衡项目，目前LVS已被集成到Linux内核模块中，因此性能较高。&nbsp;LVS属于四层负载均衡，不支持七层规则修改，机制庞大，不适合小规模应用。&nbsp;优势：高并发连接：LVS基于内核工作，有超强的承载能力和并发处理能力。单台LVS负载均衡器，可支持上万并发连接。稳定性强：是工作在网络4层之上仅作分发之用，这个特点也决定了它在负载均衡软件里的性能最强，稳定性最好，对内存和cpu资源消耗极低。成本低廉：硬件负载均衡器少则十几万，多则几十万上百万，LVS只需一台服务器和就能免费部署使用，性价比极高。配置简单：LVS配置非常简单，仅需几行命令即可完成配置，也可写成脚本进行管理。支持多种算法：支持多种论调算法，可根据业务场景灵活调配进行使用。支持多种工作模型：可根据业务场景，使用不同的工作模式来解决生产环境请求处理问题。应用范围广：因为LVS工作在4层，所以它几乎可以对所有应用做负载均衡，包括http、数据库、DNS、ftp服务等等。&nbsp;LVS核心组件：管理工具ipvsadm：用户空间的命令行工具，用于管理集群服务及集群服务上的RS等；内核模块ipvs：工作于内核上的程序，可根据用户定义的集群实现请求转发；&nbsp;IPVS是LVS项目重要组成部分，是一款运行在Linux kernel当中的4层负载均衡器，性能异常优秀。IPVS依赖于netfilter框架，位于内核源码的net/netfilter/ipvs目录下。k8s引入了IPVS模式，IPVS模式与iptables同样基于Netfilter，但是采用的hash表，因此当service数量达到一定规模时，hash查表的速度优势就会显现出来，从而提高service的服务性能。&nbsp; LVS的转发模式NAT模式(NAT)：进站和出站的数据流量都要经过负载均衡器。负载均衡器（也叫Director Server）具有公网IP和内网IP，真实服务器只有内网IP。接收到目的地址为VIP的数据包，根据调度算法从后端群集中选择一个RS（real server），将数据包的目标IP地址重写为所选RS的IP地址，然后将数据包转发到RS。当RS处理完请求之后，会把回复数据包返回给LB，此时LB会将数据包的源IP地址重写为自己的IP地址，然后发送给客户端。 RS 应该使用私有地址，RS 的网关必须指向负载均衡器。 优点：集群中的物理服务器可以使用任何支持TCP/IP操作系统，只有负载均衡器需要一个合法的IP地址，并且支持端口映射，可修改请求报文的端口。NAT 模式的优势在于配置及管理简单，由于了使用 NAT 技术，LVS 调度器及应用服务器可以在不同网段中，网络架构更灵活，应用服务器只需要进行简单的网络设定即可加入集群。 缺点：扩展性有限。当服务器节点（普通PC服务器）增长过多时,负载均衡器将成为整个系统的瓶颈。因为所有的请求包和应答包的流向都经过负载均衡器。当服务器节点过多时大量的数据包都交汇在负载均衡器那，速度就会变慢！ &nbsp;直接路由模式(DR) ：只有进站的数据流量经过负载均衡器。LB（LoadBalancer）接收到目的地址为VIP的数据包，根据调度算法从后端群集中选择一个RS（real server），将所选RS的MAC地址封装到数据包上，将数据包转发给RS。当RS处理完请求之后，会直接发送给客户端。这里LB和RS都使用同一个VIP对外服务但只有LB对ARP请求进行响应，网关会把对这个VIP的请求全部定向给LB，LB再将请求分发给RS。处理完成之后，由于IP一致，则等于直接从客户端收到这个数据包无异，可以直接将数据返给客户。限制：由于负载均衡器要对二层包头进行改换,所以负载均衡器和RS之间必须在一个广播域，也可以简单的理解为在同一台交换机上。 保证前端路由将目标地址为 VIP 报文统统发给LB，而不是 RS。实现方案：修改 RS 上内核参数（arp_ignore 和 arp_announce）将 RS 上的 VIP 配置在 lo 接口的别名上，并限制其不能响应对 VIP 地址解析请求。 RS 可以使用私有地址；也可以是公网地址，如果使用公网地址，此时可以通过互联网对 RIP 进行直接访问。 RS 的网关绝不允许指向LB。 优点：和TUN（隧道模式）一样，负载均衡器也只是分发请求，应答包通过单独的路由方法返回给客户端，但是与TUN相比，DR这种实现方式不需要隧道结构，因此可以使用大多数操作系统做为物理服务器。 缺点：RS节点需要合法IP，要求负载均衡器的网卡必须与物理网卡在一个物理段上。 &nbsp;隧道模式(TUN)：只有进站的数据流量经过负载均衡器。接收到目的地址为VIP的数据包，根据调度算法从后端群集中选择一个RS（real server），将所选RS的IP地址封装到数据包上，将数据包转发给RS。RS收到后，先把数据包的头解开，还原数据包，处理后，会直接发送给客户端。注意，由于RS需要对负载均衡器发过来的数据包进行还原,所以说必须支持IPTUNNEL协议，所以,在RS的内核中,必须编译支持IPTUNNEL这个选项。 后端服务器地址、网关地址、负载均衡器地址全是公网地址。 RS 的网关不会也不可能指向LB 优点：负载均衡器只负责将请求包分发给后端节点服务器，而RS将应答包直接发给用户。所以，减少了负载均衡器的大量数据流动，负载均衡器不再是系统的瓶颈，就能处理很巨大的请求量。这种方式，一台负载均衡器能够为很多RS进行分发。而且跑在公网上就能进行不同地域的分发。 缺点：隧道模式的RS节点需要合法IP，需要所有的服务器支持”IP Tunneling”(IP Encapsulation)协议，服务器可能只局限在部分Linux系统上。 &nbsp; 参考链接： https://segmentfault.com/a/1190000020288198 https://wsgzao.github.io/post/lvs-tun/ https://www.cnblogs.com/rexcheny/p/10778567.html http://arganzheng.life/keepalived-in-action.html","categories":[{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"负载均衡","slug":"负载均衡","permalink":"http://wht6.github.io/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"},{"name":"高可用","slug":"高可用","permalink":"http://wht6.github.io/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"}]},{"title":"Zabbix监控GPU","slug":"Zabbix监控GPU","date":"2022-03-17T06:00:00.000Z","updated":"2022-05-02T08:11:38.298Z","comments":true,"path":"posts/c620.html","link":"","permalink":"http://wht6.github.io/posts/c620.html","excerpt":"","text":"Agent配置创建GPU自动发现脚本gpu_discovery.sh到目录/etc/zabbix/。 12345678910111213141516#!/bin/shGPUS=`nvidia-smi -L | awk -F &#x27; |:&#x27; &#x27;&#123;print $2&#125;&#x27;`LENGTH=$&#123;#GPUS[*]&#125;printf &quot;&#123;\\n&quot;printf &#x27;\\t&#x27;&quot;\\&quot;data\\&quot;:[&quot;for ((i=0;i&lt;$LENGTH;i++))do printf &#x27;\\n\\t\\t&#123;&#x27; printf &quot;\\&quot;&#123;#GPU_ID&#125;\\&quot;:\\&quot;$&#123;GPUS[$i]&#125;\\&quot;&#125;&quot; if [ $i -lt $[$LENGTH-1] ];then printf &#x27;,&#x27; fidoneprintf &quot;\\n\\t]\\n&quot;printf &quot;&#125;\\n&quot; nvidia-smi -L表示列出系统中所有的GPU。 awk -F &#39; |:&#39; &#39;&#123;print $2&#125;&#39;表示指定分隔符为空格和：，并输出第二个字段。另一种写法是nvidia-smi -L | awk -F &#39;[ :]&#39; &#39;&#123;print $2&#125;&#39;。 $&#123;#GPUS[*]&#125;表示获取数组长度，利用@或*，可以将数组扩展成列表，然后使用#来获取数组元素的个数。 创建agent的配置文件userparameter_gpus.conf到/etc/zabbix/zabbix_agentd.d/目录。 12345UserParameter=gpu.discovery,/etc/zabbix/gpu_discovery.shUserParameter=gpu.name[*],nvidia-smi -i $1 --query-gpu=name --format=csv,noheader,nounitsUserParameter=gpu.mem[*],nvidia-smi -i $1 --query-gpu=memory.$2 --format=csv,noheader,nounitsUserParameter=gpu.utilization[*],nvidia-smi -i $1 --query-gpu=utilization.$2 --format=csv,noheader,nounitsUserParameter=gpu.power[*],nvidia-smi -i $1 --query-gpu=power.$2 --format=csv,noheader,nounits gpu.discovery是自动发现规则中一条规则的键值。 gpu.name[*]、gpu.mem[*]、gpu.utilization[*]、gpu.power[*]是监控项原型的键值。 Zabbix server按照查询间隔向主机发送请求，附带这些监控项中的键值，主机中的agent收到消息后，解析参数，并执行相应的命令，将执行结果返回给Zabbix server。 将zabbix_agentd.conf文件中的AllowRoot字段值配置为1，允许Zabbix以root权限执行命令。 最后重启agent使配置生效：systemctl restart zabbix-agent。 server配置在Zabbix server的web管理端创建模板gpu discovery。 创建名称为GPU的应用集。 创建自动发现规则discover gpu。 选择自动发现规则discover gpu的监控项原型，创建监控项原型。 参考资料： https://www.cnblogs.com/zhou2019/p/10864101.html https://zhuanlan.zhihu.com/p/434879809 https://blog.csdn.net/reblue520/article/details/115747023 https://blog.csdn.net/zxycyj1989/article/details/108650210 https://www.zutuanxue.com/home/4/7_359","categories":[{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"Zabbix","slug":"Zabbix","permalink":"http://wht6.github.io/tags/Zabbix/"}]},{"title":"理解Linux系统启动过程","slug":"理解Linux系统启动过程","date":"2022-03-14T09:00:00.000Z","updated":"2022-08-08T04:45:27.067Z","comments":true,"path":"posts/c9ee.html","link":"","permalink":"http://wht6.github.io/posts/c9ee.html","excerpt":"","text":"一直使用init 0关机，init 6重启，却不知道这里的init和数字代指的意思。init其实也是一个守护进程，只不过它比较特殊，它是系统启动的第一个守护进程，它是所有其他进程的直接或间接祖先，并且会持续运行到系统关闭。现在Centos 7采用Systemd作为其默认init程序。而后面的数字表示的是运行级别，这个将在下面介绍。 既然知道了系统启动的第一个进程是init，那就有必要了解一下Linux系统究竟是如何启动的。 系统的启动过程大致分为5个过程： 内核引导 系统初始化 用户登录系统 内核引导BIOS开机自检当计算机打开电源后，首先是 BIOS 开机自检，然后根据主板BIOS中的启动顺序，移交系统控制权。 当你打开计算机电源，计算机会首先加载BIOS信息，这是因为BIOS中包含了各种硬件的核心信息，包括CPU的相关信息、设备启动顺序信息、硬盘信息、内存信息、时钟信息、PnP特性等等。 然后按照 BIOS 中设置的启动设备（通常是硬盘）来启动。 MBR引导硬盘上第0磁道第一个扇区被称为MBR，也就是Master Boot Record，即主引导记录，它的大小是512字节，里面存放了预启动信息、分区表信息。 系统找到BIOS所指定的硬盘的MBR后，就会将其复制到0×7c00地址所在的物理内存中。其实被复制到物理内存的内容就是Boot Loader，具体可能是lilo或者grub等。 简单讲就是，根据MBR记录中的引导信息调用启动菜单。 GRUB程序系统控制权传递给GRUB后，将会显示启动菜单，然后根据所选项或默认值加载Linux内核文件，最后将系统控制权转交给内核。 Boot Loader就是在操作系统内核运行之前运行的一段小程序。通过这段小程序，我们可以初始化硬件设备、建立内存空间的映射图，从而将系统的软硬件环境带到一个合适的状态，以便为最终调用操作系统内核做好一切准备。 Boot Loader有若干种，其中Grub、Lilo和spfdisk是常见的Loader。 加载内核根据grub设定的内核映像所在路径，系统读取内存映像，并进行解压缩操作。此时，屏幕一般会输出“Uncompressing Linux”的提示。当解压缩内核完成后，屏幕输出“OK, booting the kernel”。系统将解压后的内核放置在内存之中，并调用start_kernel()函数来启动一系列的初始化函数并加载驱动以初始化各种设备，挂载根文件系统rootfs，完成Linux核心环境的建立。至此，Linux内核已经建立起来了，基于Linux的程序应该可以正常运行了。 默认的内核文件位于：/boot/vmlinux-版本信息。 系统初始化init是Linux系统中所有进程的祖宗。Linux内核首先会将系统中的/sbin/init进程加载到内存中，这一步骤会读取/etc/inittab文件，并依据此文件来进行初始化工作。然后负责完成一系列的系统初始化过程。 /etc/inittab文件最主要的作用就是设定Linux的运行等级，其设定形式是:id:5:initdefault:”，这就表明Linux需要运行在等级5上。 Linux的运行等级设定如下 运行级别 0：系统停机状态，系统默认运行级别不能设为 0，否则不能正常启动 运行级别 1：单用户工作状态，root 权限，用于系统维护，禁止远程登陆 运行级别 2：多用户状态 (没有 NFS) 运行级别 3：完全的多用户状态 (有 NFS)，登陆后进入控制台命令行模式 运行级别 4：系统未使用，保留 运行级别 5：X11 控制台，登陆后进入图形 GUI 模式 运行级别 6：系统正常关闭并重启，默认运行级别不能设为 6，否则不能正常启动 在设定了运行等级后，Linux系统执行的第一个用户层文件是/etc/rc.d/rc.sysinit脚本程序。它做的工作非常多，包括设定PATH、设定网络配置/etc/sysconfig/network、启动swap分区、设定/proc等等。然后根据运行级别的不同，系统会运行rc0.d到rc6.d中的相应的脚本程序，来完成相应的初始化工作和启动相应的服务。 但是这种依赖串行执行各种脚本的启动过程很慢，所有现在使用的是启动速度更快Systemd启动方式，运行的第一个init进程是/lib/systemd/systemd。 systemd能够将更多的服务进程并行启动，并且具有提供按需启动服务的能力，使得启动更少进程，从而提高系统启动速度。对于Systemd这里不进行深入介绍了，完成的功能就是把所有与系统相关的进程都启动起来，只不过是并行启动的，所以速度比较快。 用户登录系统最后一步就是执行/bin/login程序，进入登录状态。 login会接收mingetty传来的用户名并将其作为用户名参数，然后login会对用户名进行分析。主要是防止非法用户登录。 然后login将搜索/etc/passwd以及/etc/shadow来验证密码以及设置账户的其他信息，比如家目录的位置，Shell程序的位置。 login程序成功后，会向对应的终端在输出最近一次登录的信息(在/var/log/lastlog文件中有记录)，并检查用户是否有新邮件(在/var/spool/mail/的对应用户名目录里下)，然后开始设置各种环境变量。对应bash来说，系统首先寻找/etc/profile脚本并执行它；然后，如果用户的主目录中存在.bash_profile文件，就执行它，在这些文件中又可能调用了其他配置文件，所有的配置文件执行后，各种环境变量也设好了，这时会出现大家熟悉的命令提示符，到此整个启动过程就结束了。 参考链接： https://www.cnblogs.com/zyk01/p/11376445.html https://www.cnblogs.com/peiqingyi/p/14931972.html https://www.runoob.com/linux/linux-system-boot.html https://www.cnblogs.com/mylive/p/11280239.html","categories":[{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://wht6.github.io/tags/Linux/"}]},{"title":"SNMP协议复习","slug":"SNMP协议复习","date":"2022-03-13T08:00:00.000Z","updated":"2022-04-10T02:14:25.647Z","comments":true,"path":"posts/c501.html","link":"","permalink":"http://wht6.github.io/posts/c501.html","excerpt":"","text":"部署Zabbix的过程中得知其有用到SNMP协议，虽然之前学习过该协议，但已然忘记的差不多了，所有这篇文章来对SNMP协议的实现原理进行简单复习。 网络管理网络管理的定义 ：网络管理包括了硬件、 软件和人类元素的设置、 综合和协调， 以监视、 测试、 轮询、 配置、 分析、 评价和控制网络及网元资源， 用合理的成本满足实时性、 运营性能和服务质量的要求。 管理服务器（managing serve）是一个应用程序， 通常有人的参与， 并运行在网络运营中心（NOC）的集中式网络管理工作站上。 管理服务器是执行网络管理活动的地方， 它控制网络管理信息的收集、 处理、 分析和/或显示。 正是在这里， 发起控制网络行为的动作， 网络管理员与网络设备打交道。 被管设备（managed device）是网络装备的一部分（包括它的软件） ， 位于被管理的网络中。 被管设备可以是一台主机、 路由器、 交换机、 中间盒、 调制解调器、 温度计或其他联网的设备。 在一个被管设备中， 有几个所谓被管对象（managed object）。这些被管对象是被管设备中硬件的实际部分（例如， 一块网络接口卡只是一台主机或路由器的一个组件） 和用于这些硬件及软件组件的配置参数（例如， 像OSPF这样的AS内部路由选择协议） 一个被管设备中的每个被管对象的关联信息收集在管理信息库 (Management Information Base, MIB）中， 我们将看到这些信息的值可供管理服务器所用（并且在许多场合下能够被设置）。 一个MIB对象可以是： 一个计数器， 例如由于1P数据报首部差错而由路由器丢弃的IP数据报的数量， 或一台主机接收到的UDP报文段的数量； 运行在一台DNS服务器上的软件版本的描述性信息； 诸如一个特定设备功能是否正确的状态信息； 或诸如到一个目的地的路由选择路径的特定协议的信息。MIB 对象由称为 SMI （ Structure of Management Information）的数据描述语言所定义。 使用形式化定义语言可以确保网络管理数据的语法和语义是定义良好的和无二义性的。 相关的MIB对象被收集在MIB模块（module） 中。 在每个被管设备中还驻留有网络管理代理（network management agent）,它是运行在被管设备中的一个进程， 该进程与管理服务器通信， 在管理服务器的命令和控制下在被管设备中采取本地动作。 网络管理框架的最后组件是网络管理协议（network management protocol）。该协议运行在管理服务器和被管设备之间， 允许管理服务器查询被管设备的状态， 并经过其代理间接地在这些设备上采取行动。 代理能够使用网络管理协议向管理服务器通知异常事件（如组件故障或超过了性能阈值）。重要的是注意到网络管理协议自己不能管理网络。 恰恰相反， 它为网络管理员提供了一种能力， 使他们能够管理（“监视、 测试、 轮询、 配置、 分析、 评价和控制” ） 网络。 这是一种细微但却重要的区别。 SNMP简单网络管理协议（Simple Network Management Protocol）是一个应用层协议， 用于在管理服务器和代表管理服务器执行的代理之间传递网络管理控制和信息报文。 SNMP最常使用的是请求响应模式， 其中SNMP管理服务器向SNMP代理发送一个请求， 代理接收到该请求后， 执行某些动作， 然后对该请求发送一个回答。 请求通常用于查询（检索） 或修改（设置） 与某被管设备关联的MIB对象值。SNMP第二个常被使用的是代理向管理服务器发送的一种非请求报文， 该报文称为陷阱报文（trap message）。陷阱报文用于通知管理服务器， 一个异常情况（例如一个链路接口启动或关闭)已经导致了 MIB对象值的改变。 SNMPv2定义了的7种类型的报文， 这些报文一般称为协议数据单元(PDU)。 SNMPv2 PDU 类型 发送方-接收方 描述 GetRequest 管理者到代理 取得一个或多个MIB对象实例值 GetNextRequest 管理者到代理 取得列表或表格中下一个MIB对象实例值 GetBulkRequest 管理者到代理 以大数据块方式取得值， 例如大表中的值 InformRequest 管理者到管理者 向不能访问的远程管理实体通知MIB值 SetRequest 管理者到代理 设置一个或多个MIB对象实例的值 Response 代理到管理者或管理者到管理者 对Get Request, GetNextRequest. GetBulkRequest, Set RequestPDU,或InformRequest产生的响应 SNMPv2-Trap 代理到管理者 向管理者通知一个异常事件 下图是SNMP的PDU格式： GetRequest、GetNextRequest 和 GetBulkRequest PDU 都是管理服务器向代理发送的，以请求位于该代理所在的被管设备中的一个或多个MIB对象值。 其值被请求的MIB对象的对象标识符定义在该PDU的变量绑定部分。 GetRequest、GetNextRequest和GetBulkRequest的差异在于它们的数据请求粒度。 GetRequest能够请求M1B值的任意集合； 多个GetNextRequest能用于顺序地读取MIB对象的列表或表格；GetBulkRequest 允许读取大块数据， 能够避免因发送多个GetRequest或GetNextRequest报文可能导致的额外开销。 在所有这三种情况下， 代理用包括该对象标识符和它们相关值的Response PDU进行响应。 管理服务器使用SetRequest PDU来设置位于被管设备中的一个或多个MIB对象的值。 代理用带有“noError”差错状态的Response PDU进行应答， 以证实该值的确已被设置。 管理服务器使用InformRequest PDU来通知另一个MIB信息管理服务器， 后者对于接收服务器是远程的。 Response PDU通常从被管设备发送给管理服务器， 以响应来自该服务器的请求报文， 返回所请求的信息。 陷阱报文是异步产生的， 即它们不是为了响应接收到的请求而产生的， 而是为了响应管理服务器要求通知的事件而产生的。 RFC 3418定义了周知的陷阱类型， 其中包括设备的冷启动或热启动、 链路就绪或故障、 找不到相邻设备， 或鉴别失效事件。 接收到的陷阱请求不要求从管理服务器得到响应。 尽管SNMP PDU能够通过许多不同的运输协议传输， 但SNMP PDU通常是作为UDP数据报的载荷进行传的。 RFC 3417的确表明UDP是“首选的运输映射” 。 然而， 由于UDP是一种不可靠的运输协议， 因而不能确保一个请求或它的响应能够被它希望的目的地接收到。 管理服务器用该PDU的请求ID字段为它向代理发送的请求编号； 该代理的响应从接收到的请求中获取它的请求ID。因此， 该请求ID字段能被管理服务器用来检测丢失的请求或回答。 如果在一定时间后还没有收到对应的响应， 由管理服务器来决定是否重传一个请求。 特别是， SNMP标准没有强制任何特殊的重传过程， 即使初始进行重传。 它只是要求管理服务器“需要根据重传的频率和周期做岀负责任的动作” 。 &nbsp; 内容节选自：《计算机网络：自顶向下方法》","categories":[{"name":"网络","slug":"网络","permalink":"http://wht6.github.io/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"SNMP","slug":"SNMP","permalink":"http://wht6.github.io/tags/SNMP/"}]},{"title":"深入类和对象","slug":"深入类和对象","date":"2022-03-12T02:00:00.000Z","updated":"2022-03-20T01:59:38.267Z","comments":true,"path":"posts/9a3b.html","link":"","permalink":"http://wht6.github.io/posts/9a3b.html","excerpt":"","text":"鸭子类型：当看到一只鸟走起来像鸭子、游泳起来像鸭子、叫起来也像鸭子，那么这只鸟就可以被称为鸭子。 &nbsp; Java中实现多态的方式是定义一个父类，再定义一些子类，然后在这些子类中重写父类的方法。我们可以定义一个父类类型的变量并用子类来实例化它。&nbsp;1234567891011121314151617181920212223242526class Animal&#123; void say()&#123; System.out.println(&quot;i am a animal&quot;); &#125;&#125;class Cat extends Animal&#123; void say()&#123; System.out.println(&quot;i am a cat&quot;); &#125;&#125;class Dog extends Animal&#123; void say()&#123; System.out.println(&quot;i am a dog&quot;); &#125;&#125;class Duck extends Animal&#123; void say()&#123; System.out.println(&quot;i am a duck&quot;); &#125;&#125;Animal an = new Cat();an.say();&nbsp; 在Python中实现同样的效果，则不需要继承。下面是Python中的实现代码。 &nbsp;1234567891011121314class Cat(object): def say(self): print(&quot;i am a cat&quot;)class Dog(object): def say(self): print(&quot;i am a fish&quot;) class Duck(object): def say(self): print(&quot;i am a duck&quot;)animal = Catanimal().say()&nbsp; 在Python中animal就是一个动态类型，可以将任意的对象赋值给它。这样我们只需要在每个类中定义相同名称的方法，就可以轻松的实现多态。 &nbsp;123animal_list = [Cat, Dog, Duck]for animal in animal_list: animal().say()&nbsp; 这些实现同样方法的类的集合就可以划分成同一类型，这就是鸭子类型。 &nbsp;12345678910a = [&quot;bobby1&quot;, &quot;bobby2&quot;]b = [&quot;bobby2&quot;, &quot;bobby&quot;]name_tuple = (&quot;bobby3&quot;, &quot;bobby4&quot;)name_set = set()name_set.add(&quot;bobby5&quot;)name_set.add(&quot;bobby6&quot;)a.extend(b)a.extend(name_tuple)a.extend(name_set)&nbsp; 这里的extend()方法可以接收list、tuple、set，并没有限定一个特定的类型，只要是可迭代的类型都可以传进来，这个可迭代类型就是一个鸭子类型。结合魔法函数，我们可以自定义一个可迭代的类型，那么这个类型就也能够作为extend()方法的参数。 &nbsp;抽象基类又称为abc（abstract base class）模块。抽象基类类似于Java中的接口，都是不能实例化的。&nbsp;变量在Python中只是一个符号，它可以指向任意类型的对象，也就是变量本身的动态类型的属性就包含了多态。&nbsp;第一，抽象基类预先定义一些方法，所有继承抽象基类的子类都需要覆盖这些方法；第二，抽象基类无法被实例化。&nbsp; 抽象基类的作用&nbsp;用抽象基类去检查某个类是否具有某种方法，或者是否属于某种类型。&nbsp;12345678910class Company(object): def __init__(self, employee_list): self.employee = employee_list def __len__(self): return len(self.employee)com = Company([&quot;bobby1&quot;,&quot;bobby2&quot;])print(hasattr(com, &quot;__len__&quot;))&nbsp; 上面这种方法用到的是hasattr方法，其实我们更习惯的是判断Company是否是可测量长度的类型（Sized），就用到了抽象基类。Sized在collections.abc这个包当中，这个包主要是用来判断某个类型是否是具有某种属性的，里边除了用到了抽象基类，还用到了__subclasshook__这个魔法函数。 &nbsp;12from collections.abc import Sizedisinstance(com, Sized)&nbsp; 用抽象基类去强制某个子类必须实现某些方法。 &nbsp;比如，需要自定义一个web框架，但是为了保证后期的可扩展性和灵活性，我们想只定义方法而不实现方法的内容，让其子类去实现方法的内容，就可以用到抽象基类来解决。&nbsp;下面模拟一个抽象基类：&nbsp;1234567891011class CacheBase(): def get(self, key): raise NotImplementedError def set(self, key, value): raise NotImplementedError class RedisCache(CacheBase): passredis_cache = RedisCache()redis_cache.set(&quot;key&quot;, &quot;value&quot;)&nbsp; 当我们的子类中没有实现父类中的相关方法，而去调用该方法的时候就会抛出异常：NotImplementedError。 &nbsp;123456789101112class CacheBase(): def get(self, key): raise NotImplementedError def set(self, key, value): raise NotImplementedError class RedisCache(CacheBase): def set(self, key, value): passredis_cache = RedisCache()redis_cache.set(&quot;key&quot;, &quot;value&quot;)&nbsp; 当我们的子类中实现了父类中的相关方法，再去调用就不会抛异常。 &nbsp;这里只是在调用某个方法的时候才抛异常，我们更希望在初始化的时候就抛出异常。同时上面的方法也不能保证子类方法的全覆盖。&nbsp; 抽象基类的使用&nbsp;首先需要引用全局的abc包，然后继承元类abc.ABCMeta。这里的@abc.abstractmethod是装饰器。&nbsp;123456789101112131415import abcclass CacheBase(metaclass=abc.ABCMeta): @abc.abstractmethod def get(self, key): pass @abc.abstractmethod def set(self, key, value): pass class RedisCache(CacheBase): passredis_cache = RedisCache()&nbsp; 这段代码因为定义了抽象基类，所有在初始化的时候就会抛异常。 &nbsp;123class RedisCache(CacheBase): def set(self, key, value): pass&nbsp; 如果我们只实现了set方法，而没有实现get方法，那么在初始化的时候同样会抛异常。所有子类中必须实现继承的抽象基类中的所有方法。 &nbsp; isinstance和type&nbsp;123456789101112class A: passclass B(A): passb = B()print(isinstance(b, B))print(isinstance(b, A))print(type(b) is B)print(type(b) is A)&nbsp; 这段代码的输出结果是： &nbsp;1234TrueTrueTrueFalse&nbsp; isinstance可以去查找对象的继承链获取继承关系，而type查找的只是对象本身的类型而已。 &nbsp; 类变量和实例变量&nbsp;12345678class A: aa = 1 def __init__(self, x, y): self.x = x self.y = ya = A(2,3)print(a.x, a.y, a.aa)&nbsp; 这里边x和y都是实例变量（类里边的self代表的是一个实例），而aa是类变量。 &nbsp;如果我们打印A.aa也是卡可以的，但是如果我们打印A.x或A.y就会抛异常。我们在使用a.aa的时候，解释器会先去实例a里边查找，如果找不到，则会向上去它的类A里边查找。&nbsp;123A.aa = 11print(a.x, a.y, a.aa)print(A.a)&nbsp; 输出结果是： &nbsp;122 3 1111&nbsp; 如果我们改变了类变量的值，当我们再次通过实例来获取该值的时候，值也会改变，因为本质上还是查找到类变量。 &nbsp;123a.aa = 100print(a.x, a.y, a.aa)print(A.a)&nbsp; 输出结果是： &nbsp;122 3 10011&nbsp; 当我们给a.aa赋值的时候，实际上是在实例中新建了一个新的属性aa并赋值100。再去通过实例获取该值的时候，因为先去实例中查找，所以找到的就是100。当我们通过类去获取aa的值的时候还是11，实例的变化并不会影响到类。 &nbsp;在单继承的类里边，直接安装自下而上的顺序查找，对于包含多继承的继承链，查找顺序就是安装MRO算法来查找。Python中的MRO（方法查询顺序）算法用的是C3算法。最早用的是深度优先算法，后来改为广度优先算法，但是这两张方法都有问题，现在用的是比较复杂的C3算法。&nbsp;我们并不需要知道C3算法去推算方法查找的顺序，我们可以直接调用魔法函数__mro__来获取方法查找顺序。&nbsp; 类方法、静态方法和实例方法&nbsp;123456789101112131415161718class Date: #构造函数 def __init__(self, year, month, day): self.year = year self.month = month self.day = day def tomorrow(self): self.day += 1 def __str__(self): return &quot;&#123;year&#125;/&#123;month&#125;/&#123;day&#125;&quot;.format(year=self.year, month=self.month, day=self.day) if __name__ == &quot;__main__&quot;: new_day = Date(2022, 3, 12) new_day.tomorrow() print(new_day)&nbsp; self关键字代表的就是实例，而对应的方法就是实例方法。我们在调用实例方法的时候不用将实例作为参数进行传递，Python解释器会自动的帮我们完成。 &nbsp;如果我们想在初始化的时候传递一个字符串进去，就可以加入一个静态方法。&nbsp;1234567891011121314151617181920212223class Date: #构造函数 def __init__(self, year, month, day): self.year = year self.month = month self.day = day def tomorrow(self): self.day += 1 @staticmethod def parse_from_string(date_str): year, month, day = tuple(date_str.split(&quot;-&quot;)) return Date(int(year), int(month), int(day)) def __str__(self): return &quot;&#123;year&#125;/&#123;month&#125;/&#123;day&#125;&quot;.format(year=self.year, month=self.month, day=self.day) if __name__ == &quot;__main__&quot;: date_str = &quot;2018-12-31&quot; new_day = Date.parse_from_string(date_str) print (new_day)&nbsp; 静态方法的参数列表中，既没有代表实例的self，也没有类本身。但是我们在调用的时候要从类里边调用，因为它属于类的命名空间。但是静态方法如果用到了类本身，比如上面代码中的parse_from_string方法在返回值的地方用到了类本身Date，当我们更改类名称的时候，方法里边的类名称也要修改。所以如果方法中涉及到类本身，最好使用类方法。 &nbsp;1234567891011121314151617181920212223class Date: #构造函数 def __init__(self, year, month, day): self.year = year self.month = month self.day = day def tomorrow(self): self.day += 1 @classmethod def from_string(cls, date_str): year, month, day = tuple(date_str.split(&quot;-&quot;)) return cls(int(year), int(month), int(day)) def __str__(self): return &quot;&#123;year&#125;/&#123;month&#125;/&#123;day&#125;&quot;.format(year=self.year, month=self.month, day=self.day) if __name__ == &quot;__main__&quot;: date_str = &quot;2018-12-31&quot; new_day = Date.from_string(date_str) print (new_day)&nbsp; 这里的from_string就是一个类方法，cls代表的是类本身。当我们不涉及类本身的使用的时候就可以用静态方法。 &nbsp;1234567@staticmethod def valid_str(date_str): year, month, day = tuple(date_str.split(&quot;-&quot;)) if int(year)&gt;0 and (int(month) &gt;0 and int(month)&lt;=12) and (int(day) &gt;0 and int(day)&lt;=31): return True else: return False&nbsp; 类方法前边必须加装饰器@classmethod，静态方法前边必须加装饰器@staticmethod，实例方法不用加装饰器。 &nbsp;Python中没有private和protected关键字，那么它是如何实现私有属性的呢？&nbsp;1234567891011121314from chapter04.class_method import Dateclass User: def __init__(self, birthday): self.__birthday = birthday def get_age(self): #返回年龄 return 2018 - self.__birthday.yearif __name__ == &quot;__main__&quot;: user = User(Date(1990,2,1)) # print(user.__birthday) # 这么调用会报错 print(user.get_age())&nbsp; 通过在变量名的前边加上双下划线来表明该变量是私有变量，只能在定义它的类的内部使用。实际上的内部实现是把变量变成_class_attribute的形式，这里我们通过访问user._User__birthday还是可以访问的，所以Python的私有属性只是在形式上的私有，并非强制的私有。另外，私有方法也是在方法名的前边加上双下划线。 &nbsp;123456789101112131415class Person: &quot;&quot;&quot; 人 &quot;&quot;&quot; name = &quot;user&quot;class Student(Person): def __init__(self, scool_name): self.scool_name = scool_nameif __name__ == &quot;__main__&quot;: user = Student(&quot;慕课网&quot;) #通过__dict__查询属性 print(user.__dict__) print(user.name)&nbsp; 这段代码的输出结果是： &nbsp;12&#123;&#39;scool_name&#39;: &#39;慕课网&#39;&#125;user&nbsp; __dict__是用来查询对象的属性的，包括部分的隐藏的属性。通过__dict__方法无法查询到name属性，因为name属性并不在实例user里边，但是我们在使用user.name的时候，根据MRO算法会向上查询到它的父类，进而查询到该属性。 &nbsp;如果我们查询类Person的属性，就会发现更为的丰富，因为类是一种特殊的对象，并且包含了更多的隐藏的属性。如果打印Person.__dict__，输出如下：&nbsp;1&#123;&#39;__module__&#39;: &#39;__main__&#39;, &#39;__doc__&#39;: &#39;\\n 人\\n &#39;, &#39;name&#39;: &#39;user&#39;, &#39;__dict__&#39;: &lt;attribute &#39;__dict__&#39; of &#39;Person&#39; objects&gt;, &#39;__weakref__&#39;: &lt;attribute &#39;__weakref__&#39; of &#39;Person&#39; objects&gt;&#125;&nbsp; 并且我们还能够直接使用__dict__方法来更改属性。比如： &nbsp;12user.__dict__[&quot;school_addr&quot;] = &quot;北京市&quot;print(user.school_addr)&nbsp; 我们添加了一个新的属性school_addr并打印了它的值。 &nbsp;Python还提供了一个全局的方法dir(a)用于查询对象的所有属性，包括所有隐藏的属性。&nbsp;Python中我们如何调用父类的方法呢？这里就要用到super()方法。&nbsp;1234567891011class A: def __init__(self): print (&quot;A&quot;)class B(A): def __init__(self): print (&quot;B&quot;) super().__init__()if __name__ == &quot;__main__&quot;: b = B()&nbsp; 再来看一段代码，这段代码中包含了多继承。 &nbsp;123456789101112131415161718192021class A: def __init__(self): print (&quot;A&quot;)class B(A): def __init__(self): print (&quot;B&quot;) super().__init__()class C(A): def __init__(self): print (&quot;C&quot;) super().__init__() class D(B, C): def __init__(self): print (&quot;D&quot;) super(D, self).__init__()if __name__ == &quot;__main__&quot;: d = D()&nbsp; super()并不是调用它父类中的方法，而是调用MRO算法查找到的下一个类的方法。所有上面这段代码的输出结果是： &nbsp;1234DBCA&nbsp; 我们可以通过打印D.__mro__来验证。 &nbsp;我们在使用Python的过程中并不推荐使用多继承，因为这样可能会导致继承关系的混乱，还有可能出现一些意想不到的问题。但是这并不意味着多继承就不能用了，如果能提前设计好框架，提前做好预防，还是可以使用的，比如Python中的一种混合模式mixin。&nbsp;mixin模式会定义许多mixin类，每一个类只包含单一的功能，我们通过继承这些类来直接使用mixin类的功能。并且mixin类中都不会包含super方法。&nbsp; 上下文管理器&nbsp;123456789101112131415161718def exe_try(): try: print (&quot;code started&quot;) raise KeyError return 1 except KeyError as e: print (&quot;key error&quot;) return 2 else: print (&quot;other error&quot;) return 3 finally: print (&quot;finally&quot;) return 4if __name__ == &quot;__main__&quot;: result = exe_try() print (result)&nbsp; 首先正常的try语句的执行顺序是先执行try里面的代码，如果抛出异常，就进而去执行except中捕获异常的代码，如果没有抛出异常，就执行else中的代码，然后不管怎样finally中的代码都会在最后执行。所有我们通常会在try里面获取资源，在finally中释放资源。 &nbsp;那么为什么这段代码中的print (result)的结果是4呢？这是因为在执行到except中时，会把return的返回值压入栈中，到了finally中又把return的返回值压入栈中，最后从栈顶获取一个返回值，就是4。&nbsp;上下文管理器就是Python设计用来简化try结构的。&nbsp;1234567891011121314# 上下文管理器class Sample: def __enter__(self): print (&quot;enter&quot;) #获取资源 return self def __exit__(self, exc_type, exc_val, exc_tb): #释放资源 print (&quot;exit&quot;) def do_something(self): print (&quot;doing something&quot;) with Sample() as sample: sample.do_something() &nbsp; 这段代码的输出结果如下： &nbsp;123enterdoing somethingexit&nbsp; 当我们定义了__enter__和__exit__这两个魔法函数的时候，就可以使用with语句来实例化这个类，并且在进入with语句的时候会去自动执行__enter__方法，退出with语句的时候会去自动执行__exit__方法。 &nbsp;利用Python提供的contextlib包还能够进一步简化上下文管理器。通过使用装饰器@contextlib.contextmanager我们可以直接把一个函数变成一个上下文管理器。&nbsp;12345678910import contextlib@contextlib.contextmanagerdef file_open(file_name): print (&quot;file open&quot;) yield &#123;&#125; print (&quot;file end&quot;)with file_open(&quot;bobby.txt&quot;) as f_opened: print (&quot;file processing&quot;)&nbsp; yield &#123;&#125;是一个生成器，它之前的代码就相当于__enter__方法中的代码，它之后的代码就相当于__exit__方法中的代码。","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Python语言","slug":"Python语言","permalink":"http://wht6.github.io/tags/Python%E8%AF%AD%E8%A8%80/"}]},{"title":"部署和使用Zabbix","slug":"部署和使用Zabbix","date":"2022-03-11T09:00:00.000Z","updated":"2022-07-26T11:22:47.417Z","comments":true,"path":"posts/ab79.html","link":"","permalink":"http://wht6.github.io/posts/ab79.html","excerpt":"","text":"Linux中的监控命令CPU监控 12345678# 查看系统当前负载情况w# 包括系统运行时间，登录用户，终端，登录时间等# 动态查看进程，cpu，内存等tophtopglances 磁盘监控 12345678# 监控磁盘的使⽤率df -hT# 查看根目录磁盘使用率df -hT|awk &#x27;/\\/$/&#123;print$(NF-1)&#125;&#x27;# 监控磁盘IOiotopiostat 内存监控 1free -m 网络监控 12345678910111213# 查看ip信息ifconfigip aip r# 查看网络信息netstat# 查看路由信息netstat -rn# 流量监控iftop# 中间的&lt;= =&gt;这两个左右箭头，表示的是流量的⽅向。# TX：发送流量、RX：接收流量、TOTAL：总流量 Zabbix介绍zabbix是一个基于Web界面的，提供分布式系统监控以及网络监视功能的企业级的开源解决方案。 架构Zabbix主要由两部分构成Zabbix server和Zabbix agent。 Server：Zabbix server 是 Zabbix 软件的核心组件，agent 向其报告可用性、系统完整性信息和统计信息。server也是存储所有配置信息、统计信息和操作信息的核心存储库。 Agent：Zabbix agents部署在被监控目标上，用于主动监控本地资源和应用程序，并将收集的数据发送给 Zabbix server。 Zabbix还包括数据库、Web界面和Proxy。 数据库：所有配置信息以及 Zabbix 收集到的数据都被存储在数据库中。 Web界面：为了从任何地方和任何平台轻松访问 Zabbix ，我们提供了基于 web 的界面。该界面是 Zabbix server 的一部分，通常（但不一定）和 Zabbix server 运行在同一台物理机器上。 Proxy：Zabbix proxy 可以替 Zabbix server 收集性能和可用性数据。Zabbix proxy 是 Zabbix 环境部署的可选部分；然而，它对于单个 Zabbix server 负载的分担是非常有益的。 原理Zabbix 通过C/S 模式采集数据，通过B/S模式在web 端展示和配置。 被监控端：主机通过安装agent 方式采集数据，网络设备通过SNMP 方式采集数据 Server 端：通过收集SNMP 和agent 发送的数据，写入数据库（MySQL，ORACLE 等），再通过php+apache 在web 前端展示。 zabbix agent安装在被监控的主机上，zabbix agent负责定期收集客户端本地各项数据，并发送至 zabbix server 端，zabbix server 收到数据后，将数据存储到数据库中，用户基于 Zabbix WEB 可以看到数据在前端展现图像。当 zabbix 监控某个具体的项目， 该项目会设置一个触发器阈值，当被监控的指标超过该触发器设定的阈值，会进行一些必要的动作，动作包括：发送信息（邮件、微信、短信）、发送命令（shell 命令、reboot、restart、install 等）。 监控架构 Server-Client：这是Zabbix较为简单的架构，监控与被监控节点之间不通过任何代理，直接由Server与Agent进行数据交互Master-Node-Client：这种架构每个Node是一个Server，它们有自己的配置文件，Node下面可以接Proxy，也可以直接与Agent进行交互Server-Proxy-Client： Proxy没有前端，本身也不存放数据，只是将Agent转发过来的数据暂时存放，最终还是要交给Server，这种架构一般用于跨机房或者跨网络 相关名词 zabbix-server：zabbix的控制中心，收集数据，写入数据zabbix-agent：部署在被监控端的一个程序，用于收集本机信息给服务端host：服务器item：监控项，例如监控cpu平均负载就是一个监控项，每隔一段时间获取一个值反馈到服务端trigger：触发器，一些逻辑规则的组合，他有三个值，正常，异常，未知。当监控项获取的值达到设定的阈值的时候，就会触发action：当trigger符合某个值的时候，就会触发操作，比如发送邮件 运行条件 Server：Zabbix Server 需运行在LAMP（Linux+Apache+Mysql+PHP）环境下（或者LNMP），对硬件要求低 Agent：目前已有的agent 基本支持市面常见的OS，包含Linux、HPUX、Solaris、Sun、 windows SNMP：支持各类常见的网络设备 SNMP(Simple Network Management Protocol,简单网络管理协议 Zabbix进程 zabbix_agentd：客户端守护进程，此进程收集客户端数据，例如cpu负载、内存、硬盘使用情况等 zabbix_get：zabbix工具，单独使用的命令，通常在server或者proxy端执行获取远程客户端信息的命令。通常用户排错。例如在server端获取不到客户端的内存数据，我们可以使用zabbix_get获取客户端的内容的方式来做故障排查。 zabbix_sender：zabbix工具，用于发送数据给server或者proxy，通常用于耗时比较长的检查。很多检查非常耗时间，导致zabbix超时。于是我们在脚本执行完毕之后，使用sender主动提交数据。 zabbix_server：zabbix服务端守护进程。zabbix_agentd、zabbix_get、zabbix_sender、zabbix_proxy、zabbix_java_gateway的数据最终都是提交到server。备注：当然不是数据都是主动提交给zabbix_server,也有的是server主动去取数据。 zabbix_proxy：zabbix代理守护进程。功能类似server，唯一不同的是它只是一个中转站，它需要把收集到的数据提交/被提交到server里。为什么要用代理？代理是做什么的？卖个关子，请继续关注运维生存时间zabbix教程系列。 zabbix_java_gateway：zabbix2.0之后引入的一个功能。顾名思义：Java网关，类似agentd，但是只用于Java方面。需要特别注意的是，它只能主动去获取数据，而不能被动获取数据。它的数据最终会给到server或者proxy。 Zabbix安装部署安装Zabbix12345678910111213141516171819# Install Zabbix repositoryrpm -Uvh https://repo.zabbix.com/zabbix/5.0/rhel/7/x86_64/zabbix-release-5.0-1.el7.noarch.rpmyum clean all# Install Zabbix server and agentyum install zabbix-server-mysql zabbix-agent# Install Zabbix frontend# Enable Red Hat Software Collectionsyum install centos-release-scl# Edit file /etc/yum.repos.d/zabbix.repo and enable zabbix-frontend repository.[zabbix-frontend]...enabled=1...# Install Zabbix frontend packages.yum install zabbix-web-mysql-scl zabbix-apache-conf-scl 初始化数据库到MySQL官网找到MySQL Yum Repository，选择Linux 7（我的系统是centos 7），复制下载链接。 1234567891011121314# 下载wget https://dev.mysql.com/get/mysql80-community-release-el7-5.noarch.rpm# 安装yum仓库rpm -ivh mysql80-community-release-el7-5.noarch.rpm # 修改mysql源，启用mysql57，即设置enabled=1，禁用mysql80，即设置enabled=0vim /etc/yum.repos.d/mysql-community.repo# 安装mysqlyum install -y mysql-community-server # debian apt install mysql-server # 启动serversystemctl start mysqld# debian systemctl start mysql.service 修改mysql密码： 123456789101112131415161718192021222324252627282930grep password /var/log/mysqld.log# 查看默认密码，最后一个字段是默认密码mysql -uroot -p&#x27;z,wzhsXeO9q*&#x27;# 登录set global validate_password_policy=LOW;set global validate_password_length=4; # 降低密码强度alter user &#x27;root&#x27;@&#x27;localhost&#x27; identified by &#x27;123456&#x27;;# 设置新密码mysql -uroot -p123456set password=password(&#x27;123456&#x27;);# 重新设置密码grant all privileges on *.* to &#x27;root&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27; with grant option;# MySQL允许远程登录###debian 修改密码cat /etc/mysql/debian.cnfmysql -u debian-sys-maint -puse mysql;update user set authentication_string=PASSWORD(&#x27;123456&#x27;) where user=&#x27;root&#x27;;update user set plugin=&quot;mysql_native_password&quot;;flush privileges;mysql -uroot -p123456grant all privileges on *.* to &#x27;root&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27; with grant option; 创建Zabbix数据库 1234create database zabbix character set utf8 collate utf8_bin;create user zabbix@localhost identified by &#x27;password&#x27;;# 这里密码设置为passwordgrant all privileges on zabbix.* to zabbix@localhost;exit 导入zabbix初始数据(166张表) 12zcat /usr/share/doc/zabbix-server-mysql*/create.sql.gz | mysql -uzabbix -p zabbix# 这里填zabbix设置的password 配置Zabbix server的数据库配置 12vim /etc/zabbix/zabbix_server.conf Zabbix前端配置12345678vim /etc/opt/rh/rh-php72/php-fpm.d/zabbix.confphp_value[date.timezone] = Asia/Shanghai #修改时区,前边的;去掉# debian修改/etc/zabbix/apache.conf，php_value date.timezone Asia/Shanghai# 开启Zabbix server和agent，并设置开机自启systemctl restart zabbix-server zabbix-agent httpd rh-php72-php-fpmsystemctl enable zabbix-server zabbix-agent httpd rh-php72-php-fpm 访问前端：http://localhost/zabbix. nextstep —- nextstep 使用密码连接数据库，nextstep —— nextstep ——- nextstep —— finish 登录zabbix：默认的账号及密码： 用户名：Admin，密码：zabbix 更新密码：usersetting ——- change password，设置密码，语言选中文，update Agent配置Debian安装Agent。 1234567apt install zabbix-agentvim /etc/zabbix/zabbix_agentd.confServer=192.168.91.129 # 更改serverIPsystemctl start zabbix-agentsystemctl enable zabbix-agent Centos安装Agent。 12345678910rpm -ivh https://mirror.tuna.tsinghua.edu.cn/zabbix/zabbix/5.0/rhel/7/x86_64/zabbix-agent-5.0.1-1.el7.x86_64.rpmrpm -ql zabbix-agent # 列出与zabbix-agent相关的包vim /etc/zabbix/zabbix_agentd.confServer=192.168.91.129 # 更改serverIPsystemctl start zabbix-agentsystemctl enable zabbix-agentnetstat -antp # Zabbix-Agent默认监听在 10050 端⼝ 进入Server的Web前端，点击配置 —— 主机 ——- 创建主机 设置主机名，可见名，群组，填写客户端IP（我的是192.168.91.128） 点击模板，选择模板Template OS Linux by Zabbix agent，添加 查看检测主机Host。（这里发现中文乱码，又改回英文） &nbsp; 参考链接： 官方下载 https://www.zabbix.com/documentation/5.2/zh/manual http://www.manongjc.com/detail/15-otkezaadqeolbnm.html https://www.cnblogs.com/rxysg/p/15698608.html https://www.cnblogs.com/zhangyupengzuishuai/p/15354330.html","categories":[{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"Zabbix","slug":"Zabbix","permalink":"http://wht6.github.io/tags/Zabbix/"},{"name":"监控","slug":"监控","permalink":"http://wht6.github.io/tags/%E7%9B%91%E6%8E%A7/"}]},{"title":"Python常用包","slug":"Python常用包","date":"2022-03-10T08:00:00.000Z","updated":"2022-03-30T11:58:56.930Z","comments":true,"path":"posts/8322.html","link":"","permalink":"http://wht6.github.io/posts/8322.html","excerpt":"","text":"argparseargparse 是python自带的命令行参数解析包。 argparse.ArgumentParser函数用于生成argparse对象，通常传入一个description参数，将这个程序的描述信息赋值给该参数。 add_argument函数用于添加一个新的参数，并设置与该参数相关的信息。 12parser = argparse.ArgumentParser(description=&quot;Demo of argparse&quot;)parser.add_argument(&#x27;--name&#x27;, default=&#x27;Great&#x27;) 这里的name就是添加的参数，default是默认值。除此之外，常用的关于参数的其他配置信息有： required，此值为真，则参数必须设置，若命令中没有该参数则程序会报错。 type，限制参数的类型。 choices，限制参数值的选项，通常是给定一个列表。 help，参数的说明信息。 dest，设置参数在对象中的变量名，默认的变量名是--或-后面的字符串。 nargs，设置参数值的个数。 action，命令行遇到参数时的动作，默认值是 store。 当我们在命令行输入一串参数后，对于不同类型的参数是希望做不同的处理的。 那么参数动作action其实就是告诉解析器，我们希望对应的参数该被如何处理。action的类别如下： store —— 保存参数的值，这是默认的参数动作。 store_const —— 保存被 const 命名的固定值。 store_true 和 store_false —— 是 store_const 的特殊情况，用来分别保存 True 和 False。 append —— 将参数值追加保存到一个列表中。 append_const —— 将 const 命名的固定值追加保存到一个列表中（const 的默认值为 None）。 count —— 计算参数出现次数 help —— 打印解析器中所有选项和参数的完整帮助信息，然后退出。 version —— 打印命令行版本，通过指定 version 入参来指定版本，调用后退出。 socketsocket模块主机间或者一台计算机上的进程间通信。 socket函数用于创建套接字。 123456789from socket import *socket(socket_family,socket_type,protocal=0)# socket_family 可以是 AF_UNIX 或 AF_INET。socket_type 可以是 SOCK_STREAM 或 SOCK_DGRAM。protocol 一般不填,默认值为 0。# AF_UNIX：Uxin不同进程间的通讯。# AF_INET：主机之间通讯（AF_INET6代表ipv6）# SOCK_STREAM：TCP类型# SOCK_DGRAM：UDP类型# 函数返回一个socket对象 socket对象包含的方法： UDP相关 12sendto() # 发送UDP数据recvfrom() # 接收UDP数据 服务端 1s.bind() # 绑定(主机,端口号)到套接字 TCP相关 12345listen() # 开启TCP监听connect() # 客户端发送TCP连接请求send() # 发送TCP数据recv() # 接收TCP数据accept() # 接受TCP客户端连接，生成一个新的socket，用于该客户与服务器的通信 其他 1close() # 关闭套接字 进程和socket并非一一对应，socket是一种资源， 一个进程可以对应多个socket，但是一个socket只能归属一个进程。一个进程可以对应多个端口，但是一个端口只能被用于标识一个进程。 在同一台主机上，一个socket包含一个端口，一个端口可以被写进多个socket，端口和socket是1对多的关系。 服务器进程的欢迎套接字用于监听连接请求，当监听到客户端一个请求过来的时候，这个欢迎socket会生成一个新的连接socket，由这连接socket与客户端的socket相连接，这个新的socket并不会换一个端口，还是80，于是在服务器端的80端口上就有2个socket了。随着更多的客户端向这台服务器发来连接请求（假设连接都是用于HTTP），服务器80端口就会有更多的socket。 但是如何区分这些socket呢？因为socket是四元组唯一标识，四元组包括源ip和源port，目的ip和目的port，这些连接socket的目的ip和port虽然相同，但是源ip和port不同。 为了提高性能，防止阻塞，服务进程通常需要为每个新的客户连接创建一个具有新连接套接字的新线程。","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://wht6.github.io/tags/Python/"}]},{"title":"虚拟机的网络配置","slug":"虚拟机的网络配置","date":"2022-03-03T09:00:00.000Z","updated":"2022-04-09T06:01:06.856Z","comments":true,"path":"posts/c842.html","link":"","permalink":"http://wht6.github.io/posts/c842.html","excerpt":"","text":"虚拟机的网络配置NAT模式配置进入虚拟网络编辑器，将网络设置为NAT模式。 该模式可以使所有虚拟机位于同一个局域网中，从而保证彼此通信，并且可以共享主机公网。 NAT的设置包括： 网段设置，设置子网IP和子网掩码。例如：192.168.91.0和255.255.255.0。 网关设置，选择网段中的一个IP作为网关的IP，例如192.168.91.1。 dhcp设置，设定起始地址和终止地址，任意设置即可。 注意：这里边有个坑，因为除了虚拟网络编辑器，在虚拟机设置里边也有关于网络模式的设置，这里边的模式和虚拟网络编辑器的模式需要一致，否则网络不能用。所以这里边也需要设置为NAT模式。 然后我们就可以配置虚拟机中的网络配置文件了。 虚拟主机网卡配置编辑文件/etc/sysconfig/network-scripts/ifcfg-ens33。 123456789101112131415TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=noneIPADDR=192.168.91.128NETMASK=255.255.255.0GATEWAY=192.168.91.1DNS1=114.114.114.114DNS2=8.8.8.8DEFROUTE=yesIPV4_FAILURE_FATAL=noNAME=ens33UUID=cc78b1ae-eeba-42f1-b79c-d69e1c1f3fceDEVICE=ens33ONBOOT=yes 现在可以测试以下局域网的联通性： 1234systemctl restart network # 重启网络服务ip a # 查看IPip r # 查看网关地址ping 192.168.91.1 # ping网关 开启网络共享Windows上找到网络和Internet设置，更改适配器，将公网所在的网卡的属性中的共享功能开启，共享的网卡选择虚拟机的网卡，如VMnet8。 这个网卡可以认为是虚拟网关的网卡。 然后测试公网联通性; 1234systemctl restart network # 重启网络服务ping 192.168.91.1 # ping网关ping www.baidu.com # ping百度elinks www.baidu.com # 浏览器访问百度（如同ping包被过滤） 端口映射设置端口映射主要是为了使主机和虚拟主机进行连接，例如，主机通过SSH连接虚拟主机。 从主机上查看虚拟网卡的IP地址，ipconfig。例如，我的VMnet8的地址是169.254.154.79。 然后，进入虚拟网络编辑器，进入NAT设置。 添加端口映射，例如，我添加了一个端口映射，主机端口为22，虚拟机IP为192.168.91.128，虚拟机端口为22。 当你访问网关的22端口的时候，会映射到虚拟主机的22端口。 扩展：虚拟机复制粘贴失败，vmware tools灰色的解决方法。 在虚拟机设置的CDDVD中映像文件选择vmware安装路径中自带的linux.iso文件，在虚拟机中重新挂载dev中的cdrom，因为只能以只读的方式挂载，所以要把压缩包复制出去在解压，然后运行里面的pl文件重新安装vmware tools。 扩展：ssh更换端口办法。 1234vim /etc/ssh/sshd_config# 找到“#Port 22”，这一行直接键入“yyp”复制该行到下一行，然后把两行的“#”号即注释去掉，修改成Port 10086 开放selinux的对应端口。 先查看SELinux开放给ssh使用的端口 123semanage port -l|grep ssh# 我的系统打印如下：ssh_port_t tcp 22 可知，SELinux没有给SSH开放10086端口，那么我们来添加该端口： 1semanage port -a -t ssh_port_t -p tcp 10086 完成后，再次查看 12semanage port -l|grep sshssh_port_t tcp 22，10086 或者直接关闭selinux。 1234567# 查看状态getenforce# 临时关闭setenforce 0# 永久关闭vi /etc/selinux/config# 将SELINUX=enforcing改为SELINUX=disabled 防火墙开放对应端口： 12345678# 查看是否开放firewall-cmd --permanent --query-port=10086/tcp# 添加端口firewall-cmd --permanent --add-port=10086/tcp# 重载策略firewall-cmd --reload# 查看是否开放firewall-cmd --permanent --query-port=10086/tcp 或者直接关闭防火墙 1systemctl stop firewalld 添加端口映射，主机端口为10086，虚拟机IP为192.168.91.129，虚拟机端口为10086。 重启ssh。 1systemctl restart sshd 查看ssh服务端口是否改为10086 1netstat -antp | grep &quot;ssh&quot;","categories":[{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"网络","slug":"网络","permalink":"http://wht6.github.io/tags/%E7%BD%91%E7%BB%9C/"}]},{"title":"深入解析Pod对象","slug":"深入解析Pod对象","date":"2022-03-01T08:00:00.000Z","updated":"2022-05-21T07:51:54.581Z","comments":true,"path":"posts/3009.html","link":"","permalink":"http://wht6.github.io/posts/3009.html","excerpt":"","text":"原文链接： https://time.geekbang.org/column/100015201 为什么我们需要PodPod，是 Kubernetes 项目中最小的 API 对象。如果换一个更专业的说法，我们可以这样描述：Pod，是 Kubernetes 项目的原子调度单位。 不过，我相信你在学习和使用 Kubernetes 项目的过程中，已经不止一次地想要问这样一个问题：为什么我们会需要 Pod？ 要回答这个问题，我们还是要一起回忆一下我曾经反复强调的一个问题：容器的本质到底是什么？ 你现在应该可以不假思索地回答出来：容器的本质是进程。 没错。容器，就是未来云计算系统中的进程；容器镜像就是这个系统里的“.exe”安装包。那么 Kubernetes 呢？ 你应该也能立刻回答上来：Kubernetes 就是操作系统！ 非常正确。 现在，就让我们登录到一台 Linux 机器里，执行一条如下所示的命令： 1$ pstree -g 这条命令的作用，是展示当前系统中正在运行的进程的树状结构。它的返回结果如下所示： 123456789101112131415161718systemd(1)-+-accounts-daemon(1984)-+-&#123;gdbus&#125;(1984) | `-&#123;gmain&#125;(1984) |-acpid(2044) ... |-lxcfs(1936)-+-&#123;lxcfs&#125;(1936) | `-&#123;lxcfs&#125;(1936) |-mdadm(2135) |-ntpd(2358) |-polkitd(2128)-+-&#123;gdbus&#125;(2128) | `-&#123;gmain&#125;(2128) |-rsyslogd(1632)-+-&#123;in:imklog&#125;(1632) | |-&#123;in:imuxsock) S 1(1632) | `-&#123;rs:main Q:Reg&#125;(1632) |-snapd(1942)-+-&#123;snapd&#125;(1942) | |-&#123;snapd&#125;(1942) | |-&#123;snapd&#125;(1942) | |-&#123;snapd&#125;(1942) | |-&#123;snapd&#125;(1942) 不难发现，在一个真正的操作系统里，进程并不是“孤苦伶仃”地独自运行的，而是以进程组的方式，“有原则地”组织在一起。比如，这里有一个叫作 rsyslogd 的程序，它负责的是 Linux 操作系统里的日志处理。可以看到，rsyslogd 的主程序 main，和它要用到的内核日志模块 imklog 等，同属于 1632 进程组。这些进程相互协作，共同完成 rsyslogd 程序的职责。 注意：我在本篇中提到的“进程”，比如，rsyslogd 对应的 imklog，imuxsock 和 main，严格意义上来说，其实是 Linux 操作系统语境下的“线程”。这些线程，或者说，轻量级进程之间，可以共享文件、信号、数据内存、甚至部分代码，从而紧密协作共同完成一个程序的职责。所以同理，我提到的“进程组”，对应的也是 Linux 操作系统语境下的“线程组”。这种命名关系与实际情况的不一致，是 Linux 发展历史中的一个遗留问题。 而 Kubernetes 项目所做的，其实就是将“进程组”的概念映射到了容器技术中，并使其成为了这个云计算“操作系统”里的“一等公民”。 Kubernetes 项目之所以要这么做的原因，我在前面介绍 Kubernetes 和 Borg 的关系时曾经提到过：在 Borg 项目的开发和实践过程中，Google 公司的工程师们发现，他们部署的应用，往往都存在着类似于“进程和进程组”的关系。更具体地说，就是这些应用之间有着密切的协作关系，使得它们必须部署在同一台机器上。 而如果事先没有“组”的概念，像这样的运维关系就会非常难以处理。 我还是以前面的 rsyslogd 为例子。已知 rsyslogd 由三个进程组成：一个 imklog 模块，一个 imuxsock 模块，一个 rsyslogd 自己的 main 函数主进程。这三个进程一定要运行在同一台机器上，否则，它们之间基于 Socket 的通信和文件交换，都会出现问题。 现在，我要把 rsyslogd 这个应用给容器化，由于受限于容器的“单进程模型”，这三个模块必须被分别制作成三个不同的容器。而在这三个容器运行的时候，它们设置的内存配额都是 1 GB。 再次强调一下：容器的“单进程模型”，并不是指容器里只能运行“一个”进程，而是指容器没有管理多个进程的能力。这是因为容器里 PID=1 的进程就是应用本身，其他的进程都是这个 PID=1 进程的子进程。可是，用户编写的应用，并不能够像正常操作系统里的 init 进程或者 systemd 那样拥有进程管理的功能。比如，你的应用是一个 Java Web 程序（PID=1），然后你执行 docker exec 在后台启动了一个 Nginx 进程（PID=3）。可是，当这个 Nginx 进程异常退出的时候，你该怎么知道呢？这个进程退出后的垃圾收集工作，又应该由谁去做呢？ 假设我们的 Kubernetes 集群上有两个节点：node-1 上有 3 GB 可用内存，node-2 有 2.5 GB 可用内存。 这时，假设我要用 Docker Swarm 来运行这个 rsyslogd 程序。为了能够让这三个容器都运行在同一台机器上，我就必须在另外两个容器上设置一个 affinity=main（与 main 容器有亲密性）的约束，即：它们俩必须和 main 容器运行在同一台机器上。 然后，我顺序执行：“docker run main”“docker run imklog”和“docker run imuxsock”，创建这三个容器。 这样，这三个容器都会进入 Swarm 的待调度队列。然后，main 容器和 imklog 容器都先后出队并被调度到了 node-2 上（这个情况是完全有可能的）。 可是，当 imuxsock 容器出队开始被调度时，Swarm 就有点懵了：node-2 上的可用资源只有 0.5 GB 了，并不足以运行 imuxsock 容器；可是，根据 affinity=main 的约束，imuxsock 容器又只能运行在 node-2 上。 这就是一个典型的成组调度（gang scheduling）没有被妥善处理的例子。 在工业界和学术界，关于这个问题的讨论可谓旷日持久，也产生了很多可供选择的解决方案。 比如，Mesos 中就有一个资源囤积（resource hoarding）的机制，会在所有设置了 Affinity 约束的任务都达到时，才开始对它们统一进行调度。而在 Google Omega 论文中，则提出了使用乐观调度处理冲突的方法，即：先不管这些冲突，而是通过精心设计的回滚机制在出现了冲突之后解决问题。 可是这些方法都谈不上完美。资源囤积带来了不可避免的调度效率损失和死锁的可能性；而乐观调度的复杂程度，则不是常规技术团队所能驾驭的。 但是，到了 Kubernetes 项目里，这样的问题就迎刃而解了：Pod 是 Kubernetes 里的原子调度单位。这就意味着，Kubernetes 项目的调度器，是统一按照 Pod 而非容器的资源需求进行计算的。 所以，像 imklog、imuxsock 和 main 函数主进程这样的三个容器，正是一个典型的由三个容器组成的 Pod。Kubernetes 项目在调度时，自然就会去选择可用内存等于 3 GB 的 node-1 节点进行绑定，而根本不会考虑 node-2。 像这样容器间的紧密协作，我们可以称为“超亲密关系”。这些具有“超亲密关系”容器的典型特征包括但不限于：互相之间会发生直接的文件交换、使用 localhost 或者 Socket 文件进行本地通信、会发生非常频繁的远程调用、需要共享某些 Linux Namespace（比如，一个容器要加入另一个容器的 Network Namespace）等等。 这也就意味着，并不是所有有“关系”的容器都属于同一个 Pod。比如，PHP 应用容器和 MySQL 虽然会发生访问关系，但并没有必要、也不应该部署在同一台机器上，它们更适合做成两个 Pod。 不过，相信此时你可能会有第二个疑问： 对于初学者来说，一般都是先学会了用 Docker 这种单容器的工具，才会开始接触 Pod。 而如果 Pod 的设计只是出于调度上的考虑，那么 Kubernetes 项目似乎完全没有必要非得把 Pod 作为“一等公民”吧？这不是故意增加用户的学习门槛吗？ 没错，如果只是处理“超亲密关系”这样的调度问题，有 Borg 和 Omega 论文珠玉在前，Kubernetes 项目肯定可以在调度器层面给它解决掉。 不过，Pod 在 Kubernetes 项目里还有更重要的意义，那就是：容器设计模式。 为了理解这一层含义，我就必须先给你介绍一下Pod 的实现原理。 首先，关于 Pod 最重要的一个事实是：它只是一个逻辑概念。 也就是说，Kubernetes 真正处理的，还是宿主机操作系统上 Linux 容器的 Namespace 和 Cgroups，而并不存在一个所谓的 Pod 的边界或者隔离环境。 那么，Pod 又是怎么被“创建”出来的呢？ 答案是：Pod，其实是一组共享了某些资源的容器。 具体的说：Pod 里的所有容器，共享的是同一个 Network Namespace，并且可以声明共享同一个 Volume。 那这么来看的话，一个有 A、B 两个容器的 Pod，不就是等同于一个容器（容器 A）共享另外一个容器（容器 B）的网络和 Volume 的玩儿法么？ 这好像通过 docker run —net —volumes-from 这样的命令就能实现嘛，比如： 1$ docker run --net=B --volumes-from=B --name=A image-A ... 但是，你有没有考虑过，如果真这样做的话，容器 B 就必须比容器 A 先启动，这样一个 Pod 里的多个容器就不是对等关系，而是拓扑关系了。 所以，在 Kubernetes 项目里，Pod 的实现需要使用一个中间容器，这个容器叫作 Infra 容器。在这个 Pod 中，Infra 容器永远都是第一个被创建的容器，而其他用户定义的容器，则通过 Join Network Namespace 的方式，与 Infra 容器关联在一起。这样的组织关系，可以用下面这样一个示意图来表达： 如上图所示，这个 Pod 里有两个用户容器 A 和 B，还有一个 Infra 容器。很容易理解，在 Kubernetes 项目里，Infra 容器一定要占用极少的资源，所以它使用的是一个非常特殊的镜像，叫作：k8s.gcr.io/pause。这个镜像是一个用汇编语言编写的、永远处于“暂停”状态的容器，解压后的大小也只有 100~200 KB 左右。 而在 Infra 容器“Hold 住”Network Namespace 后，用户容器就可以加入到 Infra 容器的 Network Namespace 当中了。所以，如果你查看这些容器在宿主机上的 Namespace 文件（这个 Namespace 文件的路径，我已经在前面的内容中介绍过），它们指向的值一定是完全一样的。 这也就意味着，对于 Pod 里的容器 A 和容器 B 来说： 它们可以直接使用 localhost 进行通信； 它们看到的网络设备跟 Infra 容器看到的完全一样； 一个 Pod 只有一个 IP 地址，也就是这个 Pod 的 Network Namespace 对应的 IP 地址； 当然，其他的所有网络资源，都是一个 Pod 一份，并且被该 Pod 中的所有容器共享； Pod 的生命周期只跟 Infra 容器一致，而与容器 A 和 B 无关。 而对于同一个 Pod 里面的所有用户容器来说，它们的进出流量，也可以认为都是通过 Infra 容器完成的。这一点很重要，因为将来如果你要为 Kubernetes 开发一个网络插件时，应该重点考虑的是如何配置这个 Pod 的 Network Namespace，而不是每一个用户容器如何使用你的网络配置，这是没有意义的。 这就意味着，如果你的网络插件需要在容器里安装某些包或者配置才能完成的话，是不可取的：Infra 容器镜像的 rootfs 里几乎什么都没有，没有你随意发挥的空间。当然，这同时也意味着你的网络插件完全不必关心用户容器的启动与否，而只需要关注如何配置 Pod，也就是 Infra 容器的 Network Namespace 即可。 有了这个设计之后，共享 Volume 就简单多了：Kubernetes 项目只要把所有 Volume 的定义都设计在 Pod 层级即可。 这样，一个 Volume 对应的宿主机目录对于 Pod 来说就只有一个，Pod 里的容器只要声明挂载这个 Volume，就一定可以共享这个 Volume 对应的宿主机目录。比如下面这个例子： 1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata: name: two-containersspec: restartPolicy: Never volumes: - name: shared-data hostPath: path: /data containers: - name: nginx-container image: nginx volumeMounts: - name: shared-data mountPath: /usr/share/nginx/html - name: debian-container image: debian volumeMounts: - name: shared-data mountPath: /pod-data command: [&quot;/bin/sh&quot;] args: [&quot;-c&quot;, &quot;echo Hello from the debian container &gt; /pod-data/index.html&quot;] 在这个例子中，debian-container 和 nginx-container 都声明挂载了 shared-data 这个 Volume。而 shared-data 是 hostPath 类型。所以，它对应在宿主机上的目录就是：/data。而这个目录，其实就被同时绑定挂载进了上述两个容器当中。 这就是为什么，nginx-container 可以从它的 /usr/share/nginx/html 目录中，读取到 debian-container 生成的 index.html 文件的原因。 明白了 Pod 的实现原理后，我们再来讨论“容器设计模式”，就容易多了。 Pod 这种“超亲密关系”容器的设计思想，实际上就是希望，当用户想在一个容器里跑多个功能并不相关的应用时，应该优先考虑它们是不是更应该被描述成一个 Pod 里的多个容器。 为了能够掌握这种思考方式，你就应该尽量尝试使用它来描述一些用单个容器难以解决的问题。 第一个最典型的例子是：WAR 包与 Web 服务器。 我们现在有一个 Java Web 应用的 WAR 包，它需要被放在 Tomcat 的 webapps 目录下运行起来。 假如，你现在只能用 Docker 来做这件事情，那该如何处理这个组合关系呢？ 一种方法是，把 WAR 包直接放在 Tomcat 镜像的 webapps 目录下，做成一个新的镜像运行起来。可是，这时候，如果你要更新 WAR 包的内容，或者要升级 Tomcat 镜像，就要重新制作一个新的发布镜像，非常麻烦。 另一种方法是，你压根儿不管 WAR 包，永远只发布一个 Tomcat 容器。不过，这个容器的 webapps 目录，就必须声明一个 hostPath 类型的 Volume，从而把宿主机上的 WAR 包挂载进 Tomcat 容器当中运行起来。不过，这样你就必须要解决一个问题，即：如何让每一台宿主机，都预先准备好这个存储有 WAR 包的目录呢？这样来看，你只能独立维护一套分布式存储系统了。 实际上，有了 Pod 之后，这样的问题就很容易解决了。我们可以把 WAR 包和 Tomcat 分别做成镜像，然后把它们作为一个 Pod 里的两个容器“组合”在一起。这个 Pod 的配置文件如下所示： 12345678910111213141516171819202122232425apiVersion: v1kind: Podmetadata: name: javaweb-2spec: initContainers: - image: geektime/sample:v2 name: war command: [&quot;cp&quot;, &quot;/sample.war&quot;, &quot;/app&quot;] volumeMounts: - mountPath: /app name: app-volume containers: - image: geektime/tomcat:7.0 name: tomcat command: [&quot;sh&quot;,&quot;-c&quot;,&quot;/root/apache-tomcat-7.0.42-v2/bin/start.sh&quot;] volumeMounts: - mountPath: /root/apache-tomcat-7.0.42-v2/webapps name: app-volume ports: - containerPort: 8080 hostPort: 8001 volumes: - name: app-volume emptyDir: &#123;&#125; 在这个 Pod 中，我们定义了两个容器，第一个容器使用的镜像是 geektime/sample:v2，这个镜像里只有一个 WAR 包（sample.war）放在根目录下。而第二个容器则使用的是一个标准的 Tomcat 镜像。 不过，你可能已经注意到，WAR 包容器的类型不再是一个普通容器，而是一个 Init Container 类型的容器。 在 Pod 中，所有 Init Container 定义的容器，都会比 spec.containers 定义的用户容器先启动。并且，Init Container 容器会按顺序逐一启动，而直到它们都启动并且退出了，用户容器才会启动。 所以，这个 Init Container 类型的 WAR 包容器启动后，我执行了一句”cp /sample.war /app”，把应用的 WAR 包拷贝到 /app 目录下，然后退出。 而后这个 /app 目录，就挂载了一个名叫 app-volume 的 Volume。 接下来就很关键了。Tomcat 容器，同样声明了挂载 app-volume 到自己的 webapps 目录下。 所以，等 Tomcat 容器启动时，它的 webapps 目录下就一定会存在 sample.war 文件：这个文件正是 WAR 包容器启动时拷贝到这个 Volume 里面的，而这个 Volume 是被这两个容器共享的。 像这样，我们就用一种“组合”方式，解决了 WAR 包与 Tomcat 容器之间耦合关系的问题。 实际上，这个所谓的“组合”操作，正是容器设计模式里最常用的一种模式，它的名字叫：sidecar。 顾名思义，sidecar 指的就是我们可以在一个 Pod 中，启动一个辅助容器，来完成一些独立于主进程（主容器）之外的工作。 比如，在我们的这个应用 Pod 中，Tomcat 容器是我们要使用的主容器，而 WAR 包容器的存在，只是为了给它提供一个 WAR 包而已。所以，我们用 Init Container 的方式优先运行 WAR 包容器，扮演了一个 sidecar 的角色。 第二个例子，则是容器的日志收集。 比如，我现在有一个应用，需要不断地把日志文件输出到容器的 /var/log 目录中。 这时，我就可以把一个 Pod 里的 Volume 挂载到应用容器的 /var/log 目录上。 然后，我在这个 Pod 里同时运行一个 sidecar 容器，它也声明挂载同一个 Volume 到自己的 /var/log 目录上。 这样，接下来 sidecar 容器就只需要做一件事儿，那就是不断地从自己的 /var/log 目录里读取日志文件，转发到 MongoDB 或者 Elasticsearch 中存储起来。这样，一个最基本的日志收集工作就完成了。 跟第一个例子一样，这个例子中的 sidecar 的主要工作也是使用共享的 Volume 来完成对文件的操作。 但不要忘记，Pod 的另一个重要特性是，它的所有容器都共享同一个 Network Namespace。这就使得很多与 Pod 网络相关的配置和管理，也都可以交给 sidecar 完成，而完全无须干涉用户容器。这里最典型的例子莫过于 Istio 这个微服务治理项目了。 “上云”工作的完成，最终还是要靠深入理解容器的本质，即：进程。 实际上，一个运行在虚拟机里的应用，哪怕再简单，也是被管理在 systemd 或者 supervisord 之下的一组进程，而不是一个进程。这跟本地物理机上应用的运行方式其实是一样的。这也是为什么，从物理机到虚拟机之间的应用迁移，往往并不困难。 可是对于容器来说，一个容器永远只能管理一个进程。更确切地说，一个容器，就是一个进程。这是容器技术的“天性”，不可能被修改。所以，将一个原本运行在虚拟机里的应用，“无缝迁移”到容器中的想法，实际上跟容器的本质是相悖的。 这也是当初 Swarm 项目无法成长起来的重要原因之一：一旦到了真正的生产环境上，Swarm 这种单容器的工作方式，就难以描述真实世界里复杂的应用架构了。 所以，你现在可以这么理解 Pod 的本质： Pod，实际上是在扮演传统基础设施里“虚拟机”的角色；而容器，则是这个虚拟机里运行的用户程序。 所以下一次，当你需要把一个运行在虚拟机里的应用迁移到 Docker 容器中时，一定要仔细分析到底有哪些进程（组件）运行在这个虚拟机里。 然后，你就可以把整个虚拟机想象成为一个 Pod，把这些进程分别做成容器镜像，把有顺序关系的容器，定义为 Init Container。这才是更加合理的、松耦合的容器编排诀窍，也是从传统应用架构，到“微服务架构”最自然的过渡方式。 注意：Pod 这个概念，提供的是一种编排思想，而不是具体的技术方案。所以，如果愿意的话，你完全可以使用虚拟机来作为 Pod 的实现，然后把用户容器都运行在这个虚拟机里。比如，Mirantis 公司的virtlet 项目就在干这个事情。甚至，你可以去实现一个带有 Init 进程的容器项目，来模拟传统应用的运行方式。这些工作，在 Kubernetes 中都是非常轻松的，也是我们后面讲解 CRI 时会提到的内容。 相反的，如果强行把整个应用塞到一个容器里，甚至不惜使用 Docker In Docker 这种在生产环境中后患无穷的解决方案，恐怕最后往往会得不偿失。 Pod的基本概念现在，你已经非常清楚：Pod，而不是容器，才是 Kubernetes 项目中的最小编排单位。将这个设计落实到 API 对象上，容器（Container）就成了 Pod 属性里的一个普通的字段。那么，一个很自然的问题就是：到底哪些属性属于 Pod 对象，而又有哪些属性属于 Container 呢？ 要彻底理解这个问题，你就一定要牢记我在上一篇文章中提到的一个结论：Pod 扮演的是传统部署环境里“虚拟机”的角色。这样的设计，是为了使用户从传统环境（虚拟机环境）向 Kubernetes（容器环境）的迁移，更加平滑。 而如果你能把 Pod 看成传统环境里的“机器”、把容器看作是运行在这个“机器”里的“用户程序”，那么很多关于 Pod 对象的设计就非常容易理解了。 比如，凡是调度、网络、存储，以及安全相关的属性，基本上是 Pod 级别的。 这些属性的共同特征是，它们描述的是“机器”这个整体，而不是里面运行的“程序”。比如，配置这个“机器”的网卡（即：Pod 的网络定义），配置这个“机器”的磁盘（即：Pod 的存储定义），配置这个“机器”的防火墙（即：Pod 的安全定义）。更不用说，这台“机器”运行在哪个服务器之上（即：Pod 的调度）。 接下来，我就先为你介绍 Pod 中几个重要字段的含义和用法。 NodeSelector：是一个供用户将 Pod 与 Node 进行绑定的字段，用法如下所示： 123456apiVersion: v1kind: Pod...spec: nodeSelector: disktype: ssd 这样的一个配置，意味着这个 Pod 永远只能运行在携带了“disktype: ssd”标签（Label）的节点上；否则，它将调度失败。 NodeName：一旦 Pod 的这个字段被赋值，Kubernetes 项目就会被认为这个 Pod 已经经过了调度，调度的结果就是赋值的节点名字。所以，这个字段一般由调度器负责设置，但用户也可以设置它来“骗过”调度器，当然这个做法一般是在测试或者调试的时候才会用到。 HostAliases：定义了 Pod 的 hosts 文件（比如 /etc/hosts）里的内容，用法如下： 12345678910apiVersion: v1kind: Pod...spec: hostAliases: - ip: &quot;10.1.2.3&quot; hostnames: - &quot;foo.remote&quot; - &quot;bar.remote&quot;... 在这个 Pod 的 YAML 文件中，我设置了一组 IP 和 hostname 的数据。这样，这个 Pod 启动后，/etc/hosts 文件的内容将如下所示： 1234567cat /etc/hosts# Kubernetes-managed hosts file.127.0.0.1 localhost...10.244.135.10 hostaliases-pod10.1.2.3 foo.remote10.1.2.3 bar.remote 其中，最下面两行记录，就是我通过 HostAliases 字段为 Pod 设置的。需要指出的是，在 Kubernetes 项目中，如果要设置 hosts 文件里的内容，一定要通过这种方法。否则，如果直接修改了 hosts 文件的话，在 Pod 被删除重建之后，kubelet 会自动覆盖掉被修改的内容。 除了上述跟“机器”相关的配置外，你可能也会发现，凡是跟容器的 Linux Namespace 相关的属性，也一定是 Pod 级别的。这个原因也很容易理解：Pod 的设计，就是要让它里面的容器尽可能多地共享 Linux Namespace，仅保留必要的隔离和限制能力。这样，Pod 模拟出的效果，就跟虚拟机里程序间的关系非常类似了。 举个例子，在下面这个 Pod 的 YAML 文件中，我定义了 shareProcessNamespace=true： 12345678910111213apiVersion: v1kind: Podmetadata: name: nginxspec: shareProcessNamespace: true containers: - name: nginx image: nginx - name: shell image: busybox stdin: true tty: true 这就意味着这个 Pod 里的容器要共享 PID Namespace。 而在这个 YAML 文件中，我还定义了两个容器：一个是 nginx 容器，一个是开启了 tty 和 stdin 的 shell 容器。 我在前面介绍容器基础时，曾经讲解过什么是 tty 和 stdin。而在 Pod 的 YAML 文件里声明开启它们俩，其实等同于设置了 docker run 里的 -it（-i 即 stdin，-t 即 tty）参数。 如果你还是不太理解它们俩的作用的话，可以直接认为 tty 就是 Linux 给用户提供的一个常驻小程序，用于接收用户的标准输入，返回操作系统的标准输出。当然，为了能够在 tty 中输入信息，你还需要同时开启 stdin（标准输入流）。 于是，这个 Pod 被创建后，你就可以使用 shell 容器的 tty 跟这个容器进行交互了。我们一起实践一下： 1kubectl create -f nginx.yaml 接下来，我们使用 kubectl attach 命令，连接到 shell 容器的 tty 上： 1kubectl attach -it nginx -c shell 这样，我们就可以在 shell 容器里执行 ps 指令，查看所有正在运行的进程： 12345678$ kubectl attach -it nginx -c shell/ # ps axPID USER TIME COMMAND 1 root 0:00 /pause 8 root 0:00 nginx: master process nginx -g daemon off; 14 101 0:00 nginx: worker process 15 root 0:00 sh 21 root 0:00 ps ax 可以看到，在这个容器里，我们不仅可以看到它本身的 ps ax 指令，还可以看到 nginx 容器的进程，以及 Infra 容器的 /pause 进程。这就意味着，整个 Pod 里的每个容器的进程，对于所有容器来说都是可见的：它们共享了同一个 PID Namespace。 类似地，凡是 Pod 中的容器要共享宿主机的 Namespace，也一定是 Pod 级别的定义，比如： 123456789101112131415apiVersion: v1kind: Podmetadata: name: nginxspec: hostNetwork: true hostIPC: true hostPID: true containers: - name: nginx image: nginx - name: shell image: busybox stdin: true tty: true 在这个 Pod 中，我定义了共享宿主机的 Network、IPC 和 PID Namespace。这就意味着，这个 Pod 里的所有容器，会直接使用宿主机的网络、直接与宿主机进行 IPC 通信、看到宿主机里正在运行的所有进程。 当然，除了这些属性，Pod 里最重要的字段当属“Containers”了。而在上一篇文章中，我还介绍过“Init Containers”。其实，这两个字段都属于 Pod 对容器的定义，内容也完全相同，只是 Init Containers 的生命周期，会先于所有的 Containers，并且严格按照定义的顺序执行。 Kubernetes 项目中对 Container 的定义，和 Docker 相比并没有什么太大区别。我在前面的容器技术概念入门系列文章中，和你分享的 Image（镜像）、Command（启动命令）、workingDir（容器的工作目录）、Ports（容器要开发的端口），以及 volumeMounts（容器要挂载的 Volume）都是构成 Kubernetes 项目中 Container 的主要字段。不过在这里，还有这么几个属性值得你额外关注。 首先，是 ImagePullPolicy 字段。它定义了镜像拉取的策略。而它之所以是一个 Container 级别的属性，是因为容器镜像本来就是 Container 定义中的一部分。 ImagePullPolicy 的值默认是 Always，即每次创建 Pod 都重新拉取一次镜像。另外，当容器的镜像是类似于 nginx 或者 nginx:latest 这样的名字时，ImagePullPolicy 也会被认为 Always。 而如果它的值被定义为 Never 或者 IfNotPresent，则意味着 Pod 永远不会主动拉取这个镜像，或者只在宿主机上不存在这个镜像时才拉取。 其次，是 Lifecycle 字段。它定义的是 Container Lifecycle Hooks。顾名思义，Container Lifecycle Hooks 的作用，是在容器状态发生变化时触发一系列“钩子”。我们来看这样一个例子： 123456789101112131415apiVersion: v1kind: Podmetadata: name: lifecycle-demospec: containers: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo Hello from the postStart handler &gt; /usr/share/message&quot;] preStop: exec: command: [&quot;/usr/sbin/nginx&quot;,&quot;-s&quot;,&quot;quit&quot;] 这是一个来自 Kubernetes 官方文档的 Pod 的 YAML 文件。它其实非常简单，只是定义了一个 nginx 镜像的容器。不过，在这个 YAML 文件的容器（Containers）部分，你会看到这个容器分别设置了一个 postStart 和 preStop 参数。这是什么意思呢？ 先说 postStart 吧。它指的是，在容器启动后，立刻执行一个指定的操作。需要明确的是，postStart 定义的操作，虽然是在 Docker 容器 ENTRYPOINT 执行之后，但它并不严格保证顺序。也就是说，在 postStart 启动时，ENTRYPOINT 有可能还没有结束。 当然，如果 postStart 执行超时或者错误，Kubernetes 会在该 Pod 的 Events 中报出该容器启动失败的错误信息，导致 Pod 也处于失败的状态。 而类似地，preStop 发生的时机，则是容器被杀死之前（比如，收到了 SIGKILL 信号）。而需要明确的是，preStop 操作的执行，是同步的。所以，它会阻塞当前的容器杀死流程，直到这个 Hook 定义操作完成之后，才允许容器被杀死，这跟 postStart 不一样。 所以，在这个例子中，我们在容器成功启动之后，在 /usr/share/message 里写入了一句“欢迎信息”（即 postStart 定义的操作）。而在这个容器被删除之前，我们则先调用了 nginx 的退出指令（即 preStop 定义的操作），从而实现了容器的“优雅退出”。 在熟悉了 Pod 以及它的 Container 部分的主要字段之后，我再和你分享一下这样一个的 Pod 对象在 Kubernetes 中的生命周期。 Pod 生命周期的变化，主要体现在 Pod API 对象的Status 部分，这是它除了 Metadata 和 Spec 之外的第三个重要字段。其中，pod.status.phase，就是 Pod 的当前状态，它有如下几种可能的情况： Pending。这个状态意味着，Pod 的 YAML 文件已经提交给了 Kubernetes，API 对象已经被创建并保存在 Etcd 当中。但是，这个 Pod 里有些容器因为某种原因而不能被顺利创建。比如，调度不成功。 Running。这个状态下，Pod 已经调度成功，跟一个具体的节点绑定。它包含的容器都已经创建成功，并且至少有一个正在运行中。 Succeeded。这个状态意味着，Pod 里的所有容器都正常运行完毕，并且已经退出了。这种情况在运行一次性任务时最为常见。 Failed。这个状态下，Pod 里至少有一个容器以不正常的状态（非 0 的返回码）退出。这个状态的出现，意味着你得想办法 Debug 这个容器的应用，比如查看 Pod 的 Events 和日志。 Unknown。这是一个异常状态，意味着 Pod 的状态不能持续地被 kubelet 汇报给 kube-apiserver，这很有可能是主从节点（Master 和 Kubelet）间的通信出现了问题。 更进一步地，Pod 对象的 Status 字段，还可以再细分出一组 Conditions。这些细分状态的值包括：PodScheduled、Ready、Initialized，以及 Unschedulable。它们主要用于描述造成当前 Status 的具体原因是什么。 比如，Pod 当前的 Status 是 Pending，对应的 Condition 是 Unschedulable，这就意味着它的调度出现了问题。 而其中，Ready 这个细分状态非常值得我们关注：它意味着 Pod 不仅已经正常启动（Running 状态），而且已经可以对外提供服务了。这两者之间（Running 和 Ready）是有区别的，你不妨仔细思考一下。 Pod 的这些状态信息，是我们判断应用运行情况的重要标准，尤其是 Pod 进入了非“Running”状态后，你一定要能迅速做出反应，根据它所代表的异常情况开始跟踪和定位，而不是去手忙脚乱地查阅文档。 先从一种特殊的 Volume 开始，来帮助你更加深入地理解 Pod 对象各个重要字段的含义。这种特殊的 Volume，叫作 Projected Volume，你可以把它翻译为“投射数据卷”。 在 Kubernetes 中，有几种特殊的 Volume，它们存在的意义不是为了存放容器里的数据，也不是用来进行容器和宿主机之间的数据交换。这些特殊 Volume 的作用，是为容器提供预先定义好的数据。所以，从容器的角度来看，这些 Volume 里的信息就是仿佛是被 Kubernetes“投射”（Project）进入容器当中的。这正是 Projected Volume 的含义。 到目前为止，Kubernetes 支持的 Projected Volume 一共有四种： Secret； ConfigMap； Downward API； ServiceAccountToken。 我首先和你分享的是 Secret。它的作用，是帮你把 Pod 想要访问的加密数据，存放到 Etcd 中。然后，你就可以通过在 Pod 的容器里挂载 Volume 的方式，访问到这些 Secret 里保存的信息了。 Secret 最典型的使用场景，莫过于存放数据库的 Credential 信息，比如下面这个例子： 1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata: name: test-projected-volume spec: containers: - name: test-secret-volume image: busybox args: - sleep - &quot;86400&quot; volumeMounts: - name: mysql-cred mountPath: &quot;/projected-volume&quot; readOnly: true volumes: - name: mysql-cred projected: sources: - secret: name: user - secret: name: pass 在这个 Pod 中，我定义了一个简单的容器。它声明挂载的 Volume，并不是常见的 emptyDir 或者 hostPath 类型，而是 projected 类型。而这个 Volume 的数据来源（sources），则是名为 user 和 pass 的 Secret 对象，分别对应的是数据库的用户名和密码。 这里用到的数据库的用户名、密码，正是以 Secret 对象的方式交给 Kubernetes 保存的。完成这个操作的指令，如下所示： 1234567$ cat ./username.txtadmin$ cat ./password.txtc1oudc0w! $ kubectl create secret generic user --from-file=./username.txt$ kubectl create secret generic pass --from-file=./password.txt 其中，username.txt 和 password.txt 文件里，存放的就是用户名和密码；而 user 和 pass，则是我为 Secret 对象指定的名字。而我想要查看这些 Secret 对象的话，只要执行一条 kubectl get 命令就可以了： 1234$ kubectl get secretsNAME TYPE DATA AGEuser Opaque 1 51spass Opaque 1 51s 当然，除了使用 kubectl create secret 指令外，我也可以直接通过编写 YAML 文件的方式来创建这个 Secret 对象，比如： 12345678apiVersion: v1kind: Secretmetadata: name: mysecrettype: Opaquedata: user: YWRtaW4= pass: MWYyZDFlMmU2N2Rm 可以看到，通过编写 YAML 文件创建出来的 Secret 对象只有一个。但它的 data 字段，却以 Key-Value 的格式保存了两份 Secret 数据。其中，“user”就是第一份数据的 Key，“pass”是第二份数据的 Key。 需要注意的是，Secret 对象要求这些数据必须是经过 Base64 转码的，以免出现明文密码的安全隐患。这个转码操作也很简单，比如： 1234$ echo -n &#x27;admin&#x27; | base64YWRtaW4=$ echo -n &#x27;1f2d1e2e67df&#x27; | base64MWYyZDFlMmU2N2Rm 这里需要注意的是，像这样创建的 Secret 对象，它里面的内容仅仅是经过了转码，而并没有被加密。在真正的生产环境中，你需要在 Kubernetes 中开启 Secret 的加密插件，增强数据的安全性。关于开启 Secret 加密插件的内容，我会在后续专门讲解 Secret 的时候，再做进一步说明。 接下来，我们尝试一下创建这个 Pod： 1$ kubectl create -f test-projected-volume.yaml 当 Pod 变成 Running 状态之后，我们再验证一下这些 Secret 对象是不是已经在容器里了： 12345678$ kubectl exec -it test-projected-volume -- /bin/sh$ ls /projected-volume/userpass$ cat /projected-volume/userroot$ cat /projected-volume/pass1f2d1e2e67df 从返回结果中，我们可以看到，保存在 Etcd 里的用户名和密码信息，已经以文件的形式出现在了容器的 Volume 目录里。而这个文件的名字，就是 kubectl create secret 指定的 Key，或者说是 Secret 对象的 data 字段指定的 Key。 更重要的是，像这样通过挂载方式进入到容器里的 Secret，一旦其对应的 Etcd 里的数据被更新，这些 Volume 里的文件内容，同样也会被更新。其实，这是 kubelet 组件在定时维护这些 Volume。 需要注意的是，这个更新可能会有一定的延时。所以在编写应用程序时，在发起数据库连接的代码处写好重试和超时的逻辑，绝对是个好习惯。 与 Secret 类似的是 ConfigMap，它与 Secret 的区别在于，ConfigMap 保存的是不需要加密的、应用所需的配置信息。而 ConfigMap 的用法几乎与 Secret 完全相同：你可以使用 kubectl create configmap 从文件或者目录创建 ConfigMap，也可以直接编写 ConfigMap 对象的 YAML 文件。 比如，一个 Java 应用所需的配置文件（.properties 文件），就可以通过下面这样的方式保存在 ConfigMap 里： 1234567891011121314151617181920212223# .properties 文件的内容$ cat example/ui.propertiescolor.good=purplecolor.bad=yellowallow.textmode=truehow.nice.to.look=fairlyNice # 从.properties 文件创建 ConfigMap$ kubectl create configmap ui-config --from-file=example/ui.properties # 查看这个 ConfigMap 里保存的信息 (data)$ kubectl get configmaps ui-config -o yamlapiVersion: v1data: ui.properties: | color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNicekind: ConfigMapmetadata: name: ui-config ... 备注：kubectl get -o yaml 这样的参数，会将指定的 Pod API 对象以 YAML 的方式展示出来。 接下来是 Downward API，它的作用是：让 Pod 里的容器能够直接获取到这个 Pod API 对象本身的信息。 举个例子： 1234567891011121314151617181920212223242526272829303132apiVersion: v1kind: Podmetadata: name: test-downwardapi-volume labels: zone: us-est-coast cluster: test-cluster1 rack: rack-22spec: containers: - name: client-container image: k8s.gcr.io/busybox command: [&quot;sh&quot;, &quot;-c&quot;] args: - while true; do if [[ -e /etc/podinfo/labels ]]; then echo -en &#x27;\\n\\n&#x27;; cat /etc/podinfo/labels; fi; sleep 5; done; volumeMounts: - name: podinfo mountPath: /etc/podinfo readOnly: false volumes: - name: podinfo projected: sources: - downwardAPI: items: - path: &quot;labels&quot; fieldRef: fieldPath: metadata.labels 在这个 Pod 的 YAML 文件中，我定义了一个简单的容器，声明了一个 projected 类型的 Volume。只不过这次 Volume 的数据来源，变成了 Downward API。而这个 Downward API Volume，则声明了要暴露 Pod 的 metadata.labels 信息给容器。 通过这样的声明方式，当前 Pod 的 Labels 字段的值，就会被 Kubernetes 自动挂载成为容器里的 /etc/podinfo/labels 文件。 而这个容器的启动命令，则是不断打印出 /etc/podinfo/labels 里的内容。所以，当我创建了这个 Pod 之后，就可以通过 kubectl logs 指令，查看到这些 Labels 字段被打印出来，如下所示： 12345$ kubectl create -f dapi-volume.yaml$ kubectl logs test-downwardapi-volumecluster=&quot;test-cluster1&quot;rack=&quot;rack-22&quot;zone=&quot;us-est-coast&quot; 目前，Downward API 支持的字段已经非常丰富了，比如： 1234567891011121314151617181. 使用 fieldRef 可以声明使用:spec.nodeName - 宿主机名字status.hostIP - 宿主机 IPmetadata.name - Pod 的名字metadata.namespace - Pod 的 Namespacestatus.podIP - Pod 的 IPspec.serviceAccountName - Pod 的 Service Account 的名字metadata.uid - Pod 的 UIDmetadata.labels[&#x27;&lt;KEY&gt;&#x27;] - 指定 &lt;KEY&gt; 的 Label 值metadata.annotations[&#x27;&lt;KEY&gt;&#x27;] - 指定 &lt;KEY&gt; 的 Annotation 值metadata.labels - Pod 的所有 Labelmetadata.annotations - Pod 的所有 Annotation 2. 使用 resourceFieldRef 可以声明使用:容器的 CPU limit容器的 CPU request容器的 memory limit容器的 memory request 上面这个列表的内容，随着 Kubernetes 项目的发展肯定还会不断增加。所以这里列出来的信息仅供参考，你在使用 Downward API 时，还是要记得去查阅一下官方文档。 不过，需要注意的是，Downward API 能够获取到的信息，一定是 Pod 里的容器进程启动之前就能够确定下来的信息。而如果你想要获取 Pod 容器运行后才会出现的信息，比如，容器进程的 PID，那就肯定不能使用 Downward API 了，而应该考虑在 Pod 里定义一个 sidecar 容器。 其实，Secret、ConfigMap，以及 Downward API 这三种 Projected Volume 定义的信息，大多还可以通过环境变量的方式出现在容器里。但是，通过环境变量获取这些信息的方式，不具备自动更新的能力。所以，一般情况下，我都建议你使用 Volume 文件的方式获取这些信息。 在明白了 Secret 之后，我再为你讲解 Pod 中一个与它密切相关的概念：Service Account。 相信你一定有过这样的想法：我现在有了一个 Pod，我能不能在这个 Pod 里安装一个 Kubernetes 的 Client，这样就可以从容器里直接访问并且操作这个 Kubernetes 的 API 了呢？ 这当然是可以的。 不过，你首先要解决 API Server 的授权问题。 Service Account 对象的作用，就是 Kubernetes 系统内置的一种“服务账户”，它是 Kubernetes 进行权限分配的对象。比如，Service Account A，可以只被允许对 Kubernetes API 进行 GET 操作，而 Service Account B，则可以有 Kubernetes API 的所有操作的权限。 像这样的 Service Account 的授权信息和文件，实际上保存在它所绑定的一个特殊的 Secret 对象里的。这个特殊的 Secret 对象，就叫作ServiceAccountToken。任何运行在 Kubernetes 集群上的应用，都必须使用这个 ServiceAccountToken 里保存的授权信息，也就是 Token，才可以合法地访问 API Server。 所以说，Kubernetes 项目的 Projected Volume 其实只有三种，因为第四种 ServiceAccountToken，只是一种特殊的 Secret 而已。 另外，为了方便使用，Kubernetes 已经为你提供了一个的默认“服务账户”（default Service Account）。并且，任何一个运行在 Kubernetes 里的 Pod，都可以直接使用这个默认的 Service Account，而无需显示地声明挂载它。 这是如何做到的呢？ 当然还是靠 Projected Volume 机制。 如果你查看一下任意一个运行在 Kubernetes 集群里的 Pod，就会发现，每一个 Pod，都已经自动声明一个类型是 Secret、名为 default-token-xxxx 的 Volume，然后 自动挂载在每个容器的一个固定目录上。比如： 12345678910$ kubectl describe pod nginx-deployment-5c678cfb6d-lg9lwContainers:... Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-s8rbq (ro)Volumes: default-token-s8rbq: Type: Secret (a volume populated by a Secret) SecretName: default-token-s8rbq Optional: false 这个 Secret 类型的 Volume，正是默认 Service Account 对应的 ServiceAccountToken。所以说，Kubernetes 其实在每个 Pod 创建的时候，自动在它的 spec.volumes 部分添加上了默认 ServiceAccountToken 的定义，然后自动给每个容器加上了对应的 volumeMounts 字段。这个过程对于用户来说是完全透明的。 这样，一旦 Pod 创建完成，容器里的应用就可以直接从这个默认 ServiceAccountToken 的挂载目录里访问到授权信息和文件。这个容器内的路径在 Kubernetes 里是固定的，即：/var/run/secrets/kubernetes.io/serviceaccount ，而这个 Secret 类型的 Volume 里面的内容如下所示： 12$ ls /var/run/secrets/kubernetes.io/serviceaccount ca.crt namespace token 所以，你的应用程序只要直接加载这些授权文件，就可以访问并操作 Kubernetes API 了。而且，如果你使用的是 Kubernetes 官方的 Client 包（k8s.io/client-go）的话，它还可以自动加载这个目录下的文件，你不需要做任何配置或者编码操作。 这种把 Kubernetes 客户端以容器的方式运行在集群里，然后使用 default Service Account 自动授权的方式，被称作“InClusterConfig”，也是我最推荐的进行 Kubernetes API 编程的授权方式。 当然，考虑到自动挂载默认 ServiceAccountToken 的潜在风险，Kubernetes 允许你设置默认不为 Pod 里的容器自动挂载这个 Volume。 除了这个默认的 Service Account 外，我们很多时候还需要创建一些我们自己定义的 Service Account，来对应不同的权限设置。这样，我们的 Pod 里的容器就可以通过挂载这些 Service Account 对应的 ServiceAccountToken，来使用这些自定义的授权信息。在后面讲解为 Kubernetes 开发插件的时候，我们将会实践到这个操作。 接下来，我们再来看 Pod 的另一个重要的配置：容器健康检查和恢复机制。 在 Kubernetes 中，你可以为 Pod 里的容器定义一个健康检查“探针”（Probe）。这样，kubelet 就会根据这个 Probe 的返回值决定这个容器的状态，而不是直接以容器进行是否运行（来自 Docker 返回的信息）作为依据。这种机制，是生产环境中保证应用健康存活的重要手段。 我们一起来看一个 Kubernetes 文档中的例子。 123456789101112131415161718192021apiVersion: v1kind: Podmetadata: labels: test: liveness name: test-liveness-execspec: containers: - name: liveness image: busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 在这个 Pod 中，我们定义了一个有趣的容器。它在启动之后做的第一件事，就是在 /tmp 目录下创建了一个 healthy 文件，以此作为自己已经正常运行的标志。而 30 s 过后，它会把这个文件删除掉。 与此同时，我们定义了一个这样的 livenessProbe（健康检查）。它的类型是 exec，这意味着，它会在容器启动后，在容器里面执行一句我们指定的命令，比如：“cat /tmp/healthy”。这时，如果这个文件存在，这条命令的返回值就是 0，Pod 就会认为这个容器不仅已经启动，而且是健康的。这个健康检查，在容器启动 5 s 后开始执行（initialDelaySeconds: 5），每 5 s 执行一次（periodSeconds: 5）。 现在，让我们来具体实践一下这个过程。 首先，创建这个 Pod： 1$ kubectl create -f test-liveness-exec.yaml 然后，查看这个 Pod 的状态： 123$ kubectl get podNAME READY STATUS RESTARTS AGEtest-liveness-exec 1/1 Running 0 10s 可以看到，由于已经通过了健康检查，这个 Pod 就进入了 Running 状态。 而 30 s 之后，我们再查看一下 Pod 的 Events： 1$ kubectl describe pod test-liveness-exec 你会发现，这个 Pod 在 Events 报告了一个异常： 123FirstSeen LastSeen Count From SubobjectPath Type Reason Message--------- -------- ----- ---- ------------- -------- ------ -------2s 2s 1 &#123;kubelet worker0&#125; spec.containers&#123;liveness&#125; Warning Unhealthy Liveness probe failed: cat: can&#x27;t open &#x27;/tmp/healthy&#x27;: No such file or directory 显然，这个健康检查探查到 /tmp/healthy 已经不存在了，所以它报告容器是不健康的。那么接下来会发生什么呢？ 我们不妨再次查看一下这个 Pod 的状态： 123$ kubectl get pod test-liveness-execNAME READY STATUS RESTARTS AGEliveness-exec 1/1 Running 1 1m 这时我们发现，Pod 并没有进入 Failed 状态，而是保持了 Running 状态。这是为什么呢？ 其实，如果你注意到 RESTARTS 字段从 0 到 1 的变化，就明白原因了：这个异常的容器已经被 Kubernetes 重启了。在这个过程中，Pod 保持 Running 状态不变。 需要注意的是：Kubernetes 中并没有 Docker 的 Stop 语义。所以虽然是 Restart（重启），但实际却是重新创建了容器。 这个功能就是 Kubernetes 里的Pod 恢复机制，也叫 restartPolicy。它是 Pod 的 Spec 部分的一个标准字段（pod.spec.restartPolicy），默认值是 Always，即：任何时候这个容器发生了异常，它一定会被重新创建。 但一定要强调的是，Pod 的恢复过程，永远都是发生在当前节点上，而不会跑到别的节点上去。事实上，一旦一个 Pod 与一个节点（Node）绑定，除非这个绑定发生了变化（pod.spec.node 字段被修改），否则它永远都不会离开这个节点。这也就意味着，如果这个宿主机宕机了，这个 Pod 也不会主动迁移到其他节点上去。 而如果你想让 Pod 出现在其他的可用节点上，就必须使用 Deployment 这样的“控制器”来管理 Pod，哪怕你只需要一个 Pod 副本。 而作为用户，你还可以通过设置 restartPolicy，改变 Pod 的恢复策略。除了 Always，它还有 OnFailure 和 Never 两种情况： Always：在任何情况下，只要容器不在运行状态，就自动重启容器； OnFailure: 只在容器 异常时才自动重启容器； Never: 从来不重启容器。 在实际使用时，我们需要根据应用运行的特性，合理设置这三种恢复策略。 比如，一个 Pod，它只计算 1+1=2，计算完成输出结果后退出，变成 Succeeded 状态。这时，你如果再用 restartPolicy=Always 强制重启这个 Pod 的容器，就没有任何意义了。 而如果你要关心这个容器退出后的上下文环境，比如容器退出后的日志、文件和目录，就需要将 restartPolicy 设置为 Never。因为一旦容器被自动重新创建，这些内容就有可能丢失掉了（被垃圾回收了）。 值得一提的是，Kubernetes 的官方文档，把 restartPolicy 和 Pod 里容器的状态，以及 Pod 状态的对应关系，总结了非常复杂的一大堆情况。实际上，你根本不需要死记硬背这些对应关系，只要记住如下两个基本的设计原理即可： 只要 Pod 的 restartPolicy 指定的策略允许重启异常的容器（比如：Always），那么这个 Pod 就会保持 Running 状态，并进行容器重启。否则，Pod 就会进入 Failed 状态 。 对于包含多个容器的 Pod，只有它里面所有的容器都进入异常状态后，Pod 才会进入 Failed 状态。在此之前，Pod 都是 Running 状态。此时，Pod 的 READY 字段会显示正常容器的个数，比如： 123$ kubectl get pod test-liveness-execNAME READY STATUS RESTARTS AGEliveness-exec 0/1 Running 1 1m 所以，假如一个 Pod 里只有一个容器，然后这个容器异常退出了。那么，只有当 restartPolicy=Never 时，这个 Pod 才会进入 Failed 状态。而其他情况下，由于 Kubernetes 都可以重启这个容器，所以 Pod 的状态保持 Running 不变。 而如果这个 Pod 有多个容器，仅有一个容器异常退出，它就始终保持 Running 状态，哪怕即使 restartPolicy=Never。只有当所有容器也异常退出之后，这个 Pod 才会进入 Failed 状态。 其他情况，都可以以此类推出来。 现在，我们一起回到前面提到的 livenessProbe 上来。 除了在容器中执行命令外，livenessProbe 也可以定义为发起 HTTP 或者 TCP 请求的方式，定义格式如下： 12345678910...livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3 123456... livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 所以，你的 Pod 其实可以暴露一个健康检查 URL（比如 /healthz），或者直接让健康检查去检测应用的监听端口。这两种配置方法，在 Web 服务类的应用中非常常用。 在 Kubernetes 的 Pod 中，还有一个叫 readinessProbe 的字段。虽然它的用法与 livenessProbe 类似，但作用却大不一样。readinessProbe 检查结果的成功与否，决定的这个 Pod 是不是能被通过 Service 的方式访问到，而并不影响 Pod 的生命周期。这部分内容，我会留在讲解 Service 时再重点介绍。 在讲解了这么多字段之后，想必你对 Pod 对象的语义和描述能力，已经有了一个初步的感觉。 这时，你有没有产生这样一个想法：Pod 的字段这么多，我又不可能全记住，Kubernetes 能不能自动给 Pod 填充某些字段呢？ 这个需求实际上非常实用。比如，开发人员只需要提交一个基本的、非常简单的 Pod YAML，Kubernetes 就可以自动给对应的 Pod 对象加上其他必要的信息，比如 labels，annotations，volumes 等等。而这些信息，可以是运维人员事先定义好的。 这么一来，开发人员编写 Pod YAML 的门槛，就被大大降低了。 所以，这个叫作 PodPreset（Pod 预设置）的功能 已经出现在了 v1.11 版本的 Kubernetes 中。 举个例子，现在开发人员编写了如下一个 pod.yaml 文件： 12345678910111213apiVersion: v1kind: Podmetadata: name: website labels: app: website role: frontendspec: containers: - name: website image: nginx ports: - containerPort: 80 作为 Kubernetes 的初学者，你肯定眼前一亮：这不就是我最擅长编写的、最简单的 Pod 嘛。没错，这个 YAML 文件里的字段，想必你现在闭着眼睛也能写出来。 可是，如果运维人员看到了这个 Pod，他一定会连连摇头：这种 Pod 在生产环境里根本不能用啊！ 所以，这个时候，运维人员就可以定义一个 PodPreset 对象。在这个对象中，凡是他想在开发人员编写的 Pod 里追加的字段，都可以预先定义好。比如这个 preset.yaml： 1234567891011121314151617apiVersion: settings.k8s.io/v1alpha1kind: PodPresetmetadata: name: allow-databasespec: selector: matchLabels: role: frontend env: - name: DB_PORT value: &quot;6379&quot; volumeMounts: - mountPath: /cache name: cache-volume volumes: - name: cache-volume emptyDir: &#123;&#125; 在这个 PodPreset 的定义中，首先是一个 selector。这就意味着后面这些追加的定义，只会作用于 selector 所定义的、带有“role: frontend”标签的 Pod 对象，这就可以防止“误伤”。 然后，我们定义了一组 Pod 的 Spec 里的标准字段，以及对应的值。比如，env 里定义了 DB_PORT 这个环境变量，volumeMounts 定义了容器 Volume 的挂载目录，volumes 定义了一个 emptyDir 的 Volume。 接下来，我们假定运维人员先创建了这个 PodPreset，然后开发人员才创建 Pod： 12$ kubectl create -f preset.yaml$ kubectl create -f pod.yaml 这时，Pod 运行起来之后，我们查看一下这个 Pod 的 API 对象： 12345678910111213141516171819202122232425$ kubectl get pod website -o yamlapiVersion: v1kind: Podmetadata: name: website labels: app: website role: frontend annotations: podpreset.admission.kubernetes.io/podpreset-allow-database: &quot;resource version&quot;spec: containers: - name: website image: nginx volumeMounts: - mountPath: /cache name: cache-volume ports: - containerPort: 80 env: - name: DB_PORT value: &quot;6379&quot; volumes: - name: cache-volume emptyDir: &#123;&#125; 这个时候，我们就可以清楚地看到，这个 Pod 里多了新添加的 labels、env、volumes 和 volumeMount 的定义，它们的配置跟 PodPreset 的内容一样。此外，这个 Pod 还被自动加上了一个 annotation 表示这个 Pod 对象被 PodPreset 改动过。 需要说明的是，PodPreset 里定义的内容，只会在 Pod API 对象被创建之前追加在这个对象本身上，而不会影响任何 Pod 的控制器的定义。 比如，我们现在提交的是一个 nginx-deployment，那么这个 Deployment 对象本身是永远不会被 PodPreset 改变的，被修改的只是这个 Deployment 创建出来的所有 Pod。这一点请务必区分清楚。 这里有一个问题：如果你定义了同时作用于一个 Pod 对象的多个 PodPreset，会发生什么呢？ 实际上，Kubernetes 项目会帮你合并（Merge）这两个 PodPreset 要做的修改。而如果它们要做的修改有冲突的话，这些冲突字段就不会被修改。 而在学习这些字段的同时，你还应该认真体会一下 Kubernetes“一切皆对象”的设计思想：比如应用是 Pod 对象，应用的配置是 ConfigMap 对象，应用要访问的密码则是 Secret 对象。 所以，也就自然而然地有了 PodPreset 这样专门用来对 Pod 进行批量化、自动化修改的工具对象。 容器编排控制器Pod 对象，其实就是容器的升级版。它对容器进行了组合，添加了更多的属性和字段。这就好比给集装箱四面安装了吊环，使得 Kubernetes 这架“吊车”，可以更轻松地操作它。 而 Kubernetes 操作这些“集装箱”的逻辑，都由控制器（Controller）完成。先来看Deployment 这个最基本的控制器对象。 12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 这个Deployment 定义的编排动作非常简单，即：确保携带了 app=nginx 标签的 Pod 的个数，永远等于 spec.replicas 指定的个数，即 2 个。 这就意味着，如果在这个集群中，携带 app=nginx 标签的 Pod 的个数大于 2 的时候，就会有旧的 Pod 被删除；反之，就会有新的 Pod 被创建。 这时，你也许就会好奇：究竟是 Kubernetes 项目中的哪个组件，在执行这些操作呢？ 我在前面介绍 Kubernetes 架构的时候，曾经提到过一个叫作 kube-controller-manager 的组件。 实际上，这个组件，就是一系列控制器的集合。我们可以查看一下 Kubernetes 项目的 pkg/controller 目录： 1234567$ cd kubernetes/pkg/controller/$ ls -d */ deployment/ job/ podautoscaler/ cloud/ disruption/ namespace/ replicaset/ serviceaccount/ volume/cronjob/ garbagecollector/ nodelifecycle/ replication/ statefulset/ daemon/... 这个目录下面的每一个控制器，都以独有的方式负责某种编排功能。而我们的 Deployment，正是这些控制器中的一种。 实际上，这些控制器之所以被统一放在 pkg/controller 目录下，就是因为它们都遵循 Kubernetes 项目中的一个通用编排模式，即：控制循环（control loop）。 比如，现在有一种待编排的对象 X，它有一个对应的控制器。那么，我就可以用一段 Go 语言风格的伪代码，为你描述这个控制循环： 123456789for &#123; 实际状态 := 获取集群中对象 X 的实际状态（Actual State） 期望状态 := 获取集群中对象 X 的期望状态（Desired State） if 实际状态 == 期望状态&#123; 什么都不做 &#125; else &#123; 执行编排动作，将实际状态调整为期望状态 &#125;&#125; 在具体实现中，实际状态往往来自于 Kubernetes 集群本身。 比如，kubelet 通过心跳汇报的容器状态和节点状态，或者监控系统中保存的应用监控数据，或者控制器主动收集的它自己感兴趣的信息，这些都是常见的实际状态的来源。 而期望状态，一般来自于用户提交的 YAML 文件。 比如，Deployment 对象中 Replicas 字段的值。很明显，这些信息往往都保存在 Etcd 中。 接下来，以 Deployment 为例，我和你简单描述一下它对控制器模型的实现： Deployment 控制器从 Etcd 中获取到所有携带了“app: nginx”标签的 Pod，然后统计它们的数量，这就是实际状态； Deployment 对象的 Replicas 字段的值就是期望状态； Deployment 控制器将两个状态做比较，然后根据比较结果，确定是创建 Pod，还是删除已有的 Pod 可以看到，一个 Kubernetes 对象的主要编排逻辑，实际上是在第三步的“对比”阶段完成的。 这个操作，通常被叫作调谐（Reconcile）。这个调谐的过程，则被称作“Reconcile Loop”（调谐循环）或者“Sync Loop”（同步循环）。 所以，如果你以后在文档或者社区中碰到这些词，都不要担心，它们其实指的都是同一个东西：控制循环。 而调谐的最终结果，往往都是对被控制对象的某种写操作。 比如，增加 Pod，删除已有的 Pod，或者更新 Pod 的某个字段。这也是 Kubernetes 项目“面向 API 对象编程”的一个直观体现。 其实，像 Deployment 这种控制器的设计原理，就是我们前面提到过的，“用一种对象管理另一种对象”的“艺术”。 其中，这个控制器对象本身，负责定义被管理对象的期望状态。比如，Deployment 里的 replicas=2 这个字段。 而被控制对象的定义，则来自于一个“模板”。比如，Deployment 里的 template 字段。 可以看到，Deployment 这个 template 字段里的内容，跟一个标准的 Pod 对象的 API 定义，丝毫不差。而所有被这个 Deployment 管理的 Pod 实例，其实都是根据这个 template 字段的内容创建出来的。 像 Deployment 定义的 template 字段，在 Kubernetes 项目中有一个专有的名字，叫作 PodTemplate（Pod 模板）。 这个概念非常重要，因为后面我要讲解到的大多数控制器，都会使用 PodTemplate 来统一定义它所要管理的 Pod。更有意思的是，我们还会看到其他类型的对象模板，比如 Volume 的模板。 至此，我们就可以对 Deployment 以及其他类似的控制器，做一个简单总结了： 如上图所示，类似 Deployment 这样的一个控制器，实际上都是由上半部分的控制器定义（包括期望状态），加上下半部分的被控制对象的模板组成的。 这就是为什么，在所有 API 对象的 Metadata 里，都有一个字段叫作 ownerReference，用于保存当前这个 API 对象的拥有者（Owner）的信息。 这就是为什么，在所有 API 对象的 Metadata 里，都有一个字段叫作 ownerReference，用于保存当前这个 API 对象的拥有者（Owner）的信息。 那么，对于我们这个 nginx-deployment 来说，它创建出来的 Pod 的 ownerReference 就是 nginx-deployment 吗？或者说，nginx-deployment 所直接控制的，就是 Pod 对象么？","categories":[{"name":"云原生","slug":"云原生","permalink":"http://wht6.github.io/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://wht6.github.io/tags/Kubernetes/"}]},{"title":"Kubernetes集群搭建与实践","slug":"Kubernetes集群搭建与实践","date":"2022-02-27T08:00:00.000Z","updated":"2022-04-05T08:08:18.534Z","comments":true,"path":"posts/3588.html","link":"","permalink":"http://wht6.github.io/posts/3588.html","excerpt":"","text":"kubeadm要真正发挥容器技术的实力，你就不能仅仅局限于对 Linux 容器本身的钻研和使用。 这些知识更适合作为你的技术储备，以便在需要的时候可以帮你更快的定位问题，并解决问题。 而更深入的学习容器技术的关键在于，如何使用这些技术来“容器化”你的应用。 比如，我们的应用既可能是 Java Web 和 MySQL 这样的组合，也可能是 Cassandra 这样的分布式系统。而要使用容器把后者运行起来，你单单通过 Docker 把一个 Cassandra 镜像跑起来是没用的。 要把 Cassandra 应用容器化的关键，在于如何处理好这些 Cassandra 容器之间的编排关系。比如，哪些 Cassandra 容器是主，哪些是从？主从容器如何区分？它们之间又如何进行自动发现和通信？Cassandra 容器的持久化数据又如何保持，等等。 这也是为什么我们要反复强调 Kubernetes 项目的主要原因：这个项目体现出来的容器化“表达能力”，具有独有的先进性和完备性。这就使得它不仅能运行 Java Web 与 MySQL 这样的常规组合，还能够处理 Cassandra 容器集群等复杂编排问题。 作为一个典型的分布式项目，Kubernetes 的部署一直以来都是挡在初学者前面的一只“拦路虎”。尤其是在 Kubernetes 项目发布初期，它的部署完全要依靠一堆由社区维护的脚本。 其实，Kubernetes 作为一个 Golang 项目，已经免去了很多类似于 Python 项目要安装语言级别依赖的麻烦。但是，除了将各个组件编译成二进制文件外，用户还要负责为这些二进制文件编写对应的配置文件、配置自启动脚本，以及为 kube-apiserver 配置授权文件等等诸多运维工作。 目前，各大云厂商最常用的部署的方法，是使用 SaltStack、Ansible 等运维工具自动化地执行这些步骤。 但即使这样，这个部署过程依然非常繁琐。因为，SaltStack 这类专业运维工具本身的学习成本，就可能比 Kubernetes 项目还要高。 难道 Kubernetes 项目就没有简单的部署方法了吗？ 这个问题，在 Kubernetes 社区里一直没有得到足够重视。直到 2017 年，在志愿者的推动下，社区才终于发起了一个独立的部署工具，名叫：kubeadm。 这个项目的目的，就是要让用户能够通过这样两条指令完成一个 Kubernetes 集群的部署： 12345# 创建一个 Master 节点$ kubeadm init # 将一个 Node 节点加入到当前集群中$ kubeadm join &lt;Master 节点的 IP 和端口 &gt; 是不是非常方便呢？ 不过，你可能也会有所顾虑：Kubernetes 的功能那么多，这样一键部署出来的集群，能用于生产环境吗？ 为了回答这个问题，我就先和你介绍一下 kubeadm 的工作原理吧。 kubeadm 的工作原理在部署时，Kubernetes 的每一个组件都是一个需要被执行的、单独的二进制文件。所以不难想象，SaltStack 这样的运维工具或者由社区维护的脚本的功能，就是要把这些二进制文件传输到指定的机器当中，然后编写控制脚本来启停这些组件。 不过，在理解了容器技术之后，你可能已经萌生出了这样一个想法，为什么不用容器部署 Kubernetes 呢？ 这样，我只要给每个 Kubernetes 组件做一个容器镜像，然后在每台宿主机上用 docker run 指令启动这些组件容器，部署不就完成了吗？ 事实上，在 Kubernetes 早期的部署脚本里，确实有一个脚本就是用 Docker 部署 Kubernetes 项目的，这个脚本相比于 SaltStack 等的部署方式，也的确简单了不少。 但是，这样做会带来一个很麻烦的问题，即：如何容器化 kubelet。 kubelet 是 Kubernetes 项目用来操作 Docker 等容器运行时的核心组件。可是，除了跟容器运行时打交道外，kubelet 在配置容器网络、管理容器数据卷时，都需要直接操作宿主机。 而如果现在 kubelet 本身就运行在一个容器里，那么直接操作宿主机就会变得很麻烦。对于网络配置来说还好，kubelet 容器可以通过不开启 Network Namespace（即 Docker 的 host network 模式）的方式，直接共享宿主机的网络栈。可是，要让 kubelet 隔着容器的 Mount Namespace 和文件系统，操作宿主机的文件系统，就有点儿困难了。 比如，如果用户想要使用 NFS 做容器的持久化数据卷，那么 kubelet 就需要在容器进行绑定挂载前，在宿主机的指定目录上，先挂载 NFS 的远程目录。 可是，这时候问题来了。由于现在 kubelet 是运行在容器里的，这就意味着它要做的这个“mount -F nfs”命令，被隔离在了一个单独的 Mount Namespace 中。即，kubelet 做的挂载操作，不能被“传播”到宿主机上。 对于这个问题，有人说，可以使用 setns() 系统调用，在宿主机的 Mount Namespace 中执行这些挂载操作；也有人说，应该让 Docker 支持一个–mnt=host 的参数。 但是，到目前为止，在容器里运行 kubelet，依然没有很好的解决办法，我也不推荐你用容器去部署 Kubernetes 项目。 正因为如此，kubeadm 选择了一种妥协方案： 把 kubelet 直接运行在宿主机上，然后使用容器部署其他的 Kubernetes 组件。 所以，你使用 kubeadm 的第一步，是在机器上手动安装 kubeadm、kubelet 和 kubectl 这三个二进制文件。当然，kubeadm 的作者已经为各个发行版的 Linux 准备好了安装包，所以你只需要执行： 1$ apt-get install kubeadm 就可以了。 接下来，你就可以使用“kubeadm init”部署 Master 节点了。 kubeadm init 的工作流程当你执行 kubeadm init 指令后，kubeadm 首先要做的，是一系列的检查工作，以确定这台机器可以用来部署 Kubernetes。这一步检查，我们称为“Preflight Checks”，它可以为你省掉很多后续的麻烦。 其实，Preflight Checks 包括了很多方面，比如： Linux 内核的版本必须是否是 3.10 以上？ Linux Cgroups 模块是否可用？ 机器的 hostname 是否标准？在 Kubernetes 项目里，机器的名字以及一切存储在 Etcd 中的 API 对象，都必须使用标准的 DNS 命名（RFC 1123）。 用户安装的 kubeadm 和 kubelet 的版本是否匹配？ 机器上是不是已经安装了 Kubernetes 的二进制文件？ Kubernetes 的工作端口 10250/10251/10252 端口是不是已经被占用？ ip、mount 等 Linux 指令是否存在？ Docker 是否已经安装？ …… 在通过了 Preflight Checks 之后，kubeadm 要为你做的，是生成 Kubernetes 对外提供服务所需的各种证书和对应的目录。 Kubernetes 对外提供服务时，除非专门开启“不安全模式”，否则都要通过 HTTPS 才能访问 kube-apiserver。这就需要为 Kubernetes 集群配置好证书文件。 kubeadm 为 Kubernetes 项目生成的证书文件都放在 Master 节点的 /etc/kubernetes/pki 目录下。在这个目录下，最主要的证书文件是 ca.crt 和对应的私钥 ca.key。 此外，用户使用 kubectl 获取容器日志等 streaming 操作时，需要通过 kube-apiserver 向 kubelet 发起请求，这个连接也必须是安全的。kubeadm 为这一步生成的是 apiserver-kubelet-client.crt 文件，对应的私钥是 apiserver-kubelet-client.key。 除此之外，Kubernetes 集群中还有 Aggregate APIServer 等特性，也需要用到专门的证书，这里我就不再一一列举了。需要指出的是，你可以选择不让 kubeadm 为你生成这些证书，而是拷贝现有的证书到如下证书的目录里： 1/etc/kubernetes/pki/ca.&#123;crt,key&#125; 这时，kubeadm 就会跳过证书生成的步骤，把它完全交给用户处理。 证书生成后，kubeadm 接下来会为其他组件生成访问 kube-apiserver 所需的配置文件。这些文件的路径是：/etc/kubernetes/xxx.conf： 12ls /etc/kubernetes/admin.conf controller-manager.conf kubelet.conf scheduler.conf 这些文件里面记录的是，当前这个 Master 节点的服务器地址、监听端口、证书目录等信息。这样，对应的客户端（比如 scheduler，kubelet 等），可以直接加载相应的文件，使用里面的信息与 kube-apiserver 建立安全连接。 接下来，kubeadm 会为 Master 组件生成 Pod 配置文件。我已经和你介绍过 Kubernetes 有三个 Master 组件 kube-apiserver、kube-controller-manager、kube-scheduler，而它们都会被使用 Pod 的方式部署起来。 你可能会有些疑问：这时，Kubernetes 集群尚不存在，难道 kubeadm 会直接执行 docker run 来启动这些容器吗？ 当然不是。 在 Kubernetes 中，有一种特殊的容器启动方法叫做“Static Pod”。它允许你把要部署的 Pod 的 YAML 文件放在一个指定的目录里。这样，当这台机器上的 kubelet 启动时，它会自动检查这个目录，加载所有的 Pod YAML 文件，然后在这台机器上启动它们。 从这一点也可以看出，kubelet 在 Kubernetes 项目中的地位非常高，在设计上它就是一个完全独立的组件，而其他 Master 组件，则更像是辅助性的系统容器。 在 kubeadm 中，Master 组件的 YAML 文件会被生成在 /etc/kubernetes/manifests 路径下。比如，kube-apiserver.yaml： 123456789101112131415161718192021222324252627282930313233343536373839404142apiVersion: v1kind: Podmetadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot; creationTimestamp: null labels: component: kube-apiserver tier: control-plane name: kube-apiserver namespace: kube-systemspec: containers: - command: - kube-apiserver - --authorization-mode=Node,RBAC - --runtime-config=api/all=true - --advertise-address=10.168.0.2 ... - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key image: k8s.gcr.io/kube-apiserver-amd64:v1.11.1 imagePullPolicy: IfNotPresent livenessProbe: ... name: kube-apiserver resources: requests: cpu: 250m volumeMounts: - mountPath: /usr/share/ca-certificates name: usr-share-ca-certificates readOnly: true ... hostNetwork: true priorityClassName: system-cluster-critical volumes: - hostPath: path: /etc/ca-certificates type: DirectoryOrCreate name: etc-ca-certificates ... 在这里，你只需要关注这样几个信息： 这个 Pod 里只定义了一个容器，它使用的镜像是：k8s.gcr.io/kube-apiserver-amd64:v1.11.1 。这个镜像是 Kubernetes 官方维护的一个组件镜像。 这个容器的启动命令（commands）是 kube-apiserver —authorization-mode=Node,RBAC …，这样一句非常长的命令。其实，它就是容器里 kube-apiserver 这个二进制文件再加上指定的配置参数而已。 如果你要修改一个已有集群的 kube-apiserver 的配置，需要修改这个 YAML 文件。 这些组件的参数也可以在部署时指定，我很快就会讲解到。 在这一步完成后，kubeadm 还会再生成一个 Etcd 的 Pod YAML 文件，用来通过同样的 Static Pod 的方式启动 Etcd。所以，最后 Master 组件的 Pod YAML 文件如下所示： 12$ ls /etc/kubernetes/manifests/etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml 而一旦这些 YAML 文件出现在被 kubelet 监视的 /etc/kubernetes/manifests 目录下，kubelet 就会自动创建这些 YAML 文件中定义的 Pod，即 Master 组件的容器。 Master 容器启动后，kubeadm 会通过检查 localhost:6443/healthz 这个 Master 组件的健康检查 URL，等待 Master 组件完全运行起来。 然后，kubeadm 就会为集群生成一个 bootstrap token。在后面，只要持有这个 token，任何一个安装了 kubelet 和 kubadm 的节点，都可以通过 kubeadm join 加入到这个集群当中。 这个 token 的值和使用方法会，会在 kubeadm init 结束后被打印出来。 在 token 生成之后，kubeadm 会将 ca.crt 等 Master 节点的重要信息，通过 ConfigMap 的方式保存在 Etcd 当中，供后续部署 Node 节点使用。这个 ConfigMap 的名字是 cluster-info。 kubeadm init 的最后一步，就是安装默认插件。Kubernetes 默认 kube-proxy 和 DNS 这两个插件是必须安装的。它们分别用来提供整个集群的服务发现和 DNS 功能。其实，这两个插件也只是两个容器镜像而已，所以 kubeadm 只要用 Kubernetes 客户端创建两个 Pod 就可以了。 kubeadm join 的工作流程这个流程其实非常简单，kubeadm init 生成 bootstrap token 之后，你就可以在任意一台安装了 kubelet 和 kubeadm 的机器上执行 kubeadm join 了。 可是，为什么执行 kubeadm join 需要这样一个 token 呢？ 因为，任何一台机器想要成为 Kubernetes 集群中的一个节点，就必须在集群的 kube-apiserver 上注册。可是，要想跟 apiserver 打交道，这台机器就必须要获取到相应的证书文件（CA 文件）。可是，为了能够一键安装，我们就不能让用户去 Master 节点上手动拷贝这些文件。 所以，kubeadm 至少需要发起一次“不安全模式”的访问到 kube-apiserver，从而拿到保存在 ConfigMap 中的 cluster-info（它保存了 APIServer 的授权信息）。而 bootstrap token，扮演的就是这个过程中的安全验证的角色。 只要有了 cluster-info 里的 kube-apiserver 的地址、端口、证书，kubelet 就可以以“安全模式”连接到 apiserver 上，这样一个新的节点就部署完成了。 接下来，你只要在其他节点上重复这个指令就可以了。 配置 kubeadm 的部署参数我在前面讲解了 kubeadm 部署 Kubernetes 集群最关键的两个步骤，kubeadm init 和 kubeadm join。相信你一定会有这样的疑问：kubeadm 确实简单易用，可是我又该如何定制我的集群组件参数呢？ 比如，我要指定 kube-apiserver 的启动参数，该怎么办？ 在这里，我强烈推荐你在使用 kubeadm init 部署 Master 节点时，使用下面这条指令： 1$ kubeadm init --config kubeadm.yaml 这时，你就可以给 kubeadm 提供一个 YAML 文件（比如，kubeadm.yaml），它的内容如下所示（我仅列举了主要部分）： 123456789101112131415161718192021222324252627apiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.0api: advertiseAddress: 192.168.0.102 bindPort: 6443 ...etcd: local: dataDir: /var/lib/etcd image: &quot;&quot;imageRepository: k8s.gcr.iokubeProxy: config: bindAddress: 0.0.0.0 ...kubeletConfiguration: baseConfig: address: 0.0.0.0 ...networking: dnsDomain: cluster.local podSubnet: &quot;&quot; serviceSubnet: 10.96.0.0/12nodeRegistration: criSocket: /var/run/dockershim.sock ... 通过制定这样一个部署参数配置文件，你就可以很方便地在这个文件里填写各种自定义的部署参数了。比如，我现在要指定 kube-apiserver 的参数，那么我只要在这个文件里加上这样一段信息： 123456...apiServerExtraArgs: advertise-address: 192.168.0.103 anonymous-auth: false enable-admission-plugins: AlwaysPullImages,DefaultStorageClass audit-log-path: /home/johndoe/audit.log 然后，kubeadm 就会使用上面这些信息替换 /etc/kubernetes/manifests/kube-apiserver.yaml 里的 command 字段里的参数了。 而这个 YAML 文件提供的可配置项远不止这些。比如，你还可以修改 kubelet 和 kube-proxy 的配置，修改 Kubernetes 使用的基础镜像的 URL（默认的k8s.gcr.io/xxx镜像 URL 在国内访问是有困难的），指定自己的证书文件，指定特殊的容器运行时等等。 搭建完整Kubernetes集群这里所说的“完整”，指的是这个集群具备 Kubernetes 项目在 GitHub 上已经发布的所有功能，并能够模拟生产环境的所有使用需求。但并不代表这个集群是生产级别可用的：类似于高可用、授权、多租户、灾难备份等生产级别集群的功能暂时不在本篇文章的讨论范围。 这次部署，我不会依赖于任何公有云或私有云的能力，而是完全在 Bare-metal 环境中完成。这样的部署经验会更有普适性。 准备工作首先，准备机器。最直接的办法，自然是到公有云上申请几个虚拟机。当然，如果条件允许的话，拿几台本地的物理服务器来组集群是最好不过了。这些机器只要满足如下几个条件即可： 满足安装 Docker 项目所需的要求，比如 64 位的 Linux 操作系统、3.10 及以上的内核版本； x86 或者 ARM 架构均可； 机器之间网络互通，这是将来容器之间网络互通的前提； 有外网访问权限，因为需要拉取镜像； 能够访问到gcr.io、quay.io这两个 docker registry，因为有小部分镜像需要在这里拉取； 单机可用资源建议 2 核 CPU、8 GB 内存或以上，再小的话问题也不大，但是能调度的 Pod 数量就比较有限了； 30 GB 或以上的可用磁盘空间，这主要是留给 Docker 镜像和日志文件用的。 在本次部署中，我准备的机器配置如下： 2 核 CPU、 7.5 GB 内存； 30 GB 磁盘； Ubuntu 16.04； 内网互通； 外网访问权限不受限制。 然后，我再和你介绍一下今天实践的目标： 在所有节点上安装 Docker 和 kubeadm； 部署 Kubernetes Master； 部署容器网络插件； 部署 Kubernetes Worker； 部署 Dashboard 可视化插件； 部署容器存储插件。 安装 kubeadm 和 Docker前面已经介绍过 kubeadm 的基础用法，它的一键安装非常方便，我们只需要添加 kubeadm 的源，然后直接使用 apt-get 安装即可，具体流程如下所示： 123456$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -$ cat &lt;&lt;EOF &gt; /etc/apt/sources.list.d/kubernetes.listdeb http://apt.kubernetes.io/ kubernetes-xenial mainEOF$ apt-get update$ apt-get install -y docker.io kubeadm 在上述安装 kubeadm 的过程中，kubeadm 和 kubelet、kubectl、kubernetes-cni 这几个二进制文件都会被自动安装好。 另外，这里我直接使用 Ubuntu 的 docker.io 的安装源，原因是 Docker 公司每次发布的最新的 Docker CE（社区版）产品往往还没有经过 Kubernetes 项目的验证，可能会有兼容性方面的问题。 部署 Kubernetes 的 Master 节点前面已经介绍过 kubeadm 可以一键部署 Master 节点，这里既然要部署一个“完整”的 Kubernetes 集群，那我们不妨稍微提高一下难度：通过配置文件来开启一些实验性功能。 所以，这里我编写了一个给 kubeadm 用的 YAML 文件（名叫：kubeadm.yaml）： 123456789apiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationcontrollerManagerExtraArgs: horizontal-pod-autoscaler-use-rest-clients: &quot;true&quot; horizontal-pod-autoscaler-sync-period: &quot;10s&quot; node-monitor-grace-period: &quot;10s&quot;apiServerExtraArgs: runtime-config: &quot;api/all=true&quot;kubernetesVersion: &quot;stable-1.11&quot; 这个配置中，我给 kube-controller-manager 设置了： 1horizontal-pod-autoscaler-use-rest-clients: &quot;true&quot; 这意味着，将来部署的 kube-controller-manager 能够使用自定义资源（Custom Metrics）进行自动水平扩展。 其中，“stable-1.11”就是 kubeadm 帮我们部署的 Kubernetes 版本号，即：Kubernetes release 1.11 最新的稳定版，在我的环境下，它是 v1.11.1。你也可以直接指定这个版本，比如：kubernetesVersion: “v1.11.1”。 然后，我们只需要执行一句指令： 1$ kubeadm init --config kubeadm.yaml 就可以完成 Kubernetes Master 的部署了，这个过程只需要几分钟。部署完成后，kubeadm 会生成一行指令： 1kubeadm join 10.168.0.2:6443 --token 00bwbx.uvnaa2ewjflwu1ry --discovery-token-ca-cert-hash sha256:00eb62a2a6020f94132e3fe1ab721349bbcd3e9b94da9654cfe15f2985ebd711 这个 kubeadm join 命令，就是用来给这个 Master 节点添加更多工作节点（Worker）的命令。我们在后面部署 Worker 节点的时候马上会用到它，所以找一个地方把这条命令记录下来。 此外，kubeadm 还会提示我们第一次使用 Kubernetes 集群所需要的配置命令： 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 而需要这些配置命令的原因是：Kubernetes 集群默认需要加密方式访问。所以，这几条命令，就是将刚刚部署生成的 Kubernetes 集群的安全配置文件，保存到当前用户的.kube 目录下，kubectl 默认会使用这个目录下的授权信息访问 Kubernetes 集群。 如果不这么做的话，我们每次都需要通过 export KUBECONFIG 环境变量告诉 kubectl 这个安全配置文件的位置。 现在，我们就可以使用 kubectl get 命令来查看当前唯一一个节点的状态了： 1234$ kubectl get nodes NAME STATUS ROLES AGE VERSIONmaster NotReady master 1d v1.11.1 可以看到，这个 get 指令输出的结果里，Master 节点的状态是 NotReady，这是为什么呢？ 在调试 Kubernetes 集群时，最重要的手段就是用 kubectl describe 来查看这个节点（Node）对象的详细信息、状态和事件（Event），我们来试一下： 1234567$ kubectl describe node master ...Conditions:... Ready False ... KubeletNotReady runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized 通过 kubectl describe 指令的输出，我们可以看到 NodeNotReady 的原因在于，我们尚未部署任何网络插件。 另外，我们还可以通过 kubectl 检查这个节点上各个系统 Pod 的状态，其中，kube-system 是 Kubernetes 项目预留的系统 Pod 的工作空间（Namepsace，注意它并不是 Linux Namespace，它只是 Kubernetes 划分不同工作空间的单位）： 12345678910$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGEcoredns-78fcdf6894-j9s52 0/1 Pending 0 1hcoredns-78fcdf6894-jm4wf 0/1 Pending 0 1hetcd-master 1/1 Running 0 2skube-apiserver-master 1/1 Running 0 1skube-controller-manager-master 0/1 Pending 0 1skube-proxy-xbd47 1/1 NodeLost 0 1hkube-scheduler-master 1/1 Running 0 1s 可以看到，CoreDNS、kube-controller-manager 等依赖于网络的 Pod 都处于 Pending 状态，即调度失败。这当然是符合预期的：因为这个 Master 节点的网络尚未就绪。 部署网络插件在 Kubernetes 项目“一切皆容器”的设计理念指导下，部署网络插件非常简单，只需要执行一句 kubectl apply 指令，以 Weave 为例： 1$ kubectl apply -f https://git.io/weave-kube-1.6 部署完成后，我们可以通过 kubectl get 重新检查 Pod 的状态： 1234567891011$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGEcoredns-78fcdf6894-j9s52 1/1 Running 0 1dcoredns-78fcdf6894-jm4wf 1/1 Running 0 1detcd-master 1/1 Running 0 9skube-apiserver-master 1/1 Running 0 9skube-controller-manager-master 1/1 Running 0 9skube-proxy-xbd47 1/1 Running 0 1dkube-scheduler-master 1/1 Running 0 9sweave-net-cmk27 2/2 Running 0 19s 可以看到，所有的系统 Pod 都成功启动了，而刚刚部署的 Weave 网络插件则在 kube-system 下面新建了一个名叫 weave-net-cmk27 的 Pod，一般来说，这些 Pod 就是容器网络插件在每个节点上的控制组件。 Kubernetes 支持容器网络插件，使用的是一个名叫 CNI 的通用接口，它也是当前容器网络的事实标准，市面上的所有容器网络开源项目都可以通过 CNI 接入 Kubernetes，比如 Flannel、Calico、Canal、Romana 等等，它们的部署方式也都是类似的“一键部署”。 至此，Kubernetes 的 Master 节点就部署完成了。如果你只需要一个单节点的 Kubernetes，现在你就可以使用了。不过，在默认情况下，Kubernetes 的 Master 节点是不能运行用户 Pod 的，所以还需要额外做一个小操作。稍后我会介绍到它。 部署 Kubernetes 的 Worker 节点Kubernetes 的 Worker 节点跟 Master 节点几乎是相同的，它们运行着的都是一个 kubelet 组件。唯一的区别在于，在 kubeadm init 的过程中，kubelet 启动后，Master 节点上还会自动运行 kube-apiserver、kube-scheduler、kube-controller-manger 这三个系统 Pod。 所以，相比之下，部署 Worker 节点反而是最简单的，只需要两步即可完成。 第一步，在所有 Worker 节点上执行“安装 kubeadm 和 Docker”一节的所有步骤。 第二步，执行部署 Master 节点时生成的 kubeadm join 指令： 1$ kubeadm join 10.168.0.2:6443 --token 00bwbx.uvnaa2ewjflwu1ry --discovery-token-ca-cert-hash sha256:00eb62a2a6020f94132e3fe1ab721349bbcd3e9b94da9654cfe15f2985ebd711 通过 Taint/Toleration 调整 Master 执行 Pod 的策略我在前面提到过，默认情况下 Master 节点是不允许运行用户 Pod 的。而 Kubernetes 做到这一点，依靠的是 Kubernetes 的 Taint/Toleration 机制。 它的原理非常简单：一旦某个节点被加上了一个 Taint，即被“打上了污点”，那么所有 Pod 就都不能在这个节点上运行，因为 Kubernetes 的 Pod 都有“洁癖”。 除非，有个别的 Pod 声明自己能“容忍”这个“污点”，即声明了 Toleration，它才可以在这个节点上运行。 其中，为节点打上“污点”（Taint）的命令是： 1$ kubectl taint nodes node1 foo=bar:NoSchedule 这时，该 node1 节点上就会增加一个键值对格式的 Taint，即：foo=bar:NoSchedule。其中值里面的 NoSchedule，意味着这个 Taint 只会在调度新 Pod 时产生作用，而不会影响已经在 node1 上运行的 Pod，哪怕它们没有 Toleration。 那么 Pod 又如何声明 Toleration 呢？ 我们只要在 Pod 的.yaml 文件中的 spec 部分，加入 tolerations 字段即可： 123456789apiVersion: v1kind: Pod...spec: tolerations: - key: &quot;foo&quot; operator: &quot;Equal&quot; value: &quot;bar&quot; effect: &quot;NoSchedule&quot; 这个 Toleration 的含义是，这个 Pod 能“容忍”所有键值对为 foo=bar 的 Taint（ operator: “Equal”，“等于”操作）。 现在回到我们已经搭建的集群上来。这时，如果你通过 kubectl describe 检查一下 Master 节点的 Taint 字段，就会有所发现了： 12345$ kubectl describe node master Name: masterRoles: masterTaints: node-role.kubernetes.io/master:NoSchedule 可以看到，Master 节点默认被加上了node-role.kubernetes.io/master:NoSchedule这样一个“污点”，其中“键”是node-role.kubernetes.io/master，而没有提供“值”。 此时，你就需要像下面这样用“Exists”操作符（operator: “Exists”，“存在”即可）来说明，该 Pod 能够容忍所有以 foo 为键的 Taint，才能让这个 Pod 运行在该 Master 节点上： 12345678apiVersion: v1kind: Pod...spec: tolerations: - key: &quot;foo&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot; 当然，如果你就是想要一个单节点的 Kubernetes，删除这个 Taint 才是正确的选择： 1$ kubectl taint nodes --all node-role.kubernetes.io/master- 如上所示，我们在“node-role.kubernetes.io/master”这个键后面加上了一个短横线“-”，这个格式就意味着移除所有以“node-role.kubernetes.io/master”为键的 Taint。 到了这一步，一个基本完整的 Kubernetes 集群就部署完毕了。是不是很简单呢？ 有了 kubeadm 这样的原生管理工具，Kubernetes 的部署已经被大大简化。更重要的是，像证书、授权、各个组件的配置等部署中最麻烦的操作，kubeadm 都已经帮你完成了。 接下来，我们再在这个 Kubernetes 集群上安装一些其他的辅助插件，比如 Dashboard 和存储插件。 部署 Dashboard 可视化插件在 Kubernetes 社区中，有一个很受欢迎的 Dashboard 项目，它可以给用户提供一个可视化的 Web 界面来查看当前集群的各种信息。毫不意外，它的部署也相当简单： 1$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml 部署完成之后，我们就可以查看 Dashboard 对应的 Pod 的状态了： 123$ kubectl get pods -n kube-system kubernetes-dashboard-6948bdb78-f67xk 1/1 Running 0 1m 需要注意的是，由于 Dashboard 是一个 Web Server，很多人经常会在自己的公有云上无意地暴露 Dashboard 的端口，从而造成安全隐患。所以，1.7 版本之后的 Dashboard 项目部署完成后，默认只能通过 Proxy 的方式在本地访问。具体的操作，你可以查看 Dashboard 项目的官方文档。 而如果你想从集群外访问这个 Dashboard 的话，就需要用到 Ingress。 部署容器存储插件接下来，让我们完成这个 Kubernetes 集群的最后一块拼图：容器持久化存储。 我在前面介绍容器原理时已经提到过，很多时候我们需要用数据卷（Volume）把外面宿主机上的目录或者文件挂载进容器的 Mount Namespace 中，从而达到容器和宿主机共享这些目录或者文件的目的。容器里的应用，也就可以在这些数据卷中新建和写入文件。 可是，如果你在某一台机器上启动的一个容器，显然无法看到其他机器上的容器在它们的数据卷里写入的文件。这是容器最典型的特征之一：无状态。 而容器的持久化存储，就是用来保存容器存储状态的重要手段：存储插件会在容器里挂载一个基于网络或者其他机制的远程数据卷，使得在容器里创建的文件，实际上是保存在远程存储服务器上，或者以分布式的方式保存在多个节点上，而与当前宿主机没有任何绑定关系。这样，无论你在其他哪个宿主机上启动新的容器，都可以请求挂载指定的持久化存储卷，从而访问到数据卷里保存的内容。这就是“持久化”的含义。 由于 Kubernetes 本身的松耦合设计，绝大多数存储项目，比如 Ceph、GlusterFS、NFS 等，都可以为 Kubernetes 提供持久化存储能力。在这次的部署实战中，我会选择部署一个很重要的 Kubernetes 存储插件项目：Rook。 Rook 项目是一个基于 Ceph 的 Kubernetes 存储插件（它后期也在加入对更多存储实现的支持）。不过，不同于对 Ceph 的简单封装，Rook 在自己的实现中加入了水平扩展、迁移、灾难备份、监控等大量的企业级功能，使得这个项目变成了一个完整的、生产级别可用的容器存储插件。 得益于容器化技术，用两条指令，Rook 就可以把复杂的 Ceph 存储后端部署起来： 123$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml $ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml 在部署完成后，你就可以看到 Rook 项目会将自己的 Pod 放置在由它自己管理的两个 Namespace 当中： 12345678910$ kubectl get pods -n rook-ceph-systemNAME READY STATUS RESTARTS AGErook-ceph-agent-7cv62 1/1 Running 0 15srook-ceph-operator-78d498c68c-7fj72 1/1 Running 0 44srook-discover-2ctcv 1/1 Running 0 15s $ kubectl get pods -n rook-cephNAME READY STATUS RESTARTS AGErook-ceph-mon0-kxnzh 1/1 Running 0 13srook-ceph-mon1-7dn2t 1/1 Running 0 2s 这样，一个基于 Rook 持久化存储集群就以容器的方式运行起来了，而接下来在 Kubernetes 项目上创建的所有 Pod 就能够通过 Persistent Volume（PV）和 Persistent Volume Claim（PVC）的方式，在容器里挂载由 Ceph 提供的数据卷了。 而 Rook 项目，则会负责这些数据卷的生命周期管理、灾难备份等运维工作。 这时候，你可能会有个疑问：为什么我要选择 Rook 项目呢？ 其实，是因为这个项目很有前途。 如果你去研究一下 Rook 项目的实现，就会发现它巧妙地依赖了 Kubernetes 提供的编排能力，合理的使用了很多诸如 Operator、CRD 等重要的扩展特性。这使得 Rook 项目，成为了目前社区中基于 Kubernetes API 构建的最完善也最成熟的容器存储插件。 备注：其实，在很多时候，大家说的所谓“云原生”，就是“Kubernetes 原生”的意思。而像 Rook、Istio 这样的项目，正是贯彻这个思路的典范。在我们后面讲解了声明式 API 之后，相信你对这些项目的设计思想会有更深刻的体会。 这个集群的部署过程并不像传说中那么繁琐，这主要得益于： kubeadm 项目大大简化了部署 Kubernetes 的准备工作，尤其是配置文件、证书、二进制文件的准备和制作，以及集群版本管理等操作，都被 kubeadm 接管了。 Kubernetes 本身“一切皆容器”的设计思想，加上良好的可扩展机制，使得插件的部署非常简便。 上述思想，也是开发和使用 Kubernetes 的重要指导思想，即：基于 Kubernetes 开展工作时，你一定要优先考虑这两个问题： 我的工作是不是可以容器化？ 我的工作是不是可以借助 Kubernetes API 和可扩展机制来完成？ 而一旦这项工作能够基于 Kubernetes 实现容器化，就很有可能像上面的部署过程一样，大幅简化原本复杂的运维工作。对于时间宝贵的技术人员来说，这个变化的重要性是不言而喻的。 发布容器化应用在开始实践之前，我先给你讲解一下 Kubernetes 里面与开发者关系最密切的几个概念。 作为一个应用开发者，你首先要做的，是制作容器的镜像。而有了容器镜像之后，你需要按照 Kubernetes 项目的规范和要求，将你的镜像组织为它能够“认识”的方式，然后提交上去。 那么，什么才是 Kubernetes 项目能“认识”的方式呢？ 这就是使用 Kubernetes 的必备技能：编写配置文件。 备注：这些配置文件可以是 YAML 或者 JSON 格式的。为方便阅读与理解，在后面的讲解中，我会统一使用 YAML 文件来指代它们。 Kubernetes 跟 Docker 等很多项目最大的不同，就在于它不推荐你使用命令行的方式直接运行容器（虽然 Kubernetes 项目也支持这种方式，比如：kubectl run），而是希望你用 YAML 文件的方式，即：把容器的定义、参数、配置，统统记录在一个 YAML 文件中，然后用这样一句指令把它运行起来： 1$ kubectl create -f 我的配置文件 这么做最直接的好处是，你会有一个文件能记录下 Kubernetes 到底“run”了什么。比如下面这个例子： 12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 像这样的一个 YAML 文件，对应到 Kubernetes 中，就是一个 API Object（API 对象）。当你为这个对象的各个字段填好值并提交给 Kubernetes 之后，Kubernetes 就会负责创建出这些对象所定义的容器或者其他类型的 API 资源。 可以看到，这个 YAML 文件中的 Kind 字段，指定了这个 API 对象的类型（Type），是一个 Deployment。 所谓 Deployment，是一个定义多副本应用（即多个副本 Pod）的对象。此外，Deployment 还负责在 Pod 定义发生变化时，对每个副本进行滚动更新（Rolling Update）。 在上面这个 YAML 文件中，我给它定义的 Pod 副本个数 (spec.replicas) 是：2。 而这些 Pod 具体的又长什么样子呢？ 为此，我定义了一个 Pod 模版（spec.template），这个模版描述了我想要创建的 Pod 的细节。在上面的例子里，这个 Pod 里只有一个容器，这个容器的镜像（spec.containers.image）是 nginx:1.7.9，这个容器监听端口（containerPort）是 80。 Pod 就是 Kubernetes 世界里的“应用”；而一个应用，可以由多个容器组成。 需要注意的是，像这样使用一种 API 对象（Deployment）管理另一种 API 对象（Pod）的方法，在 Kubernetes 中，叫作“控制器”模式（controller pattern）。在我们的例子中，Deployment 扮演的正是 Pod 的控制器的角色。 你可能还注意到，这样的每一个 API 对象都有一个叫作 Metadata 的字段，这个字段就是 API 对象的“标识”，即元数据，它也是我们从 Kubernetes 里找到这个对象的主要依据。这其中最主要使用到的字段是 Labels。 顾名思义，Labels 就是一组 key-value 格式的标签。而像 Deployment 这样的控制器对象，就可以通过这个 Labels 字段从 Kubernetes 中过滤出它所关心的被控制对象。 比如，在上面这个 YAML 文件中，Deployment 会把所有正在运行的、携带“app: nginx”标签的 Pod 识别为被管理的对象，并确保这些 Pod 的总数严格等于两个。 而这个过滤规则的定义，是在 Deployment 的“spec.selector.matchLabels”字段。我们一般称之为：Label Selector。 另外，在 Metadata 中，还有一个与 Labels 格式、层级完全相同的字段叫 Annotations，它专门用来携带 key-value 格式的内部信息。所谓内部信息，指的是对这些信息感兴趣的，是 Kubernetes 组件本身，而不是用户。所以大多数 Annotations，都是在 Kubernetes 运行过程中，被自动加在这个 API 对象上。 一个 Kubernetes 的 API 对象的定义，大多可以分为 Metadata 和 Spec 两个部分。前者存放的是这个对象的元数据，对所有 API 对象来说，这一部分的字段和格式基本上是一样的；而后者存放的，则是属于这个对象独有的定义，用来描述它所要表达的功能。 在了解了上述 Kubernetes 配置文件的基本知识之后，我们现在就可以把这个 YAML 文件“运行”起来。正如前所述，你可以使用 kubectl create 指令完成这个操作： 1$ kubectl create -f nginx-deployment.yaml 然后，通过 kubectl get 命令检查这个 YAML 运行起来的状态是不是与我们预期的一致： 1234$ kubectl get pods -l app=nginxNAME READY STATUS RESTARTS AGEnginx-deployment-67594d6bf6-9gdvr 1/1 Running 0 10mnginx-deployment-67594d6bf6-v6j7w 1/1 Running 0 10m kubectl get 指令的作用，就是从 Kubernetes 里面获取（GET）指定的 API 对象。可以看到，在这里我还加上了一个 -l 参数，即获取所有匹配 app: nginx 标签的 Pod。需要注意的是，在命令行中，所有 key-value 格式的参数，都使用“=”而非“:”表示。 从这条指令返回的结果中，我们可以看到现在有两个 Pod 处于 Running 状态，也就意味着我们这个 Deployment 所管理的 Pod 都处于预期的状态。 此外， 你还可以使用 kubectl describe 命令，查看一个 API 对象的细节，比如： 12345678910111213141516171819202122232425$ kubectl describe pod nginx-deployment-67594d6bf6-9gdvrName: nginx-deployment-67594d6bf6-9gdvrNamespace: defaultPriority: 0PriorityClassName: &lt;none&gt;Node: node-1/10.168.0.3Start Time: Thu, 16 Aug 2018 08:48:42 +0000Labels: app=nginx pod-template-hash=2315082692Annotations: &lt;none&gt;Status: RunningIP: 10.32.0.23Controlled By: ReplicaSet/nginx-deployment-67594d6bf6...Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 1m default-scheduler Successfully assigned default/nginx-deployment-67594d6bf6-9gdvr to node-1 Normal Pulling 25s kubelet, node-1 pulling image &quot;nginx:1.7.9&quot; Normal Pulled 17s kubelet, node-1 Successfully pulled image &quot;nginx:1.7.9&quot; Normal Created 17s kubelet, node-1 Created container Normal Started 17s kubelet, node-1 Started container 在 kubectl describe 命令返回的结果中，你可以清楚地看到这个 Pod 的详细信息，比如它的 IP 地址等等。其中，有一个部分值得你特别关注，它就是Events（事件）。 在 Kubernetes 执行的过程中，对 API 对象的所有重要操作，都会被记录在这个对象的 Events 里，并且显示在 kubectl describe 指令返回的结果中。 比如，对于这个 Pod，我们可以看到它被创建之后，被调度器调度（Successfully assigned）到了 node-1，拉取了指定的镜像（pulling image），然后启动了 Pod 里定义的容器（Started container）。 所以，这个部分正是我们将来进行 Debug 的重要依据。如果有异常发生，你一定要第一时间查看这些 Events，往往可以看到非常详细的错误信息。 接下来，如果我们要对这个 Nginx 服务进行升级，把它的镜像版本从 1.7.9 升级为 1.8，要怎么做呢？ 很简单，我们只要修改这个 YAML 文件即可。 1234567... spec: containers: - name: nginx image: nginx:1.8 # 这里被从 1.7.9 修改为 1.8 ports: - containerPort: 80 可是，这个修改目前只发生在本地，如何让这个更新在 Kubernetes 里也生效呢？ 我们可以使用 kubectl replace 指令来完成这个更新： 1$ kubectl replace -f nginx-deployment.yaml 不过，在本专栏里，我推荐你使用 kubectl apply 命令，来统一进行 Kubernetes 对象的创建和更新操作，具体做法如下所示： 12345$ kubectl apply -f nginx-deployment.yaml # 修改 nginx-deployment.yaml 的内容 $ kubectl apply -f nginx-deployment.yaml 这样的操作方法，是 Kubernetes“声明式 API”所推荐的使用方法。也就是说，作为用户，你不必关心当前的操作是创建，还是更新，你执行的命令始终是 kubectl apply，而 Kubernetes 则会根据 YAML 文件的内容变化，自动进行具体的处理。 而这个流程的好处是，它有助于帮助开发和运维人员，围绕着可以版本化管理的 YAML 文件，而不是“行踪不定”的命令行进行协作，从而大大降低开发人员和运维人员之间的沟通成本。 举个例子，一位开发人员开发好一个应用，制作好了容器镜像。那么他就可以在应用的发布目录里附带上一个 Deployment 的 YAML 文件。 而运维人员，拿到这个应用的发布目录后，就可以直接用这个 YAML 文件执行 kubectl apply 操作把它运行起来。 这时候，如果开发人员修改了应用，生成了新的发布内容，那么这个 YAML 文件，也就需要被修改，并且成为这次变更的一部分。 而接下来，运维人员可以使用 git diff 命令查看到这个 YAML 文件本身的变化，然后继续用 kubectl apply 命令更新这个应用。 所以说，如果通过容器镜像，我们能够保证应用本身在开发与部署环境里的一致性的话，那么现在，Kubernetes 项目通过这些 YAML 文件，就保证了应用的“部署参数”在开发与部署环境中的一致性。 而当应用本身发生变化时，开发人员和运维人员可以依靠容器镜像来进行同步；当应用部署参数发生变化时，这些 YAML 文件就是他们相互沟通和信任的媒介。 以上，就是 Kubernetes 发布应用的最基本操作了。 接下来，我们再在这个 Deployment 中尝试声明一个 Volume。 在 Kubernetes 中，Volume 是属于 Pod 对象的一部分。所以，我们就需要修改这个 YAML 文件里的 template.spec 字段，如下所示： 12345678910111213141516171819202122232425apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.8 ports: - containerPort: 80 volumeMounts: - mountPath: &quot;/usr/share/nginx/html&quot; name: nginx-vol volumes: - name: nginx-vol emptyDir: &#123;&#125; 可以看到，我们在 Deployment 的 Pod 模板部分添加了一个 volumes 字段，定义了这个 Pod 声明的所有 Volume。它的名字叫作 nginx-vol，类型是 emptyDir。 那什么是 emptyDir 类型呢？ 它其实就等同于我们之前讲过的 Docker 的隐式 Volume 参数，即：不显式声明宿主机目录的 Volume。所以，Kubernetes 也会在宿主机上创建一个临时目录，这个目录将来就会被绑定挂载到容器所声明的 Volume 目录上。 备注：不难看到，Kubernetes 的 emptyDir 类型，只是把 Kubernetes 创建的临时目录作为 Volume 的宿主机目录，交给了 Docker。这么做的原因，是 Kubernetes 不想依赖 Docker 自己创建的那个 _data 目录。 而 Pod 中的容器，使用的是 volumeMounts 字段来声明自己要挂载哪个 Volume，并通过 mountPath 字段来定义容器内的 Volume 目录，比如：/usr/share/nginx/html。 当然，Kubernetes 也提供了显式的 Volume 定义，它叫做 hostPath。比如下面的这个 YAML 文件： 12345... volumes: - name: nginx-vol hostPath: path: /var/data 这样，容器 Volume 挂载的宿主机目录，就变成了 /var/data。 在上述修改完成后，我们还是使用 kubectl apply 指令，更新这个 Deployment: 1$ kubectl apply -f nginx-deployment.yaml 接下来，你可以通过 kubectl get 指令，查看两个 Pod 被逐一更新的过程： 123456789$ kubectl get podsNAME READY STATUS RESTARTS AGEnginx-deployment-5c678cfb6d-v5dlh 0/1 ContainerCreating 0 4snginx-deployment-67594d6bf6-9gdvr 1/1 Running 0 10mnginx-deployment-67594d6bf6-v6j7w 1/1 Running 0 10m$ kubectl get podsNAME READY STATUS RESTARTS AGEnginx-deployment-5c678cfb6d-lg9lw 1/1 Running 0 8snginx-deployment-5c678cfb6d-v5dlh 1/1 Running 0 19s 从返回结果中，我们可以看到，新旧两个 Pod，被交替创建、删除，最后剩下的就是新版本的 Pod。这个滚动更新的过程，我也会在后续进行详细的讲解。 然后，你可以使用 kubectl describe 查看一下最新的 Pod，就会发现 Volume 的信息已经出现在了 Container 描述部分： 12345678910111213...Containers: nginx: Container ID: docker://07b4f89248791c2aa47787e3da3cc94b48576cd173018356a6ec8db2b6041343 Image: nginx:1.8 ... Environment: &lt;none&gt; Mounts: /usr/share/nginx/html from nginx-vol (rw)...Volumes: nginx-vol: Type: EmptyDir (a temporary directory that shares a pod&#x27;s lifetime) 最后，你还可以使用 kubectl exec 指令，进入到这个 Pod 当中（即容器的 Namespace 中）查看这个 Volume 目录： 12$ kubectl exec -it nginx-deployment-5c678cfb6d-lg9lw -- /bin/bash# ls /usr/share/nginx/html 此外，你想要从 Kubernetes 集群中删除这个 Nginx Deployment 的话，直接执行： 1$ kubectl delete -f nginx-deployment.yaml 就可以了。 可以看到，Kubernetes 推荐的使用方式，是用一个 YAML 文件来描述你所要部署的 API 对象。然后，统一使用 kubectl apply 命令完成对这个对象的创建和更新操作。 而 Kubernetes 里“最小”的 API 对象是 Pod。Pod 可以等价为一个应用，所以，Pod 可以由多个紧密协作的容器组成。 在 Kubernetes 中，我们经常会看到它通过一种 API 对象来管理另一种 API 对象，比如 Deployment 和 Pod 之间的关系；而由于 Pod 是“最小”的对象，所以它往往都是被其他对象控制的。这种组合方式，正是 Kubernetes 进行容器编排的重要模式。 而像这样的 Kubernetes API 对象，往往由 Metadata 和 Spec 两部分组成，其中 Metadata 里的 Labels 字段是 Kubernetes 过滤对象的主要手段。 在这些字段里面，容器想要使用的数据卷，也就是 Volume，正是 Pod 的 Spec 字段的一部分。而 Pod 里的每个容器，则需要显式的声明自己要挂载哪个 Volume。 上面这些基于 YAML 文件的容器管理方式，跟 Docker、Mesos 的使用习惯都是不一样的，而从 docker run 这样的命令行操作，向 kubectl apply YAML 文件这样的声明式 API 的转变，是每一个容器技术学习者，必须要跨过的第一道门槛。 所以，如果你想要快速熟悉 Kubernetes，请按照下面的流程进行练习： 首先，在本地通过 Docker 测试代码，制作镜像； 然后，选择合适的 Kubernetes API 对象，编写对应 YAML 文件（比如，Pod，Deployment）； 最后，在 Kubernetes 上部署这个 YAML 文件。 更重要的是，在部署到 Kubernetes 之后，接下来的所有操作，要么通过 kubectl 来执行，要么通过修改 YAML 文件来实现，就尽量不要再碰 Docker 的命令行了。 &nbsp; 原文链接： https://time.geekbang.org/column/100015201","categories":[{"name":"云原生","slug":"云原生","permalink":"http://wht6.github.io/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://wht6.github.io/tags/Kubernetes/"},{"name":"集群","slug":"集群","permalink":"http://wht6.github.io/tags/%E9%9B%86%E7%BE%A4/"}]},{"title":"Docker容器实现原理","slug":"Docker容器实现原理","date":"2022-02-26T08:00:00.000Z","updated":"2022-08-10T07:50:47.624Z","comments":true,"path":"posts/bbe1.html","link":"","permalink":"http://wht6.github.io/posts/bbe1.html","excerpt":"","text":"原文链接： https://time.geekbang.org/column/100015201 容器与进程的关系假如，现在你要写一个计算加法的小程序，这个程序需要的输入来自于一个文件，计算完成后的结果则输出到另一个文件中。 由于计算机只认识 0 和 1，所以无论用哪种语言编写这段代码，最后都需要通过某种方式翻译成二进制文件，才能在计算机操作系统中运行起来。 而为了能够让这些代码正常运行，我们往往还要给它提供数据，比如我们这个加法程序所需要的输入文件。这些数据加上代码本身的二进制文件，放在磁盘上，就是我们平常所说的一个“程序”，也叫代码的可执行镜像（executable image）。 然后，我们就可以在计算机上运行这个“程序”了。 首先，操作系统从“程序”中发现输入数据保存在一个文件中，所以这些数据就被会加载到内存中待命。同时，操作系统又读取到了计算加法的指令，这时，它就需要指示 CPU 完成加法操作。而 CPU 与内存协作进行加法计算，又会使用寄存器存放数值、内存堆栈保存执行的命令和变量。同时，计算机里还有被打开的文件，以及各种各样的 I/O 设备在不断地调用中修改自己的状态。 就这样，一旦“程序”被执行起来，它就从磁盘上的二进制文件，变成了计算机内存中的数据、寄存器里的值、堆栈中的指令、被打开的文件，以及各种设备的状态信息的一个集合。像这样一个程序运起来后的计算机执行环境的总和，就是我们今天的主角：进程。 所以，对于进程来说，它的静态表现就是程序，平常都安安静静地待在磁盘上；而一旦运行起来，它就变成了计算机里的数据和状态的总和，这就是它的动态表现。 而容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造出一个“边界”。 对于 Docker 等大多数 Linux 容器来说，Cgroups 技术是用来制造约束的主要手段，而Namespace 技术则是用来修改进程视图的主要方法。 你可能会觉得 Cgroups 和 Namespace 这两个概念很抽象，别担心，接下来我们一起动手实践一下，你就很容易理解这两项技术了。 假设你已经有了一个 Linux 操作系统上的 Docker 项目在运行，比如我的环境是 Ubuntu 16.04 和 Docker CE 18.05。 接下来，让我们首先创建一个容器来试试。 1$ docker run -it busybox /bin/sh 这个命令是 Docker 项目最重要的一个操作，即大名鼎鼎的 docker run。 而 -it 参数告诉了 Docker 项目在启动容器后，需要给我们分配一个文本输入 / 输出环境，也就是 TTY，跟容器的标准输入相关联，这样我们就可以和这个 Docker 容器进行交互了。而 /bin/sh 就是我们要在 Docker 容器里运行的程序。 所以，上面这条指令翻译成人类的语言就是：请帮我启动一个容器，在容器里执行 /bin/sh，并且给我分配一个命令行终端跟这个容器交互。 这样，我的 Ubuntu 16.04 机器就变成了一个宿主机，而一个运行着 /bin/sh 的容器，就跑在了这个宿主机里面。 上面的例子和原理，如果你已经玩过 Docker，一定不会感到陌生。此时，如果我们在容器里执行一下 ps 指令，就会发现一些更有趣的事情： 1234/ # psPID USER TIME COMMAND 1 root 0:00 /bin/sh 10 root 0:00 ps 可以看到，我们在 Docker 里最开始执行的 /bin/sh，就是这个容器内部的第 1 号进程（PID=1），而这个容器里一共只有两个进程在运行。这就意味着，前面执行的 /bin/sh，以及我们刚刚执行的 ps，已经被 Docker 隔离在了一个跟宿主机完全不同的世界当中。 这究竟是怎么做到呢？ 本来，每当我们在宿主机上运行了一个 /bin/sh 程序，操作系统都会给它分配一个进程编号，比如 PID=100。这个编号是进程的唯一标识，就像员工的工牌一样。所以 PID=100，可以粗略地理解为这个 /bin/sh 是我们公司里的第 100 号员工，而第 1 号员工就自然是比尔 · 盖茨这样统领全局的人物。 而现在，我们要通过 Docker 把这个 /bin/sh 程序运行在一个容器当中。这时候，Docker 就会在这个第 100 号员工入职时给他施一个“障眼法”，让他永远看不到前面的其他 99 个员工，更看不到比尔 · 盖茨。这样，他就会错误地以为自己就是公司里的第 1 号员工。 这种机制，其实就是对被隔离应用的进程空间做了手脚，使得这些进程只能看到重新计算过的进程编号，比如 PID=1。可实际上，他们在宿主机的操作系统里，还是原来的第 100 号进程。 这种技术，就是 Linux 里面的 Namespace 机制。而 Namespace 的使用方式也非常有意思：它其实只是 Linux 创建新进程的一个可选参数。我们知道，在 Linux 系统中创建线程的系统调用是 clone()，比如： 1int pid = clone(main_function, stack_size, SIGCHLD, NULL); 这个系统调用就会为我们创建一个新的进程，并且返回它的进程号 pid。 而当我们用 clone() 系统调用创建一个新进程时，就可以在参数中指定 CLONE_NEWPID 参数，比如： 1int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); 这时，新创建的这个进程将会“看到”一个全新的进程空间，在这个进程空间里，它的 PID 是 1。之所以说“看到”，是因为这只是一个“障眼法”，在宿主机真实的进程空间里，这个进程的 PID 还是真实的数值，比如 100。 当然，我们还可以多次执行上面的 clone() 调用，这样就会创建多个 PID Namespace，而每个 Namespace 里的应用进程，都会认为自己是当前容器里的第 1 号进程，它们既看不到宿主机里真正的进程空间，也看不到其他 PID Namespace 里的具体情况。 而除了我们刚刚用到的 PID Namespace，Linux 操作系统还提供了 Mount、UTS、IPC、Network 和 User 这些 Namespace，用来对各种不同的进程上下文进行“障眼法”操作。 比如，Mount Namespace，用于让被隔离进程只看到当前 Namespace 里的挂载点信息；Network Namespace，用于让被隔离进程看到当前 Namespace 里的网络设备和配置。（这里有点像这个局部变量和全局变量的关系，namespace技术就像把这些新建的进程的有关变量全部用花括号给括起来，变成一个个的局部变量，而无法访问花括号之外的变量） 这，就是 Linux 容器最基本的实现原理了。 所以，Docker 容器这个听起来玄而又玄的概念，实际上是在创建容器进程时，指定了这个进程所需要启用的一组 Namespace 参数。这样，容器就只能“看”到当前 Namespace 所限定的资源、文件、设备、状态，或者配置。而对于宿主机以及其他不相关的程序，它就完全看不到了。 所以说，容器，其实是一种特殊的进程而已。 谈到为“进程划分一个独立空间”的思想，相信你一定会联想到虚拟机。而且，你应该还看过一张虚拟机和容器的对比图。 这幅图的左边，画出了虚拟机的工作原理。其中，名为 Hypervisor 的软件是虚拟机最主要的部分。它通过硬件虚拟化功能，模拟出了运行一个操作系统需要的各种硬件，比如 CPU、内存、I/O 设备等等。然后，它在这些虚拟的硬件上安装了一个新的操作系统，即 Guest OS。 这样，用户的应用进程就可以运行在这个虚拟的机器中，它能看到的自然也只有 Guest OS 的文件和目录，以及这个机器里的虚拟设备。这就是为什么虚拟机也能起到将不同的应用进程相互隔离的作用。 而这幅图的右边，则用一个名为 Docker Engine 的软件替换了 Hypervisor。这也是为什么，很多人会把 Docker 项目称为“轻量级”虚拟化技术的原因，实际上就是把虚拟机的概念套在了容器上。 可是这样的说法，却并不严谨。 在理解了 Namespace 的工作方式之后，你就会明白，跟真实存在的虚拟机不同，在使用 Docker 的时候，并没有一个真正的“Docker 容器”运行在宿主机里面。Docker 项目帮助用户启动的，还是原来的应用进程，只不过在创建这些进程时，Docker 为它们加上了各种各样的 Namespace 参数。 这时，这些进程就会觉得自己是各自 PID Namespace 里的第 1 号进程，只能看到各自 Mount Namespace 里挂载的目录和文件，只能访问到各自 Network Namespace 里的网络设备，就仿佛运行在一个个“容器”里面，与世隔绝。 隔离与限制你应该能够明白，Namespace 技术实际上修改了应用进程看待整个计算机“视图”，即它的“视线”被操作系统做了限制，只能“看到”某些指定的内容。但对于宿主机来说，这些被“隔离”了的进程跟其他进程并没有太大区别。 说到这一点，相信你也能够知道我在上一篇文章最后给你留下的第一个思考题的答案了：在之前虚拟机与容器技术的对比图里，不应该把 Docker Engine 或者任何容器管理工具放在跟 Hypervisor 相同的位置，因为它们并不像 Hypervisor 那样对应用进程的隔离环境负责，也不会创建任何实体的“容器”，真正对隔离环境负责的是宿主机操作系统本身： 所以，在这个对比图里，我们应该把 Docker 画在跟应用同级别并且靠边的位置。这意味着，用户运行在容器里的应用进程，跟宿主机上的其他进程一样，都由宿主机操作系统统一管理，只不过这些被隔离的进程拥有额外设置过的 Namespace 参数。而 Docker 项目在这里扮演的角色，更多的是旁路式的辅助和管理工作。 这样的架构也解释了为什么 Docker 项目比虚拟机更受欢迎的原因。 这是因为，使用虚拟化技术作为应用沙盒，就必须要由 Hypervisor 来负责创建虚拟机，这个虚拟机是真实存在的，并且它里面必须运行一个完整的 Guest OS 才能执行用户的应用进程。这就不可避免地带来了额外的资源消耗和占用。 根据实验，一个运行着 CentOS 的 KVM 虚拟机启动后，在不做优化的情况下，虚拟机自己就需要占用 100~200 MB 内存。此外，用户应用运行在虚拟机里面，它对宿主机操作系统的调用就不可避免地要经过虚拟化软件的拦截和处理，这本身又是一层性能损耗，尤其对计算资源、网络和磁盘 I/O 的损耗非常大。 而相比之下，容器化后的用户应用，却依然还是一个宿主机上的普通进程，这就意味着这些因为虚拟化而带来的性能损耗都是不存在的；而另一方面，使用 Namespace 作为隔离手段的容器并不需要单独的 Guest OS，这就使得容器额外的资源占用几乎可以忽略不计。 所以说，“敏捷”和“高性能”是容器相较于虚拟机最大的优势，也是它能够在 PaaS 这种更细粒度的资源管理平台上大行其道的重要原因。 不过，有利就有弊，基于 Linux Namespace 的隔离机制相比于虚拟化技术也有很多不足之处，其中最主要的问题就是：隔离得不彻底。 首先，既然容器只是运行在宿主机上的一种特殊的进程，那么多个容器之间使用的就还是同一个宿主机的操作系统内核。 尽管你可以在容器里通过 Mount Namespace 单独挂载其他不同版本的操作系统文件，比如 CentOS 或者 Ubuntu，但这并不能改变共享宿主机内核的事实。这意味着，如果你要在 Windows 宿主机上运行 Linux 容器，或者在低版本的 Linux 宿主机上运行高版本的 Linux 容器，都是行不通的。 而相比之下，拥有硬件虚拟化技术和独立 Guest OS 的虚拟机就要方便得多了。最极端的例子是，Microsoft 的云计算平台 Azure，实际上就是运行在 Windows 服务器集群上的，但这并不妨碍你在它上面创建各种 Linux 虚拟机出来。 其次，在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，最典型的例子就是：时间。 这就意味着，如果你的容器中的程序使用 settimeofday(2) 系统调用修改了时间，整个宿主机的时间都会被随之修改，这显然不符合用户的预期。相比于在虚拟机里面可以随便折腾的自由度，在容器里部署应用的时候，“什么能做，什么不能做”，就是用户必须考虑的一个问题。 此外，由于上述问题，尤其是共享宿主机内核的事实，容器给应用暴露出来的攻击面是相当大的，应用“越狱”的难度自然也比虚拟机低得多。 更为棘手的是，尽管在实践中我们确实可以使用 Seccomp 等技术，对容器内部发起的所有系统调用进行过滤和甄别来进行安全加固，但这种方法因为多了一层对系统调用的过滤，一定会拖累容器的性能。何况，默认情况下，谁也不知道到底该开启哪些系统调用，禁止哪些系统调用。 所以，在生产环境中，没有人敢把运行在物理机上的 Linux 容器直接暴露到公网上。当然，基于虚拟化或者独立内核技术的容器实现，则可以比较好地在隔离与性能之间做出平衡。 在介绍完容器的“隔离”技术之后，我们再来研究一下容器的“限制”问题。 也许你会好奇，我们不是已经通过 Linux Namespace 创建了一个“容器”吗，为什么还需要对容器做“限制”呢？ 我还是以 PID Namespace 为例，来给你解释这个问题。 虽然容器内的第 1 号进程在“障眼法”的干扰下只能看到容器里的情况，但是宿主机上，它作为第 100 号进程与其他所有进程之间依然是平等的竞争关系。这就意味着，虽然第 100 号进程表面上被隔离了起来，但是它所能够使用到的资源（比如 CPU、内存），却是可以随时被宿主机上的其他进程（或者其他容器）占用的。当然，这个 100 号进程自己也可能把所有资源吃光。这些情况，显然都不是一个“沙盒”应该表现出来的合理行为。 而Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。 有意思的是，Google 的工程师在 2006 年发起这项特性的时候，曾将它命名为“进程容器”（process container）。实际上，在 Google 内部，“容器”这个术语长期以来都被用于形容被 Cgroups 限制过的进程组。后来 Google 的工程师们说，他们的 KVM 虚拟机也运行在 Borg 所管理的“容器”里，其实也是运行在 Cgroups“容器”当中。这和我们今天说的 Docker 容器差别很大。 Linux Cgroups 的全称是 Linux Control Group。它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。 此外，Cgroups 还能够对进程进行优先级设置、审计，以及将进程挂起和恢复等操作。这里只重点探讨它与容器关系最紧密的“限制”能力，并通过一组实践来带你认识一下 Cgroups。 在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织在操作系统的 /sys/fs/cgroup 路径下。在 Ubuntu 16.04 机器里，我可以用 mount 指令把它们展示出来，这条命令是： 1234567$ mount -t cgroup cpuset on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)cpu on /sys/fs/cgroup/cpu type cgroup (rw,nosuid,nodev,noexec,relatime,cpu)cpuacct on /sys/fs/cgroup/cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct)blkio on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)memory on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)... 它的输出结果，是一系列文件系统目录。如果你在自己的机器上没有看到这些目录，那你就需要自己去挂载 Cgroups。 可以看到，在 /sys/fs/cgroup 下面有很多诸如 cpuset、cpu、 memory 这样的子目录，也叫子系统。这些都是我这台机器当前可以被 Cgroups 进行限制的资源种类。而在子系统对应的资源种类下，你就可以看到该类资源具体可以被限制的方法。比如，对 CPU 子系统来说，我们就可以看到如下几个配置文件，这个指令是： 123$ ls /sys/fs/cgroup/cpucgroup.clone_children cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_releasecgroup.procs cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks 如果熟悉 Linux CPU 管理的话，你就会在它的输出里注意到 cfs_period 和 cfs_quota 这样的关键词。这两个参数需要组合使用，可以用来限制进程在长度为 cfs_period 的一段时间内，只能被分配到总量为 cfs_quota 的 CPU 时间。 而这样的配置文件又如何使用呢？ 你需要在对应的子系统下面创建一个目录，比如，我们现在进入 /sys/fs/cgroup/cpu 目录下： 1234root@ubuntu:/sys/fs/cgroup/cpu$ mkdir containerroot@ubuntu:/sys/fs/cgroup/cpu$ ls container/cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_releasecgroup.procs cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks 这个目录就称为一个“控制组”。你会发现，操作系统会在你新创建的 container 目录下，自动生成该子系统对应的资源限制文件。 现在，我们在后台执行这样一条脚本： 12$ while : ; do : ; done &amp;[1] 226 显然，它执行了一个死循环，可以把计算机的 CPU 吃到 100%，根据它的输出，我们可以看到这个脚本在后台运行的进程号（PID）是 226。 这样，我们可以用 top 指令来确认一下 CPU 有没有被打满： 12$ top%Cpu0 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st 在输出里可以看到，CPU 的使用率已经 100% 了（%Cpu0 :100.0 us）。 而此时，我们可以通过查看 container 目录下的文件，看到 container 控制组里的 CPU quota 还没有任何限制（即：-1），CPU period 则是默认的 100 ms（100000 us）： 1234$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us -1$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_period_us 100000 接下来，我们可以通过修改这些文件的内容来设置限制。 比如，向 container 组里的 cfs_quota 文件写入 20 ms（20000 us）： 1$ echo 20000 &gt; /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us 结合前面的介绍，你应该能明白这个操作的含义，它意味着在每 100 ms 的时间里，被该控制组限制的进程只能使用 20 ms 的 CPU 时间，也就是说这个进程只能使用到 20% 的 CPU 带宽。 接下来，我们把被限制的进程的 PID 写入 container 组里的 tasks 文件，上面的设置就会对该进程生效了： 1$ echo 226 &gt; /sys/fs/cgroup/cpu/container/tasks 我们可以用 top 指令查看一下： 12$ top%Cpu0 : 20.3 us, 0.0 sy, 0.0 ni, 79.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st 可以看到，计算机的 CPU 使用率立刻降到了 20%（%Cpu0 : 20.3 us）。 除 CPU 子系统外，Cgroups 的每一项子系统都有其独有的资源限制能力，比如： blkio，为块设备设定I/O 限制，一般用于磁盘等设备； cpuset，为进程分配单独的 CPU 核和对应的内存节点； memory，为进程设定内存使用的限制。 Linux Cgroups 的设计还是比较易用的，简单粗暴地理解呢，它就是一个子系统目录加上一组资源限制文件的组合。而对于 Docker 等 Linux 容器项目来说，它们只需要在每个子系统下面，为每个容器创建一个控制组（即创建一个新目录），然后在启动容器进程之后，把这个进程的 PID 填写到对应控制组的 tasks 文件中就可以了。 而至于在这些控制组下面的资源文件里填上什么值，就靠用户执行 docker run 时的参数指定了，比如这样一条命令： 1docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash 在启动这个容器后，我们可以通过查看 Cgroups 文件系统下，CPU 子系统中，“docker”这个控制组里的资源限制文件的内容来确认： 1234$ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_period_us 100000$ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_quota_us 20000 这就意味着这个 Docker 容器，只能使用到 20% 的 CPU 带宽。 你现在应该能够理解，一个正在运行的 Docker 容器，其实就是一个启用了多个 Linux Namespace 的应用进程，而这个进程能够使用的资源量，则受 Cgroups 配置的限制。 这也是容器技术中一个非常重要的概念，即：容器是一个“单进程”模型。 由于一个容器的本质就是一个进程，用户的应用进程实际上就是容器里 PID=1 的进程，也是其他后续创建的所有进程的父进程。这就意味着，在一个容器中，你没办法同时运行两个不同的应用，除非你能事先找到一个公共的 PID=1 的程序来充当两个不同应用的父进程，这也是为什么很多人都会用 systemd 或者 supervisord 这样的软件来代替应用本身作为容器的启动进程。 但是，在后面分享容器设计模式时，我还会推荐其他更好的解决办法。这是因为容器本身的设计，就是希望容器和应用能够同生命周期，这个概念对后续的容器编排非常重要。否则，一旦出现类似于“容器是正常运行的，但是里面的应用早已经挂了”的情况，编排系统处理起来就非常麻烦了。 另外，跟 Namespace 的情况类似，Cgroups 对资源的限制能力也有很多不完善的地方，被提及最多的自然是 /proc 文件系统的问题。 众所周知，Linux 下的 /proc 目录存储的是记录当前内核运行状态的一系列特殊文件，用户可以通过访问这些文件，查看系统以及当前正在运行的进程的信息，比如 CPU 使用情况、内存占用率等，这些文件也是 top 指令查看系统信息的主要数据来源。 但是，你如果在容器里执行 top 指令，就会发现，它显示的信息居然是宿主机的 CPU 和内存数据，而不是当前容器的数据。 造成这个问题的原因就是，/proc 文件系统并不知道用户通过 Cgroups 给这个容器做了什么样的资源限制，即：/proc 文件系统不了解 Cgroups 限制的存在。 在生产环境中，这个问题必须进行修正，否则应用程序在容器里读取到的 CPU 核数、可用内存等信息都是宿主机上的数据，这会给应用的运行带来非常大的困惑和风险。这也是在企业中，容器化应用碰到的一个常见问题，也是容器相较于虚拟机另一个不尽如人意的地方。 问题：如何修复容器中的 top 指令以及 /proc 文件系统中的信息呢？ 其实，这个问题的答案在提示里其实已经给出了，即 lxcfs 方案。通过 lxcfs，你可以把宿主机的 /var/lib/lxcfs/proc 文件系统挂载到 Docker 容器的 /proc 目录下。使得容器中进程读取相应文件内容时，实际上会从容器对应的 Cgroups 中读取正确的资源限制。 从而得到正确的 top 命令的返回值。 容器镜像Namespace 的作用是“隔离”，它让应用进程只能看到该 Namespace 内的“世界”；而 Cgroups 的作用是“限制”，它给这个“世界”围上了一圈看不见的墙。这么一折腾，进程就真的被“装”在了一个与世隔绝的房间里，而这些房间就是 PaaS 项目赖以生存的应用“沙盒”。 可是，还有一个问题不知道你有没有仔细思考过：这个房间四周虽然有了墙，但是如果容器进程低头一看地面，又是怎样一副景象呢？ 换句话说，容器里的进程看到的文件系统又是什么样子的呢？ 可能你立刻就能想到，这一定是一个关于 Mount Namespace 的问题：容器里的应用进程，理应看到一份完全独立的文件系统。这样，它就可以在自己的容器目录（比如 /tmp）下进行操作，而完全不会受宿主机以及其他容器的影响。 那么，真实情况是这样吗？ 下面这段小程序的作用是，在创建子进程时开启指定的 Namespace。使用它来验证一下刚刚提到的问题。 12345678910111213141516171819202122232425262728293031#define _GNU_SOURCE#include &lt;sys/mount.h&gt; #include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;stdio.h&gt;#include &lt;sched.h&gt;#include &lt;signal.h&gt;#include &lt;unistd.h&gt;#define STACK_SIZE (1024 * 1024)static char container_stack[STACK_SIZE];char* const container_args[] = &#123; &quot;/bin/bash&quot;, NULL&#125;; int container_main(void* arg)&#123; printf(&quot;Container - inside the container!\\n&quot;); execv(container_args[0], container_args); printf(&quot;Something&#x27;s wrong!\\n&quot;); return 1;&#125; int main()&#123; printf(&quot;Parent - start a container!\\n&quot;); int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWNS | SIGCHLD , NULL); waitpid(container_pid, NULL, 0); printf(&quot;Parent - container stopped!\\n&quot;); return 0;&#125; 这段代码的功能非常简单：在 main 函数里，我们通过 clone() 系统调用创建了一个新的子进程 container_main，并且声明要为它启用 Mount Namespace（即：CLONE_NEWNS 标志）。 而这个子进程执行的，是一个“/bin/bash”程序，也就是一个 shell。所以这个 shell 就运行在了 Mount Namespace 的隔离环境中。 我们来一起编译一下这个程序： 1234$ gcc -o ns ns.c$ ./nsParent - start a container!Container - inside the container! 这样，我们就进入了这个“容器”当中。可是，如果在“容器”里执行一下 ls 指令的话，我们就会发现一个有趣的现象： /tmp 目录下的内容跟宿主机的内容是一样的。 12$ ls /tmp# 你会看到好多宿主机的文件 也就是说： 即使开启了 Mount Namespace，容器进程看到的文件系统也跟宿主机完全一样。 这是怎么回事呢？ 仔细思考一下，你会发现这其实并不难理解：Mount Namespace 修改的，是容器进程对文件系统“挂载点”的认知。但是，这也就意味着，只有在“挂载”这个操作发生之后，进程的视图才会被改变。而在此之前，新创建的容器会直接继承宿主机的各个挂载点。 这时，你可能已经想到了一个解决办法：创建新进程时，除了声明要启用 Mount Namespace 之外，我们还可以告诉容器进程，有哪些目录需要重新挂载，就比如这个 /tmp 目录。于是，我们在容器进程执行前可以添加一步重新挂载 /tmp 目录的操作： 12345678910int container_main(void* arg)&#123; printf(&quot;Container - inside the container!\\n&quot;); // 如果你的机器的根目录的挂载类型是 shared，那必须先重新挂载根目录 // mount(&quot;&quot;, &quot;/&quot;, NULL, MS_PRIVATE, &quot;&quot;); mount(&quot;none&quot;, &quot;/tmp&quot;, &quot;tmpfs&quot;, 0, &quot;&quot;); execv(container_args[0], container_args); printf(&quot;Something&#x27;s wrong!\\n&quot;); return 1;&#125; 可以看到，在修改后的代码里，我在容器进程启动之前，加上了一句 mount(“none”, “/tmp”, “tmpfs”, 0, “”) 语句。就这样，我告诉了容器以 tmpfs（内存盘）格式，重新挂载了 /tmp 目录。 这段修改后的代码，编译执行后的结果又如何呢？我们可以试验一下： 12345$ gcc -o ns ns.c$ ./nsParent - start a container!Container - inside the container!$ ls /tmp 可以看到，这次 /tmp 变成了一个空目录，这意味着重新挂载生效了。我们可以用 mount -l 检查一下： 12$ mount -l | grep tmpfsnone on /tmp type tmpfs (rw,relatime) 可以看到，容器里的 /tmp 目录是以 tmpfs 方式单独挂载的。 更重要的是，因为我们创建的新进程启用了 Mount Namespace，所以这次重新挂载的操作，只在容器进程的 Mount Namespace 中有效。如果在宿主机上用 mount -l 来检查一下这个挂载，你会发现它是不存在的： 12# 在宿主机上$ mount -l | grep tmpfs 这就是 Mount Namespace 跟其他 Namespace 的使用略有不同的地方：它对容器进程视图的改变，一定是伴随着挂载操作（mount）才能生效。 可是，作为一个普通用户，我们希望的是一个更友好的情况：每当创建一个新容器时，我希望容器进程看到的文件系统就是一个独立的隔离环境，而不是继承自宿主机的文件系统。怎么才能做到这一点呢？ 不难想到，我们可以在容器进程启动之前重新挂载它的整个根目录“/”。而由于 Mount Namespace 的存在，这个挂载对宿主机不可见，所以容器进程就可以在里面随便折腾了。 在 Linux 操作系统里，有一个名为 chroot 的命令可以帮助你在 shell 中方便地完成这个工作。顾名思义，它的作用就是帮你“change root file system”，即改变进程的根目录到你指定的位置。它的用法也非常简单。 假设，我们现在有一个 $HOME/test 目录，想要把它作为一个 /bin/bash 进程的根目录。 首先，创建一个 test 目录和几个 lib 文件夹： 123$ mkdir -p $HOME/test$ mkdir -p $HOME/test/&#123;bin,lib64,lib&#125;$ cd $T 然后，把 bash 命令拷贝到 test 目录对应的 bin 路径下： 1$ cp -v /bin/&#123;bash,ls&#125; $HOME/test/bin 接下来，把 bash 命令需要的所有 so 文件，也拷贝到 test 目录对应的 lib 路径下。找到 so 文件可以用 ldd 命令： 123$ T=$HOME/test$ list=&quot;$(ldd /bin/ls | egrep -o &#x27;/lib.*\\.[0-9]&#x27;)&quot;$ for i in $list; do cp -v &quot;$i&quot; &quot;$&#123;T&#125;$&#123;i&#125;&quot;; done 最后，执行 chroot 命令，告诉操作系统，我们将使用 $HOME/test 目录作为 /bin/bash 进程的根目录： 1$ chroot $HOME/test /bin/bash 这时，你如果执行 “ls /“，就会看到，它返回的都是 $HOME/test 目录下面的内容，而不是宿主机的内容。 更重要的是，对于被 chroot 的进程来说，它并不会感受到自己的根目录已经被“修改”成 $HOME/test 了。 这种视图被修改的原理，是不是跟我之前介绍的 Linux Namespace 很类似呢？ 没错！ 实际上，Mount Namespace 正是基于对 chroot 的不断改良才被发明出来的，它也是 Linux 操作系统里的第一个 Namespace。 当然，为了能够让容器的这个根目录看起来更“真实”，我们一般会在这个容器的根目录下挂载一个完整操作系统的文件系统，比如 Ubuntu16.04 的 ISO。这样，在容器启动之后，我们在容器里通过执行 “ls /“ 查看根目录下的内容，就是 Ubuntu 16.04 的所有目录和文件。 而这个挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：rootfs（根文件系统）。 所以，一个最常见的 rootfs，或者说容器镜像，会包括如下所示的一些目录和文件，比如 /bin，/etc，/proc 等等： 12$ ls /bin dev etc home lib lib64 mnt opt proc root run sbin sys tmp usr var 而你进入容器之后执行的 /bin/bash，就是 /bin 目录下的可执行文件，与宿主机的 /bin/bash 完全不同。 现在，你应该可以理解，对 Docker 项目来说，它最核心的原理实际上就是为待创建的用户进程： 启用 Linux Namespace 配置； 设置指定的 Cgroups 参数； 切换进程的根目录（Change Root）。 这样，一个完整的容器就诞生了。不过，Docker 项目在最后一步的切换上会优先使用 pivot_root 系统调用，如果系统不支持，才会使用 chroot。这两个系统调用虽然功能类似，但是也有细微的区别。 另外，需要明确的是，rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在 Linux 操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像。 所以说，rootfs 只包括了操作系统的“躯壳”，并没有包括操作系统的“灵魂”。 那么，对于容器来说，这个操作系统的“灵魂”又在哪里呢？ 实际上，同一台机器上的所有容器，都共享宿主机操作系统的内核。 这就意味着，如果你的应用程序需要配置内核参数、加载额外的内核模块，以及跟内核进行直接的交互，你就需要注意了：这些操作和依赖的对象，都是宿主机操作系统的内核，它对于该机器上的所有容器来说是一个“全局变量”，牵一发而动全身。 这也是容器相比于虚拟机的主要缺陷之一：毕竟后者不仅有模拟出来的硬件机器充当沙盒，而且每个沙盒里还运行着一个完整的 Guest OS 给应用随便折腾。 不过，正是由于 rootfs 的存在，容器才有了一个被反复宣传至今的重要特性：一致性。 什么是容器的“一致性”呢？ 由于云端与本地服务器环境不同，应用的打包过程，一直是使用 PaaS 时最“痛苦”的一个步骤。 但有了容器之后，更准确地说，有了容器镜像（即 rootfs）之后，这个问题被非常优雅地解决了。 由于 rootfs 里打包的不只是应用，而是整个操作系统的文件和目录，也就意味着，应用以及它运行所需要的所有依赖，都被封装在了一起。 事实上，对于大多数开发者而言，他们对应用依赖的理解，一直局限在编程语言层面。比如 Golang 的 Godeps.json。但实际上，一个一直以来很容易被忽视的事实是，对一个应用来说，操作系统本身才是它运行所需要的最完整的“依赖库”。 有了容器镜像“打包操作系统”的能力，这个最基础的依赖环境也终于变成了应用沙盒的一部分。这就赋予了容器所谓的一致性：无论在本地、云端，还是在一台任何地方的机器上，用户只需要解压打包好的容器镜像，那么这个应用运行所需要的完整的执行环境就被重现出来了。 这种深入到操作系统级别的运行环境一致性，打通了应用在本地开发和远端执行环境之间难以逾越的鸿沟。 不过，这时你可能已经发现了另一个非常棘手的问题：难道我每开发一个应用，或者升级一下现有的应用，都要重复制作一次 rootfs 吗？ 比如，我现在用 Ubuntu 操作系统的 ISO 做了一个 rootfs，然后又在里面安装了 Java 环境，用来部署我的 Java 应用。那么，我的另一个同事在发布他的 Java 应用时，显然希望能够直接使用我安装过 Java 环境的 rootfs，而不是重复这个流程。 一种比较直观的解决办法是，我在制作 rootfs 的时候，每做一步“有意义”的操作，就保存一个 rootfs 出来，这样其他同事就可以按需求去用他需要的 rootfs 了。 但是，这个解决办法并不具备推广性。原因在于，一旦你的同事们修改了这个 rootfs，新旧两个 rootfs 之间就没有任何关系了。这样做的结果就是极度的碎片化。 那么，既然这些修改都基于一个旧的 rootfs，我们能不能以增量的方式去做这些修改呢？这样做的好处是，所有人都只需要维护相对于 base rootfs 修改的增量内容，而不是每次修改都制造一个“fork”。 答案当然是肯定的。 这也正是为何，Docker 公司在实现 Docker 镜像时并没有沿用以前制作 rootfs 的标准流程，而是做了一个小小的创新： Docker 在镜像的设计中，引入了层（layer）的概念。也就是说，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量 rootfs。 当然，这个想法不是凭空臆造出来的，而是用到了一种叫作联合文件系统（Union File System）的能力。 Union File System 也叫 UnionFS，最主要的功能是将多个不同位置的目录联合挂载（union mount）到同一个目录下。比如，我现在有两个目录 A 和 B，它们分别有两个文件： 12345678$ tree.├── A│ ├── a│ └── x└── B ├── b └── x 然后，我使用联合挂载的方式，将这两个目录挂载到一个公共的目录 C 上： 12$ mkdir C$ mount -t aufs -o dirs=./A:./B none ./C 这时，我再查看目录 C 的内容，就能看到目录 A 和 B 下的文件被合并到了一起： 12345$ tree ./C./C├── a├── b└── x 可以看到，在这个合并后的目录 C 里，有 a、b、x 三个文件，并且 x 文件只有一份。这，就是“合并”的含义。此外，如果你在目录 C 里对 a、b、x 文件做修改，这些修改也会在对应的目录 A、B 中生效。 那么，在 Docker 项目中，又是如何使用这种 Union File System 的呢？ 我的环境是 Ubuntu 16.04 和 Docker CE 18.05，这对组合默认使用的是 AuFS 这个联合文件系统的实现。你可以通过 docker info 命令，查看到这个信息。 AuFS 的全称是 Another UnionFS，后改名为 Alternative UnionFS，再后来干脆改名叫作 Advance UnionFS，它是对 Linux 原生 UnionFS 的重写和改进。 对于 AuFS 来说，它最关键的目录结构在 /var/lib/docker 路径下的 diff 目录： 1/var/lib/docker/aufs/diff/&lt;layer_id&gt; 而这个目录的作用，我们不妨通过一个具体例子来看一下。 现在，我们启动一个容器，比如： 1$ docker run -d ubuntu:latest sleep 3600 这时候，Docker 就会从 Docker Hub 上拉取一个 Ubuntu 镜像到本地。 这个所谓的“镜像”，实际上就是一个 Ubuntu 操作系统的 rootfs，它的内容是 Ubuntu 操作系统的所有文件和目录。不过，与之前我们讲述的 rootfs 稍微不同的是，Docker 镜像使用的 rootfs，往往由多个“层”组成： 123456789101112$ docker image inspect ubuntu:latest... &quot;RootFS&quot;: &#123; &quot;Type&quot;: &quot;layers&quot;, &quot;Layers&quot;: [ &quot;sha256:f49017d4d5ce9c0f544c...&quot;, &quot;sha256:8f2b771487e9d6354080...&quot;, &quot;sha256:ccd4d61916aaa2159429...&quot;, &quot;sha256:c01d74f99de40e097c73...&quot;, &quot;sha256:268a067217b5fe78e000...&quot; ] &#125; 可以看到，这个 Ubuntu 镜像，实际上由五个层组成。这五个层就是五个增量 rootfs，每一层都是 Ubuntu 操作系统文件与目录的一部分；而在使用镜像时，Docker 会把这些增量联合挂载在一个统一的挂载点上（等价于前面例子里的“/C”目录）。 这个挂载点就是 /var/lib/docker/aufs/mnt/，比如： 1/var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fcfa2a2f5c89dc21ee30e166be823ceaeba15dce645b3e 不出意外的，这个目录里面正是一个完整的 Ubuntu 操作系统： 12$ ls /var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fcfa2a2f5c89dc21ee30e166be823ceaeba15dce645b3ebin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var 那么，前面提到的五个镜像层，又是如何被联合挂载成这样一个完整的 Ubuntu 文件系统的呢？ 这个信息记录在 AuFS 的系统目录 /sys/fs/aufs 下面。 首先，通过查看 AuFS 的挂载信息，我们可以找到这个目录对应的 AuFS 的内部 ID（也叫：si）： 12$ cat /proc/mounts| grep aufsnone /var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fc... aufs rw,relatime,si=972c6d361e6b32ba,dio,dirperm1 0 0 即，si=972c6d361e6b32ba。 然后使用这个 ID，你就可以在 /sys/fs/aufs 下查看被联合挂载在一起的各个层的信息： 12345678$ cat /sys/fs/aufs/si_972c6d361e6b32ba/br[0-9]*/var/lib/docker/aufs/diff/6e3be5d2ecccae7cc...=rw/var/lib/docker/aufs/diff/6e3be5d2ecccae7cc...-init=ro+wh/var/lib/docker/aufs/diff/32e8e20064858c0f2...=ro+wh/var/lib/docker/aufs/diff/2b8858809bce62e62...=ro+wh/var/lib/docker/aufs/diff/20707dce8efc0d267...=ro+wh/var/lib/docker/aufs/diff/72b0744e06247c7d0...=ro+wh/var/lib/docker/aufs/diff/a524a729adadedb90...=ro+wh 从这些信息里，我们可以看到，镜像的层都放置在 /var/lib/docker/aufs/diff 目录下，然后被联合挂载在 /var/lib/docker/aufs/mnt 里面。 而且，从这个结构可以看出来，这个容器的 rootfs 由如下图所示的三部分组成： 第一部分，只读层。 它是这个容器的 rootfs 最下面的五层，对应的正是 ubuntu:latest 镜像的五层。可以看到，它们的挂载方式都是只读的（ro+wh，即 readonly+whiteout，至于什么是 whiteout，我下面马上会讲到）。 这时，我们可以分别查看一下这些层的内容： 123456$ ls /var/lib/docker/aufs/diff/72b0744e06247c7d0...etc sbin usr var$ ls /var/lib/docker/aufs/diff/32e8e20064858c0f2...run$ ls /var/lib/docker/aufs/diff/a524a729adadedb900...bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var 可以看到，这些层，都以增量的方式分别包含了 Ubuntu 操作系统的一部分。 第二部分，可读写层。 它是这个容器的 rootfs 最上面的一层（6e3be5d2ecccae7cc），它的挂载方式为：rw，即 read write。在没有写入文件之前，这个目录是空的。而一旦在容器里做了写操作，你修改产生的内容就会以增量的方式出现在这个层中。 可是，你有没有想到这样一个问题：如果我现在要做的，是删除只读层里的一个文件呢？ 为了实现这样的删除操作，AuFS 会在可读写层创建一个 whiteout 文件，把只读层里的文件“遮挡”起来。 比如，你要删除只读层里一个名叫 foo 的文件，那么这个删除操作实际上是在可读写层创建了一个名叫.wh.foo 的文件。这样，当这两个层被联合挂载之后，foo 文件就会被.wh.foo 文件“遮挡”起来，“消失”了。这个功能，就是“ro+wh”的挂载方式，即只读 +whiteout 的含义。我喜欢把 whiteout 形象地翻译为：“白障”。 所以，最上面这个可读写层的作用，就是专门用来存放你修改 rootfs 后产生的增量，无论是增、删、改，都发生在这里。而当我们使用完了这个被修改过的容器之后，还可以使用 docker commit 和 push 指令，保存这个被修改过的可读写层，并上传到 Docker Hub 上，供其他人使用；而与此同时，原先的只读层里的内容则不会有任何变化。这，就是增量 rootfs 的好处。 第三部分，Init 层。 它是一个以“-init”结尾的层，夹在只读层和读写层之间。Init 层是 Docker 项目单独生成的一个内部层，专门用来存放 /etc/hosts、/etc/resolv.conf 等信息。 需要这样一层的原因是，这些文件本来属于只读的 Ubuntu 镜像的一部分，但是用户往往需要在启动容器时写入一些指定的值比如 hostname，所以就需要在可读写层对它们进行修改。 可是，这些修改往往只对当前的容器有效，我们并不希望执行 docker commit 时，把这些信息连同可读写层一起提交掉。 所以，Docker 做法是，在修改了这些文件之后，以一个单独的层挂载了出来。而用户执行 docker commit 只会提交可读写层，所以是不包含这些内容的。 最终，这 7 个层都被联合挂载到 /var/lib/docker/aufs/mnt 目录下，表现为一个完整的 Ubuntu 操作系统供容器使用。 总结： 我们经常提到的容器镜像，也叫作：rootfs。它只是一个操作系统的所有文件和目录，并不包含内核，最多也就几百兆。而相比之下，传统虚拟机的镜像大多是一个磁盘的“快照”，磁盘有多大，镜像就至少有多大。 通过结合使用 Mount Namespace 和 rootfs，容器就能够为进程构建出一个完善的文件系统隔离环境。当然，这个功能的实现还必须感谢 chroot 和 pivot_root 这两个系统调用切换进程根目录的能力。 而在 rootfs 的基础上，Docker 公司创新性地提出了使用多个增量 rootfs 联合挂载一个完整 rootfs 的方案，这就是容器镜像中“层”的概念。 通过“分层镜像”的设计，以 Docker 镜像为核心，来自不同公司、不同团队的技术人员被紧密地联系在了一起。而且，由于容器镜像的操作是增量式的，这样每次镜像拉取、推送的内容，比原本多个完整的操作系统的大小要小得多；而共享层的存在，可以使得所有这些容器镜像需要的总空间，也比每个镜像的总和要小。这样就使得基于容器镜像的团队协作，要比基于动则几个 GB 的虚拟机磁盘镜像的协作要敏捷得多。 更重要的是，一旦这个镜像被发布，那么你在全世界的任何一个地方下载这个镜像，得到的内容都完全一致，可以完全复现这个镜像制作者当初的完整环境。这，就是容器技术“强一致性”的重要体现。 而这种价值正是支撑 Docker 公司在 2014~2016 年间迅猛发展的核心动力。容器镜像的发明，不仅打通了“开发 - 测试 - 部署”流程的每一个环节，更重要的是： 容器镜像将会成为未来软件的主流发布方式。 问题：既然容器的 rootfs（比如，Ubuntu 镜像），是以只读方式挂载的，那么又如何在容器里修改 Ubuntu 镜像的内容呢？ 简单地说，修改一个镜像里的文件的时候，联合文件系统首先会从上到下在各个层中查找有没有目标文件。如果找到，就把这个文件复制到可读写层进行修改。这个修改的结果会屏蔽掉下层的文件，这种方式就被称为 copy-on-write。 重新认识Docker容器这一次，我要用 Docker 部署一个用 Python 编写的 Web 应用。这个应用的代码部分非常简单： 1234567891011121314from flask import Flaskimport socketimport os app = Flask(__name__) @app.route(&#x27;/&#x27;)def hello(): html = &quot;&lt;h3&gt;Hello &#123;name&#125;!&lt;/h3&gt;&quot; \\ &quot;&lt;b&gt;Hostname:&lt;/b&gt; &#123;hostname&#125;&lt;br/&gt;&quot; return html.format(name=os.getenv(&quot;NAME&quot;, &quot;world&quot;), hostname=socket.gethostname()) if __name__ == &quot;__main__&quot;: app.run(host=&#x27;0.0.0.0&#x27;, port=80) 在这段代码中，我使用 Flask 框架启动了一个 Web 服务器，而它唯一的功能是：如果当前环境中有“NAME”这个环境变量，就把它打印在“Hello”后，否则就打印“Hello world”，最后再打印出当前环境的 hostname。 这个应用的依赖，则被定义在了同目录下的 requirements.txt 文件里，内容如下所示： 12$ cat requirements.txtFlask 而将这样一个应用容器化的第一步，是制作容器镜像。 不过，相较于我之前介绍的制作 rootfs 的过程，Docker 为你提供了一种更便捷的方式，叫作 Dockerfile，如下所示。 1234567891011121314151617181920# 使用官方提供的 Python 开发镜像作为基础镜像FROM python:2.7-slim # 将工作目录切换为 /appWORKDIR /app # 将当前目录下的所有内容复制到 /app 下ADD . /app # 使用 pip 命令安装这个应用所需要的依赖RUN pip install --trusted-host pypi.python.org -r requirements.txt # 允许外界访问容器的 80 端口EXPOSE 80 # 设置环境变量ENV NAME World # 设置容器进程为：python app.py，即：这个 Python 应用的启动命令CMD [&quot;python&quot;, &quot;app.py&quot;] 通过这个文件的内容，你可以看到Dockerfile 的设计思想，是使用一些标准的原语（即大写高亮的词语），描述我们所要构建的 Docker 镜像。并且这些原语，都是按顺序处理的。 比如 FROM 原语，指定了“python:2.7-slim”这个官方维护的基础镜像，从而免去了安装 Python 等语言环境的操作。否则，这一段我们就得这么写了： 123FROM ubuntu:latestRUN apt-get update -yRUN apt-get install -y python-pip python-dev build-essential... 其中，RUN 原语就是在容器里执行 shell 命令的意思。 而 WORKDIR，意思是在这一句之后，Dockerfile 后面的操作都以这一句指定的 /app 目录作为当前目录。 所以，到了最后的 CMD，意思是 Dockerfile 指定 python app.py 为这个容器的进程。这里，app.py 的实际路径是 /app/app.py。所以，CMD [“python”, “app.py”] 等价于 “docker run python app.py”。 另外，在使用 Dockerfile 时，你可能还会看到一个叫作 ENTRYPOINT 的原语。实际上，它和 CMD 都是 Docker 容器进程启动所必需的参数，完整执行格式是：“ENTRYPOINT CMD”。 但是，默认情况下，Docker 会为你提供一个隐含的 ENTRYPOINT，即：/bin/sh -c。所以，在不指定 ENTRYPOINT 时，比如在我们这个例子里，实际上运行在容器里的完整进程是：/bin/sh -c “python app.py”，即 CMD 的内容就是 ENTRYPOINT 的参数。 备注：基于以上原因，我们后面会统一称 Docker 容器的启动进程为 ENTRYPOINT，而不是 CMD。 需要注意的是，Dockerfile 里的原语并不都是指对容器内部的操作。就比如 ADD，它指的是把当前目录（即 Dockerfile 所在的目录）里的文件，复制到指定容器内的目录当中。 读懂这个 Dockerfile 之后，我再把上述内容，保存到当前目录里一个名叫“Dockerfile”的文件中： 12$ lsDockerfile app.py requirements.txt 接下来，我就可以让 Docker 制作这个镜像了，在当前目录执行： 1$ docker build -t helloworld . 其中，-t 的作用是给这个镜像加一个 Tag，即：起一个好听的名字。docker build 会自动加载当前目录下的 Dockerfile 文件，然后按照顺序，执行文件中的原语。而这个过程，实际上可以等同于 Docker 使用基础镜像启动了一个容器，然后在容器中依次执行 Dockerfile 中的原语。 需要注意的是，Dockerfile 中的每个原语执行后，都会生成一个对应的镜像层。即使原语本身并没有明显地修改文件的操作（比如，ENV 原语），它对应的层也会存在。只不过在外界看来，这个层是空的。 docker build 操作完成后，我可以通过 docker images 命令查看结果： 1234$ docker image ls REPOSITORY TAG IMAGE IDhelloworld latest 653287cdf998 通过这个镜像 ID，就可以查看这些新增的层在 AuFS 路径下对应的文件和目录了。 接下来，我使用这个镜像，通过 docker run 命令启动容器： 1$ docker run -p 4000:80 helloworld 在这一句命令中，镜像名 helloworld 后面，我什么都不用写，因为在 Dockerfile 中已经指定了 CMD。否则，我就得把进程的启动命令加在后面： 1$ docker run -p 4000:80 helloworld python app.py 容器启动之后，我可以使用 docker ps 命令看到： 123$ docker psCONTAINER ID IMAGE COMMAND CREATED4ddf4638572d helloworld &quot;python app.py&quot; 10 seconds ago 同时，我已经通过 -p 4000:80 告诉了 Docker，请把容器内的 80 端口映射在宿主机的 4000 端口上。 这样做的目的是，只要访问宿主机的 4000 端口，我就可以看到容器里应用返回的结果： 12$ curl http://localhost:4000&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 4ddf4638572d&lt;br/&gt; 否则，我就得先用 docker inspect 命令查看容器的 IP 地址，然后访问“http://&lt; 容器 IP 地址 &gt;:80”才可以看到容器内应用的返回。 至此，我已经使用容器完成了一个应用的开发与测试，如果现在想要把这个容器的镜像上传到 DockerHub 上分享给更多的人，我要怎么做呢？ 为了能够上传镜像，我首先需要注册一个 Docker Hub 账号，然后使用 docker login 命令登录。 接下来，我要用 docker tag 命令给容器镜像起一个完整的名字： 1$ docker tag helloworld geektime/helloworld:v1 其中，geektime 是我在 Docker Hub 上的用户名，它的“学名”叫镜像仓库（Repository）；“/”后面的 helloworld 是这个镜像的名字，而“v1”则是我给这个镜像分配的版本号。 然后，我执行 docker push： 1$ docker push geektime/helloworld:v1 这样，我就可以把这个镜像上传到 Docker Hub 上了。 此外，我还可以使用 docker commit 指令，把一个正在运行的容器，直接提交为一个镜像。一般来说，需要这么操作原因是：这个容器运行起来后，我又在里面做了一些操作，并且要把操作结果保存到镜像里，比如： 1234567$ docker exec -it 4ddf4638572d /bin/sh# 在容器内部新建了一个文件root@4ddf4638572d:/app# touch test.txtroot@4ddf4638572d:/app# exit # 将这个新建的文件提交到镜像中保存$ docker commit 4ddf4638572d geektime/helloworld:v2 这里，我使用了 docker exec 命令进入到了容器当中。在了解了 Linux Namespace 的隔离机制后，你应该会很自然地想到一个问题：docker exec 是怎么做到进入容器里的呢？ 实际上，Linux Namespace 创建的隔离空间虽然看不见摸不着，但一个进程的 Namespace 信息在宿主机上是确确实实存在的，并且是以一个文件的方式存在。 比如，通过如下指令，你可以看到当前正在运行的 Docker 容器的进程号（PID）是 25686： 12$ docker inspect --format &#x27;&#123;&#123; .State.Pid &#125;&#125;&#x27; 4ddf4638572d25686 这时，你可以通过查看宿主机的 proc 文件，看到这个 25686 进程的所有 Namespace 对应的文件： 12345678910$ ls -l /proc/25686/nstotal 0lrwxrwxrwx 1 root root 0 Aug 13 14:05 cgroup -&gt; cgroup:[4026531835]lrwxrwxrwx 1 root root 0 Aug 13 14:05 ipc -&gt; ipc:[4026532278]lrwxrwxrwx 1 root root 0 Aug 13 14:05 mnt -&gt; mnt:[4026532276]lrwxrwxrwx 1 root root 0 Aug 13 14:05 net -&gt; net:[4026532281]lrwxrwxrwx 1 root root 0 Aug 13 14:05 pid -&gt; pid:[4026532279]lrwxrwxrwx 1 root root 0 Aug 13 14:05 pid_for_children -&gt; pid:[4026532279]lrwxrwxrwx 1 root root 0 Aug 13 14:05 user -&gt; user:[4026531837]lrwxrwxrwx 1 root root 0 Aug 13 14:05 uts -&gt; uts:[4026532277] 可以看到，一个进程的每种 Linux Namespace，都在它对应的 /proc/[进程号]/ns 下有一个对应的虚拟文件，并且链接到一个真实的 Namespace 文件上。 有了这样一个可以“hold 住”所有 Linux Namespace 的文件，我们就可以对 Namespace 做一些很有意义事情了，比如：加入到一个已经存在的 Namespace 当中。 这也就意味着：一个进程，可以选择加入到某个进程已有的 Namespace 当中，从而达到“进入”这个进程所在容器的目的，这正是 docker exec 的实现原理。 而这个操作所依赖的，乃是一个名叫 setns() 的 Linux 系统调用。它的调用方法，我可以用如下一段小程序为你说明： 12345678910111213141516171819#define _GNU_SOURCE#include &lt;fcntl.h&gt;#include &lt;sched.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt; #define errExit(msg) do &#123; perror(msg); exit(EXIT_FAILURE);&#125; while (0) int main(int argc, char *argv[]) &#123; int fd; fd = open(argv[1], O_RDONLY); if (setns(fd, 0) == -1) &#123; errExit(&quot;setns&quot;); &#125; execvp(argv[2], &amp;argv[2]); errExit(&quot;execvp&quot;);&#125; 这段代码功能非常简单：它一共接收两个参数，第一个参数是 argv[1]，即当前进程要加入的 Namespace 文件的路径，比如 /proc/25686/ns/net；而第二个参数，则是你要在这个 Namespace 里运行的进程，比如 /bin/bash。 这段代码的的核心操作，则是通过 open() 系统调用打开了指定的 Namespace 文件，并把这个文件的描述符 fd 交给 setns() 使用。在 setns() 执行后，当前进程就加入了这个文件对应的 Linux Namespace 当中了。 现在，你可以编译执行一下这个程序，加入到容器进程（PID=25686）的 Network Namespace 中： 1234567891011121314151617181920$ gcc -o set_ns set_ns.c $ ./set_ns /proc/25686/ns/net /bin/bash $ ifconfigeth0 Link encap:Ethernet HWaddr 02:42:ac:11:00:02 inet addr:172.17.0.2 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:12 errors:0 dropped:0 overruns:0 frame:0 TX packets:10 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:976 (976.0 B) TX bytes:796 (796.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) 正如上所示，当我们执行 ifconfig 命令查看网络设备时，我会发现能看到的网卡“变少”了：只有两个。而我的宿主机则至少有四个网卡。这是怎么回事呢？ 实际上，在 setns() 之后我看到的这两个网卡，正是我在前面启动的 Docker 容器里的网卡。也就是说，我新创建的这个 /bin/bash 进程，由于加入了该容器进程（PID=25686）的 Network Namepace，它看到的网络设备与这个容器里是一样的，即：/bin/bash 进程的网络设备视图，也被修改了。 而一旦一个进程加入到了另一个 Namespace 当中，在宿主机的 Namespace 文件上，也会有所体现。 在宿主机上，你可以用 ps 指令找到这个 set_ns 程序执行的 /bin/bash 进程，其真实的 PID 是 28499： 123# 在宿主机上ps aux | grep /bin/bashroot 28499 0.0 0.0 19944 3612 pts/0 S 14:15 0:00 /bin/bash 这时，如果按照前面介绍过的方法，查看一下这个 PID=28499 的进程的 Namespace，你就会发现这样一个事实： 12345$ ls -l /proc/28499/ns/netlrwxrwxrwx 1 root root 0 Aug 13 14:18 /proc/28499/ns/net -&gt; net:[4026532281] $ ls -l /proc/25686/ns/netlrwxrwxrwx 1 root root 0 Aug 13 14:05 /proc/25686/ns/net -&gt; net:[4026532281] 在 /proc/[PID]/ns/net 目录下，这个 PID=28499 进程，与我们前面的 Docker 容器进程（PID=25686）指向的 Network Namespace 文件完全一样。这说明这两个进程，共享了这个名叫 net:[4026532281] 的 Network Namespace。 此外，Docker 还专门提供了一个参数，可以让你启动一个容器并“加入”到另一个容器的 Network Namespace 里，这个参数就是 -net，比如: 1$ docker run -it --net container:4ddf4638572d busybox ifconfig 这样，我们新启动的这个容器，就会直接加入到 ID=4ddf4638572d 的容器，也就是我们前面的创建的 Python 应用容器（PID=25686）的 Network Namespace 中。所以，这里 ifconfig 返回的网卡信息，跟我前面那个小程序返回的结果一模一样，你也可以尝试一下。 而如果我指定–net=host，就意味着这个容器不会为进程启用 Network Namespace。这就意味着，这个容器拆除了 Network Namespace 的“隔离墙”，所以，它会和宿主机上的其他普通进程一样，直接共享宿主机的网络栈。这就为容器直接操作和使用宿主机网络提供了一个渠道。 转了一个大圈子，我其实是为你详细解读了 docker exec 这个操作背后，Linux Namespace 更具体的工作原理。 这种通过操作系统进程相关的知识，逐步剖析 Docker 容器的方法，是理解容器的一个关键思路，希望你一定要掌握。 现在，我们再一起回到前面提交镜像的操作 docker commit 上来吧。 docker commit，实际上就是在容器运行起来后，把最上层的“可读写层”，加上原先容器镜像的只读层，打包组成了一个新的镜像。当然，下面这些只读层在宿主机上是共享的，不会占用额外的空间。 而由于使用了联合文件系统，你在容器里对镜像 rootfs 所做的任何修改，都会被操作系统先复制到这个可读写层，然后再修改。这就是所谓的：Copy-on-Write。 而正如前所说，Init 层的存在，就是为了避免你执行 docker commit 时，把 Docker 自己对 /etc/hosts 等文件做的修改，也一起提交掉。 有了新的镜像，我们就可以把它推送到 Docker Hub 上了： 1$ docker push geektime/helloworld:v2 你可能还会有这样的问题：我在企业内部，能不能也搭建一个跟 Docker Hub 类似的镜像上传系统呢？ 当然可以，这个统一存放镜像的系统，就叫作 Docker Registry。感兴趣的话，你可以查看Docker 的官方文档，以及VMware 的 Harbor 项目。 最后，我再来讲解一下 Docker 项目另一个重要的内容：Volume（数据卷）。 前面我已经介绍过，容器技术使用了 rootfs 机制和 Mount Namespace，构建出了一个同宿主机完全隔离开的文件系统环境。这时候，我们就需要考虑这样两个问题： 容器里进程新建的文件，怎么才能让宿主机获取到？ 宿主机上的文件和目录，怎么才能让容器里的进程访问到？ 这正是 Docker Volume 要解决的问题：Volume 机制，允许你将宿主机上指定的目录或者文件，挂载到容器里面进行读取和修改操作。 在 Docker 项目里，它支持两种 Volume 声明方式，可以把宿主机目录挂载进容器的 /test 目录当中： 12$ docker run -v /test ...$ docker run -v /home:/test ... 而这两种声明方式的本质，实际上是相同的：都是把一个宿主机的目录挂载进了容器的 /test 目录。 只不过，在第一种情况下，由于你并没有显示声明宿主机目录，那么 Docker 就会默认在宿主机上创建一个临时目录 /var/lib/docker/volumes/[VOLUME_ID]/_data，然后把它挂载到容器的 /test 目录上。而在第二种情况下，Docker 就直接把宿主机的 /home 目录挂载到容器的 /test 目录上。 那么，Docker 又是如何做到把一个宿主机上的目录或者文件，挂载到容器里面去呢？难道又是 Mount Namespace 的黑科技吗？ 实际上，并不需要这么麻烦。 我已经介绍过，当容器进程被创建之后，尽管开启了 Mount Namespace，但是在它执行 chroot（或者 pivot_root）之前，容器进程一直可以看到宿主机上的整个文件系统。 而宿主机上的文件系统，也自然包括了我们要使用的容器镜像。这个镜像的各个层，保存在 /var/lib/docker/aufs/diff 目录下，在容器进程启动后，它们会被联合挂载在 /var/lib/docker/aufs/mnt/ 目录中，这样容器所需的 rootfs 就准备好了。 所以，我们只需要在 rootfs 准备好之后，在执行 chroot 之前，把 Volume 指定的宿主机目录（比如 /home 目录），挂载到指定的容器目录（比如 /test 目录）在宿主机上对应的目录（即 /var/lib/docker/aufs/mnt/[可读写层 ID]/test）上，这个 Volume 的挂载工作就完成了。 更重要的是，由于执行这个挂载操作时，“容器进程”已经创建了，也就意味着此时 Mount Namespace 已经开启了。所以，这个挂载事件只在这个容器里可见。你在宿主机上，是看不见容器内部的这个挂载点的。这就保证了容器的隔离性不会被 Volume 打破。 注意：这里提到的 “ 容器进程 “，是 Docker 创建的一个容器初始化进程 (dockerinit)，而不是应用进程 (ENTRYPOINT + CMD)。dockerinit 会负责完成根目录的准备、挂载设备和目录、配置 hostname 等一系列需要在容器内进行的初始化操作。最后，它通过 execv() 系统调用，让应用进程取代自己，成为容器里的 PID=1 的进程。 而这里要使用到的挂载技术，就是 Linux 的绑定挂载（bind mount）机制。它的主要作用就是，允许你将一个目录或者文件，而不是整个设备，挂载到一个指定的目录上。并且，这时你在该挂载点上进行的任何操作，只是发生在被挂载的目录或者文件上，而原挂载点的内容则会被隐藏起来且不受影响。 其实，如果你了解 Linux 内核的话，就会明白，绑定挂载实际上是一个 inode 替换的过程。在 Linux 操作系统中，inode 可以理解为存放文件内容的“对象”，而 dentry，也叫目录项，就是访问这个 inode 所使用的“指针”。 正如上图所示，mount —bind /home /test，会将 /home 挂载到 /test 上。其实相当于将 /test 的 dentry，重定向到了 /home 的 inode。这样当我们修改 /test 目录时，实际修改的是 /home 目录的 inode。这也就是为何，一旦执行 umount 命令，/test 目录原先的内容就会恢复：因为修改真正发生在的，是 /home 目录里。 所以，在一个正确的时机，进行一次绑定挂载，Docker 就可以成功地将一个宿主机上的目录或文件，不动声色地挂载到容器中。 这样，进程在容器里对这个 /test 目录进行的所有操作，都实际发生在宿主机的对应目录（比如，/home，或者 /var/lib/docker/volumes/[VOLUME_ID]/_data）里，而不会影响容器镜像的内容。 那么，这个 /test 目录里的内容，既然挂载在容器 rootfs 的可读写层，它会不会被 docker commit 提交掉呢？ 也不会。 这个原因其实我们前面已经提到过。容器的镜像操作，比如 docker commit，都是发生在宿主机空间的。而由于 Mount Namespace 的隔离作用，宿主机并不知道这个绑定挂载的存在。所以，在宿主机看来，容器中可读写层的 /test 目录（/var/lib/docker/aufs/mnt/[可读写层 ID]/test），始终是空的。 不过，由于 Docker 一开始还是要创建 /test 这个目录作为挂载点，所以执行了 docker commit 之后，你会发现新产生的镜像里，会多出来一个空的 /test 目录。毕竟，新建目录操作，又不是挂载操作，Mount Namespace 对它可起不到“障眼法”的作用。 结合以上的讲解，我们现在来亲自验证一下： 首先，启动一个 helloworld 容器，给它声明一个 Volume，挂载在容器里的 /test 目录上： 12$ docker run -d -v /test helloworldcf53b766fa6f 容器启动之后，我们来查看一下这个 Volume 的 ID： 123$ docker volume lsDRIVER VOLUME NAMElocal cb1c2f7221fa9b0971cc35f68aa1034824755ac44a034c0c0a1dd318838d3a6d 然后，使用这个 ID，可以找到它在 Docker 工作目录下的 volumes 路径： 1$ ls /var/lib/docker/volumes/cb1c2f7221fa/_data/ 这个 _data 文件夹，就是这个容器的 Volume 在宿主机上对应的临时目录了。 接下来，我们在容器的 Volume 里，添加一个文件 text.txt： 123$ docker exec -it cf53b766fa6f /bin/shcd test/touch text.txt 这时，我们再回到宿主机，就会发现 text.txt 已经出现在了宿主机上对应的临时目录里： 12$ ls /var/lib/docker/volumes/cb1c2f7221fa/_data/text.txt 可是，如果你在宿主机上查看该容器的可读写层，虽然可以看到这个 /test 目录，但其内容是空的： 1$ ls /var/lib/docker/aufs/mnt/6780d0778b8a/test 可以确认，容器 Volume 里的信息，并不会被 docker commit 提交掉；但这个挂载点目录 /test 本身，则会出现在新的镜像当中。 以上内容，就是 Docker Volume 的核心原理了。 Docker 容器，我们实际上就可以用下面这个“全景图”描述出来： 这个容器进程“python app.py”，运行在由 Linux Namespace 和 Cgroups 构成的隔离环境里；而它运行所需要的各种文件，比如 python，app.py，以及整个操作系统文件，则由多个联合挂载在一起的 rootfs 层提供。 这些 rootfs 层的最下层，是来自 Docker 镜像的只读层。 在只读层之上，是 Docker 自己添加的 Init 层，用来存放被临时修改过的 /etc/hosts 等文件。 而 rootfs 的最上层是一个可读写层，它以 Copy-on-Write 的方式存放任何对只读层的修改，容器声明的 Volume 的挂载点，也出现在这一层。 问题：你在查看 Docker 容器的 Namespace 时，是否注意到有一个叫 cgroup 的 Namespace？它是 Linux 4.6 之后新增加的一个 Namespace，你知道它的作用吗？ Linux 内核从 4.6 开始，支持了一个新的 Namespace 叫作：Cgroup Namespace。 我们知道，正常情况下，在一个容器里查看 /proc/$PID/cgroup，是会看到整个宿主机的 cgroup 信息的。而有了 Cgroup Namespace 后，每个容器里的进程都会有自己 Cgroup Namespace，从而获得一个属于自己的 Cgroups 文件目录视图。也就是说，Cgroups 文件系统也可以被 Namespace 隔离起来了。 Kubernetes的本质一个“容器”，实际上是一个由 Linux Namespace、Linux Cgroups 和 rootfs 三种技术构建出来的进程的隔离环境。 从这个结构中我们不难看出，一个正在运行的 Linux 容器，其实可以被“一分为二”地看待： 一组联合挂载在 /var/lib/docker/aufs/mnt 上的 rootfs，这一部分我们称为“容器镜像”（Container Image），是容器的静态视图； 一个由 Namespace+Cgroups 构成的隔离环境，这一部分我们称为“容器运行时”（Container Runtime），是容器的动态视图。 更进一步地说，作为一名开发者，我并不关心容器运行时的差异。因为，在整个“开发 - 测试 - 发布”的流程中，真正承载着容器信息进行传递的，是容器镜像，而不是容器运行时。 这个重要假设，正是容器技术圈在 Docker 项目成功后不久，就迅速走向了“容器编排”这个“上层建筑”的主要原因：作为一家云服务商或者基础设施提供商，我只要能够将用户提交的 Docker 镜像以容器的方式运行起来，就能成为这个非常热闹的容器生态图上的一个承载点，从而将整个容器技术栈上的价值，沉淀在我的这个节点上。 更重要的是，只要从我这个承载点向 Docker 镜像制作者和使用者方向回溯，整条路径上的各个服务节点，比如 CI/CD、监控、安全、网络、存储等等，都有我可以发挥和盈利的余地。这个逻辑，正是所有云计算提供商如此热衷于容器技术的重要原因：通过容器镜像，它们可以和潜在用户（即，开发者）直接关联起来。 从一个开发者和单一的容器镜像，到无数开发者和庞大的容器集群，容器技术实现了从“容器”到“容器云”的飞跃，标志着它真正得到了市场和生态的认可。 这样，容器就从一个开发者手里的小工具，一跃成为了云计算领域的绝对主角；而能够定义容器组织和管理规范的“容器编排”技术，则当仁不让地坐上了容器技术领域的“头把交椅”。 这其中，最具代表性的容器编排工具，当属 Docker 公司的 Compose+Swarm 组合，以及 Google 与 RedHat 公司共同主导的 Kubernetes 项目。 跟很多基础设施领域先有工程实践、后有方法论的发展路线不同，Kubernetes 项目的理论基础则要比工程实践走得靠前得多，这当然要归功于 Google 公司在 2015 年 4 月发布的 Borg 论文了。 Borg 系统，一直以来都被誉为 Google 公司内部最强大的“秘密武器”。虽然略显夸张，但这个说法倒不算是吹牛。 因为，相比于 Spanner、BigTable 等相对上层的项目，Borg 要承担的责任，是承载 Google 公司整个基础设施的核心依赖。在 Google 公司已经公开发表的基础设施体系论文中，Borg 项目当仁不让地位居整个基础设施技术栈的最底层。 正是由于这样的定位，Borg 可以说是 Google 最不可能开源的一个项目。而幸运地是，得益于 Docker 项目和容器技术的风靡，它却终于得以以另一种方式与开源社区见面，这个方式就是 Kubernetes 项目。 所以，相比于“小打小闹”的 Docker 公司、“旧瓶装新酒”的 Mesos 社区，Kubernetes 项目从一开始就比较幸运地站上了一个他人难以企及的高度：在它的成长阶段，这个项目每一个核心特性的提出，几乎都脱胎于 Borg/Omega 系统的设计与经验。更重要的是，这些特性在开源社区落地的过程中，又在整个社区的合力之下得到了极大的改进，修复了很多当年遗留在 Borg 体系中的缺陷和问题。 所以，尽管在发布之初被批评是“曲高和寡”，但是在逐渐觉察到 Docker 技术栈的“稚嫩”和 Mesos 社区的“老迈”之后，这个社区很快就明白了：Kubernetes 项目在 Borg 体系的指导下，体现出了一种独有的“先进性”与“完备性”，而这些特质才是一个基础设施领域开源项目赖以生存的核心价值。 为了更好地理解这两种特质，我们不妨从 Kubernetes 的顶层设计说起。 首先，Kubernetes 项目要解决的问题是什么？ 编排？调度？容器云？还是集群管理？ 实际上，这个问题到目前为止都没有固定的答案。因为在不同的发展阶段，Kubernetes 需要着重解决的问题是不同的。 但是，对于大多数用户来说，他们希望 Kubernetes 项目带来的体验是确定的：现在我有了应用的容器镜像，请帮我在一个给定的集群上把这个应用运行起来。 更进一步地说，我还希望 Kubernetes 能给我提供路由网关、水平扩展、监控、备份、灾难恢复等一系列运维能力。 等一下，这些功能听起来好像有些耳熟？这不就是经典 PaaS（比如，Cloud Foundry）项目的能力吗？ 而且，有了 Docker 之后，我根本不需要什么 Kubernetes、PaaS，只要使用 Docker 公司的 Compose+Swarm 项目，就完全可以很方便地 DIY 出这些功能了！ 所以说，如果 Kubernetes 项目只是停留在拉取用户镜像、运行容器，以及提供常见的运维功能的话，那么别说跟“原生”的 Docker Swarm 项目竞争了，哪怕跟经典的 PaaS 项目相比也难有什么优势可言。 而实际上，在定义核心功能的过程中，Kubernetes 项目正是依托着 Borg 项目的理论优势，才在短短几个月内迅速站稳了脚跟，进而确定了一个如下图所示的全局架构： 我们可以看到，Kubernetes 项目的架构，跟它的原型项目 Borg 非常类似，都由 Master 和 Node 两种节点组成，而这两种角色分别对应着控制节点和计算节点。 其中，控制节点，即 Master 节点，由三个紧密协作的独立组件组合而成，它们分别是负责 API 服务的 kube-apiserver、负责调度的 kube-scheduler，以及负责容器编排的 kube-controller-manager。整个集群的持久化数据，则由 kube-apiserver 处理后保存在 Etcd 中。 而计算节点上最核心的部分，则是一个叫作 kubelet 的组件。 在 Kubernetes 项目中，kubelet 主要负责同容器运行时（比如 Docker 项目）打交道。而这个交互所依赖的，是一个称作 CRI（Container Runtime Interface）的远程调用接口，这个接口定义了容器运行时的各项核心操作，比如：启动一个容器需要的所有参数。 这也是为何，Kubernetes 项目并不关心你部署的是什么容器运行时、使用的什么技术实现，只要你的这个容器运行时能够运行标准的容器镜像，它就可以通过实现 CRI 接入到 Kubernetes 项目当中。 而具体的容器运行时，比如 Docker 项目，则一般通过 OCI 这个容器运行时规范同底层的 Linux 操作系统进行交互，即：把 CRI 请求翻译成对 Linux 操作系统的调用（操作 Linux Namespace 和 Cgroups 等）。 此外，kubelet 还通过 gRPC 协议同一个叫作 Device Plugin 的插件进行交互。这个插件，是 Kubernetes 项目用来管理 GPU 等宿主机物理设备的主要组件，也是基于 Kubernetes 项目进行机器学习训练、高性能作业支持等工作必须关注的功能。 而kubelet 的另一个重要功能，则是调用网络插件和存储插件为容器配置网络和持久化存储。这两个插件与 kubelet 进行交互的接口，分别是 CNI（Container Networking Interface）和 CSI（Container Storage Interface）。 实际上，kubelet 这个奇怪的名字，来自于 Borg 项目里的同源组件 Borglet。不过，如果你浏览过 Borg 论文的话，就会发现，这个命名方式可能是 kubelet 组件与 Borglet 组件的唯一相似之处。因为 Borg 项目，并不支持我们这里所讲的容器技术，而只是简单地使用了 Linux Cgroups 对进程进行限制。 这就意味着，像 Docker 这样的“容器镜像”在 Borg 中是不存在的，Borglet 组件也自然不需要像 kubelet 这样考虑如何同 Docker 进行交互、如何对容器镜像进行管理的问题，也不需要支持 CRI、CNI、CSI 等诸多容器技术接口。 可以说，kubelet 完全就是为了实现 Kubernetes 项目对容器的管理能力而重新实现的一个组件，与 Borg 之间并没有直接的传承关系。 备注：虽然不使用 Docker，但 Google 内部确实在使用一个包管理工具，名叫 Midas Package Manager (MPM)，其实它可以部分取代 Docker 镜像的角色。 那么，Borg 对于 Kubernetes 项目的指导作用又体现在哪里呢？ 答案是，Master 节点。 虽然在 Master 节点的实现细节上 Borg 项目与 Kubernetes 项目不尽相同，但它们的出发点却高度一致，即：如何编排、管理、调度用户提交的作业？ 所以，Borg 项目完全可以把 Docker 镜像看做是一种新的应用打包方式。这样，Borg 团队过去在大规模作业管理与编排上的经验就可以直接“套”在 Kubernetes 项目上了。 这些经验最主要的表现就是，从一开始，Kubernetes 项目就没有像同时期的各种“容器云”项目那样，把 Docker 作为整个架构的核心，而仅仅把它作为最底层的一个容器运行时实现。 而 Kubernetes 项目要着重解决的问题，则来自于 Borg 的研究人员在论文中提到的一个非常重要的观点： 运行在大规模集群中的各种任务之间，实际上存在着各种各样的关系。这些关系的处理，才是作业编排和管理系统最困难的地方。 事实也正是如此。 其实，这种任务与任务之间的关系，在我们平常的各种技术场景中随处可见。比如，一个 Web 应用与数据库之间的访问关系，一个负载均衡器和它的后端服务之间的代理关系，一个门户应用与授权组件之间的调用关系。 更进一步地说，同属于一个服务单位的不同功能之间，也完全可能存在这样的关系。比如，一个 Web 应用与日志搜集组件之间的文件交换关系。 而在容器技术普及之前，传统虚拟机环境对这种关系的处理方法都是比较“粗粒度”的。你会经常发现很多功能并不相关的应用被一股脑儿地部署在同一台虚拟机中，只是因为它们之间偶尔会互相发起几个 HTTP 请求。 更常见的情况则是，一个应用被部署在虚拟机里之后，你还得手动维护很多跟它协作的守护进程（Daemon），用来处理它的日志搜集、灾难恢复、数据备份等辅助工作。 但容器技术出现以后，你就不难发现，在“功能单位”的划分上，容器有着独一无二的“细粒度”优势：毕竟容器的本质，只是一个进程而已。 也就是说，只要你愿意，那些原先拥挤在同一个虚拟机里的各个应用、组件、守护进程，都可以被分别做成镜像，然后运行在一个个专属的容器中。它们之间互不干涉，拥有各自的资源配额，可以被调度在整个集群里的任何一台机器上。而这，正是一个 PaaS 系统最理想的工作状态，也是所谓“微服务”思想得以落地的先决条件。 当然，如果只做到“封装微服务、调度单容器”这一层次，Docker Swarm 项目就已经绰绰有余了。如果再加上 Compose 项目，你甚至还具备了处理一些简单依赖关系的能力，比如：一个“Web 容器”和它要访问的数据库“DB 容器”。 在 Compose 项目中，你可以为这样的两个容器定义一个“link”，而 Docker 项目则会负责维护这个“link”关系，其具体做法是：Docker 会在 Web 容器中，将 DB 容器的 IP 地址、端口等信息以环境变量的方式注入进去，供应用进程使用，比如： 123456DB_NAME=/web/db DB_PORT=tcp://172.17.0.5:5432 DB_PORT_5432_TCP=tcp://172.17.0.5:5432 DB_PORT_5432_TCP_PROTO=tcp DB_PORT_5432_TCP_PORT=5432 DB_PORT_5432_TCP_ADDR=172.17.0.5 而当 DB 容器发生变化时（比如，镜像更新，被迁移到其他宿主机上等等），这些环境变量的值会由 Docker 项目自动更新。这就是平台项目自动地处理容器间关系的典型例子。 可是，如果我们现在的需求是，要求这个项目能够处理前面提到的所有类型的关系，甚至还要能够支持未来可能出现的更多种类的关系呢？ 这时，“link”这种单独针对一种案例设计的解决方案就太过简单了。如果你做过架构方面的工作，就会深有感触：一旦要追求项目的普适性，那就一定要从顶层开始做好设计。 所以，Kubernetes 项目最主要的设计思想是，从更宏观的角度，以统一的方式来定义任务之间的各种关系，并且为将来支持更多种类的关系留有余地。 比如，Kubernetes 项目对容器间的“访问”进行了分类，首先总结出了一类非常常见的“紧密交互”的关系，即：这些应用之间需要非常频繁的交互和访问；又或者，它们会直接通过本地文件进行信息交换。 在常规环境下，这些应用往往会被直接部署在同一台机器上，通过 Localhost 通信，通过本地磁盘目录交换文件。而在 Kubernetes 项目中，这些容器则会被划分为一个“Pod”，Pod 里的容器共享同一个 Network Namespace、同一组数据卷，从而达到高效率交换信息的目的。 Pod 是 Kubernetes 项目中最基础的一个对象，源自于 Google Borg 论文中一个名叫 Alloc 的设计。 而对于另外一种更为常见的需求，比如 Web 应用与数据库之间的访问关系，Kubernetes 项目则提供了一种叫作“Service”的服务。像这样的两个应用，往往故意不部署在同一台机器上，这样即使 Web 应用所在的机器宕机了，数据库也完全不受影响。可是，我们知道，对于一个容器来说，它的 IP 地址等信息不是固定的，那么 Web 应用又怎么找到数据库容器的 Pod 呢？ 所以，Kubernetes 项目的做法是给 Pod 绑定一个 Service 服务，而 Service 服务声明的 IP 地址等信息是“终生不变”的。这个Service 服务的主要作用，就是作为 Pod 的代理入口（Portal），从而代替 Pod 对外暴露一个固定的网络地址。 这样，对于 Web 应用的 Pod 来说，它需要关心的就是数据库 Pod 的 Service 信息。不难想象，Service 后端真正代理的 Pod 的 IP 地址、端口等信息的自动更新、维护，则是 Kubernetes 项目的职责。 像这样，围绕着容器和 Pod 不断向真实的技术场景扩展，我们就能够摸索出一幅如下所示的 Kubernetes 项目核心功能的“全景图”。 按照这幅图的线索，我们从容器这个最基础的概念出发，首先遇到了容器间“紧密协作”关系的难题，于是就扩展到了 Pod；有了 Pod 之后，我们希望能一次启动多个应用的实例，这样就需要 Deployment 这个 Pod 的多实例管理器；而有了这样一组相同的 Pod 后，我们又需要通过一个固定的 IP 地址和端口以负载均衡的方式访问它，于是就有了 Service。 可是，如果现在两个不同 Pod 之间不仅有“访问关系”，还要求在发起时加上授权信息。最典型的例子就是 Web 应用对数据库访问时需要 Credential（数据库的用户名和密码）信息。那么，在 Kubernetes 中这样的关系又如何处理呢？ Kubernetes 项目提供了一种叫作 Secret 的对象，它其实是一个保存在 Etcd 里的键值对数据。这样，你把 Credential 信息以 Secret 的方式存在 Etcd 里，Kubernetes 就会在你指定的 Pod（比如，Web 应用的 Pod）启动时，自动把 Secret 里的数据以 Volume 的方式挂载到容器里。这样，这个 Web 应用就可以访问数据库了。 除了应用与应用之间的关系外，应用运行的形态是影响“如何容器化这个应用”的第二个重要因素。 为此，Kubernetes 定义了新的、基于 Pod 改进后的对象。比如 Job，用来描述一次性运行的 Pod（比如，大数据任务）；再比如 DaemonSet，用来描述每个宿主机上必须且只能运行一个副本的守护进程服务；又比如 CronJob，则用于描述定时任务等等。 如此种种，正是 Kubernetes 项目定义容器间关系和形态的主要方法。 可以看到，Kubernetes 项目并没有像其他项目那样，为每一个管理功能创建一个指令，然后在项目中实现其中的逻辑。这种做法，的确可以解决当前的问题，但是在更多的问题来临之后，往往会力不从心。 相比之下，在 Kubernetes 项目中，我们所推崇的使用方法是： 首先，通过一个“编排对象”，比如 Pod、Job、CronJob 等，来描述你试图管理的应用； 然后，再为它定义一些“服务对象”，比如 Service、Secret、Horizontal Pod Autoscaler（自动水平扩展器）等。这些对象，会负责具体的平台级功能。 这种使用方法，就是所谓的“声明式 API”。这种 API 对应的“编排对象”和“服务对象”，都是 Kubernetes 项目中的 API 对象（API Object）。 这就是 Kubernetes 最核心的设计理念，也是接下来我会重点剖析的关键技术点。 最后，我来回答一个更直接的问题：Kubernetes 项目如何启动一个容器化任务呢？ 比如，我现在已经制作好了一个 Nginx 容器镜像，希望让平台帮我启动这个镜像。并且，我要求平台帮我运行两个完全相同的 Nginx 副本，以负载均衡的方式共同对外提供服务。 如果是自己 DIY 的话，可能需要启动两台虚拟机，分别安装两个 Nginx，然后使用 keepalived 为这两个虚拟机做一个虚拟 IP。 而如果使用 Kubernetes 项目呢？你需要做的则是编写如下这样一个 YAML 文件（比如名叫 nginx-deployment.yaml）： 123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 在上面这个 YAML 文件中，我们定义了一个 Deployment 对象，它的主体部分（spec.template 部分）是一个使用 Nginx 镜像的 Pod，而这个 Pod 的副本数是 2（replicas=2）。 然后执行： 1$ kubectl create -f nginx-deployment.yaml 这样，两个完全相同的 Nginx 容器副本就被启动了。 实际上，过去很多的集群管理项目（比如 Yarn、Mesos，以及 Swarm）所擅长的，都是把一个容器，按照某种规则，放置在某个最佳节点上运行起来。这种功能，我们称为“调度”。 而 Kubernetes 项目所擅长的，是按照用户的意愿和整个系统的规则，完全自动化地处理好容器之间的各种关系。这种功能，就是我们经常听到的一个概念：编排。 所以说，Kubernetes 项目的本质，是为用户提供一个具有普遍意义的容器编排工具。 不过，更重要的是，Kubernetes 项目为用户提供的不仅限于一个工具。它真正的价值，乃在于提供了一套基于容器构建分布式系统的基础依赖。 &nbsp;","categories":[{"name":"云原生","slug":"云原生","permalink":"http://wht6.github.io/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://wht6.github.io/tags/Docker/"},{"name":"容器","slug":"容器","permalink":"http://wht6.github.io/tags/%E5%AE%B9%E5%99%A8/"}]},{"title":"SQL语法基础","slug":"SQL语法基础","date":"2022-02-18T08:00:00.000Z","updated":"2022-04-20T13:48:18.676Z","comments":true,"path":"posts/d569.html","link":"","permalink":"http://wht6.github.io/posts/d569.html","excerpt":"","text":"为什么需要数据库？ 数据库面向软件，数据库支持事务，数据库有SQL语言，数据库可扩展。 SQL执行过程SQL 在 Oracle 中的执行过程 QL 语句在 Oracle 中经历了以下的几个步骤。 语法检查：检查 SQL 拼写是否正确，如果不正确，Oracle 会报语法错误。 语义检查：检查 SQL 中的访问对象是否存在。比如我们在写 SELECT 语句的时候，列名写错了，系统就会提示错误。语法检查和语义检查的作用是保证 SQL 语句没有错误。 权限检查：看用户是否具备访问该数据的权限。 共享池检查：共享池（Shared Pool）是一块内存池，最主要的作用是缓存 SQL 语句和该语句的执行计划。Oracle 通过检查共享池是否存在 SQL 语句的执行计划，来判断进行软解析，还是硬解析。那软解析和硬解析又该怎么理解呢？ 在共享池中，Oracle 首先对 SQL 语句进行 Hash 运算，然后根据 Hash 值在库缓存（Library Cache）中查找，如果存在 SQL 语句的执行计划，就直接拿来执行，直接进入“执行器”的环节，这就是软解析。 如果没有找到 SQL 语句和执行计划，Oracle 就需要创建解析树进行解析，生成执行计划，进入“优化器”这个步骤，这就是硬解析。 优化器：优化器中就是要进行硬解析，也就是决定怎么做，比如创建解析树，生成执行计划。 执行器：当有了解析树和执行计划之后，就知道了 SQL 该怎么被执行，这样就可以在执行器中执行语句了。 共享池是 Oracle 中的术语，包括了库缓存，数据字典缓冲区等。库缓存区，它主要缓存 SQL 语句和执行计划。而数据字典缓冲区存储的是 Oracle 中的对象定义，比如表、视图、索引等对象。当对 SQL 语句进行解析的时候，如果需要相关的数据，会从数据字典缓冲区中提取。 库缓存这一个步骤，决定了 SQL 语句是否需要进行硬解析。为了提升 SQL 的执行效率，我们应该尽量避免硬解析，因为在 SQL 的执行过程中，创建解析树，生成执行计划是很消耗资源的。 如何避免硬解析，尽量使用软解析呢？在 Oracle 中，绑定变量是它的一大特色。绑定变量就是在 SQL 语句中使用变量，通过不同的变量取值来改变 SQL 的执行结果。这样做的好处是能提升软解析的可能性，不足之处在于可能会导致生成的执行计划不够优化，因此是否需要绑定变量还需要视情况而定。 SQL 在 MySQL中的执行过程首先 MySQL 是典型的 C/S 架构，即 Client/Server 架构，服务器端程序使用的 mysqld。整体的 MySQL 流程如下图所示： MySQL 由三层组成： 连接层：客户端和服务器端建立连接，客户端发送 SQL 至服务器端； SQL 层：对 SQL 语句进行查询处理； 存储引擎层：与数据库文件打交道，负责数据的存储和读取。 其中 SQL 层与数据库文件的存储方式无关，我们来看下 SQL 层的结构： 查询缓存：Server 如果在查询缓存中发现了这条 SQL 语句，就会直接将结果返回给客户端；如果没有，就进入到解析器阶段。需要说明的是，因为查询缓存往往效率不高，所以在 MySQL8.0 之后就抛弃了这个功能。 解析器：在解析器中对 SQL 语句进行语法分析、语义分析。 优化器：在优化器中会确定 SQL 语句的执行路径，比如是根据全表检索，还是根据索引来检索等。 执行器：在执行之前需要判断该用户是否具备权限，如果具备权限就执行 SQL 查询并返回结果。在 MySQL8.0 以下的版本，如果设置了查询缓存，这时会将查询结果进行缓存。 你能看到 SQL 语句在 MySQL 中的流程是：SQL 语句→缓存查询→解析器→优化器→执行器。在一部分中，MySQL 和 Oracle 执行 SQL 的原理是一样的。 与 Oracle 不同的是，MySQL 的存储引擎采用了插件的形式，每个存储引擎都面向一种特定的数据库应用环境。同时开源的 MySQL 还允许开发人员设置自己的存储引擎，下面是一些常见的存储引擎： InnoDB 存储引擎：它是 MySQL 5.5 版本之后默认的存储引擎，最大的特点是支持事务、行级锁定、外键约束等。 MyISAM 存储引擎：在 MySQL 5.5 版本之前是默认的存储引擎，不支持事务，也不支持外键，最大的特点是速度快，占用资源少。 Memory 存储引擎：使用系统内存作为存储介质，以便得到更快的响应速度。不过如果 mysqld 进程崩溃，则会导致所有的数据丢失，因此我们只有当数据是临时的情况下才使用 Memory 存储引擎。 NDB 存储引擎：也叫做 NDB Cluster 存储引擎，主要用于 MySQL Cluster 分布式集群环境，类似于 Oracle 的 RAC 集群。 Archive 存储引擎：它有很好的压缩机制，用于文件归档，在请求写入时会进行压缩，所以也经常用来做仓库。但是查询非常慢。 需要注意的是，数据库的设计在于表的设计，而在 MySQL 中每个表的设计都可以采用不同的存储引擎，我们可以根据实际的数据处理需要来选择存储引擎，这也是 MySQL 的强大之处。 知道了SQL语句的执行过程之后，再看MySQL的软件架构： 从图上可以看出，MySQL Server就是客户端到文件系统的一个中介，它的作用就是将客户端的指令落实到操作系统的文件上。写数据的时候，MySQL Server将客户端发来的数据存到操作系统的文件里，客户端要查询数据的时候，MySQL Server再将文件中的数据取出来，返回到客户端。 存储引擎就是将执行器的物理执行计划转化为对系统文件的读和写。 TCP/IP协议是MySQL最常用的连接方式。 缓存：执行过的语句会以KV的形式缓存在内存中，K就是查询语句，V就是查询结果。但是问题是数据表修改之后（意味着之前的查询结果失效了），会删除所有相关缓存。对于一个在线的业务，其表的更新速率是非常高的，所以这种缓存很容易失效，使用它反而会降低性能。 执行器：首先校验用户对目标数据有无权限，执行器会以行为粒度调用存储引擎执行SQL。在没有索引的情况下，执行器会循环查询所有行。 存储引擎的任务是将执行器的指令落实在数据文件上。不同的存储引擎的原理和执行方法有很大不同。 在MySQL中查看profile创建数据库，表，并插入一条数据： 1234567891011121314create database db_example;show databases;USE db_example;create table tutorials_tbl( tutorial_id INT NOT NULL AUTO_INCREMENT, tutorial_title VARCHAR(100) NOT NULL, tutorial_author VARCHAR(40) NOT NULL, submission_date DATE, PRIMARY KEY ( tutorial_id ));INSERT INTO tutorials_tbl (tutorial_title, tutorial_author, submission_date) VALUES (&quot;Learn PHP&quot;, &quot;John Poul&quot;, NOW()); 开启profiling可以让 MySQL 收集在 SQL 执行时所使用的资源情况，命令如下： 12345678910111213141516171819202122232425262728select @@profiling;set profiling=1; # 开启profilingselect * from tutorials_tbl; # 执行一个查询show profiles; # 查看所有 profileshow profile for query 9; # 查看查询9的profile+----------------------+----------+| Status | Duration |+----------------------+----------+| starting | 0.000123 | | checking permissions | 0.000010 | 权限检查| Opening tables | 0.000016 | 打开表 | init | 0.000038 | 初始化| System lock | 0.000009 | 系统锁| optimizing | 0.000004 | 优化查询| statistics | 0.000032 || preparing | 0.000012 | 准备| executing | 0.000003 | 执行| Sending data | 0.000102 || end | 0.000007 || query end | 0.000006 || closing tables | 0.000014 || freeing items | 0.000011 || cleaning up | 0.000011 |+----------------------+----------+select version(); # 查看MySQL的版本 DDL的使用DDL 的英文全称是 Data Definition Language，中文是数据定义语言。它定义了数据库的结构和数据表的结构。 在 DDL 中，我们常用的功能是增删改，分别对应的命令是 CREATE、DROP 和 ALTER。需要注意的是，在执行 DDL 的时候，不需要 COMMIT，就可以完成执行任务。 对数据库进行定义 12CREATE DATABASE nba; // 创建一个名为 nba 的数据库DROP DATABASE nba; // 删除一个名为 nba 的数据库 对数据表进行定义 1CREATE TABLE table_name 创建表结构比如我们想创建一个球员表，表名为 player，里面有两个字段，一个是 player_id，它是 int 类型，另一个 player_name 字段是varchar(255)类型。这两个字段都不为空，且 player_id 是递增的。 那么创建的时候就可以写为： 1234CREATE TABLE player ( player_id int(11) NOT NULL AUTO_INCREMENT, player_name varchar(255) NOT NULL); 需要注意的是，语句最后以分号（;）作为结束符，最后一个字段的定义结束后没有逗号。数据类型中 int(11) 代表整数类型，显示长度为 11 位，括号中的参数 11 代表的是最大有效显示长度，与类型包含的数值范围大小无关。varchar(255)代表的是最大长度为 255 的可变字符串类型。NOT NULL表明整个字段不能是空值，是一种数据约束。AUTO_INCREMENT代表主键自动增长。 实际上，我们通常很少自己写 DDL 语句，可以使用一些可视化工具来创建和操作数据库和数据表。推荐使用 Navicat，它是一个数据库管理和设计工具，跨平台，支持很多种数据库管理软件，比如 MySQL、Oracle、MariaDB 等。 下面使用navicat来创建数据表： 我们还可以对 player_name 字段进行索引，索引类型为Unique。使用 Navicat 设置如下： 这样一张 player 表就通过可视化工具设计好了。我们可以把这张表导出来，可以看看这张表对应的 SQL 语句是怎样的。方法是在 Navicat 左侧用右键选中 player 这张表，然后选择“转储 SQL 文件”→“仅结构”，这样就可以看到导出的 SQL 文件了，代码如下： 123456789DROP TABLE IF EXISTS `player`;CREATE TABLE `player` ( `player_id` int(11) NOT NULL AUTO_INCREMENT, `team_id` int(11) NOT NULL, `player_name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL, `height` float(3, 2) NULL DEFAULT 0.00, PRIMARY KEY (`player_id`) USING BTREE, UNIQUE INDEX `player_name`(`player_name`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic; 你能看到整个 SQL 文件中的 DDL 处理，首先先删除 player 表（如果数据库中存在该表的话），然后再创建 player 表，里面的数据表和字段都使用了反引号，这是为了避免它们的名称与 MySQL 保留字段相同，对数据表和字段名称都加上了反引号。 其中 player_name 字段的字符集是 utf8，排序规则是utf8_general_ci，代表对大小写不敏感，如果设置为utf8_bin，代表对大小写敏感，还有许多其他排序规则这里不进行介绍。 因为 player_id 设置为了主键，因此在 DDL 中使用PRIMARY KEY进行规定，同时索引方法采用 BTREE。 因为我们对 player_name 字段进行索引，在设置字段索引时，我们可以设置为UNIQUE INDEX（唯一索引），也可以设置为其他索引方式，比如NORMAL INDEX（普通索引），这里我们采用UNIQUE INDEX。唯一索引和普通索引的区别在于它对字段进行了唯一性的约束。在索引方式上，你可以选择BTREE或者HASH，这里采用了BTREE方法进行索引。 整个数据表的存储规则采用 InnoDB。之前我们简单介绍过 InnoDB，它是 MySQL5.5 版本之后默认的存储引擎。同时，我们将字符集设置为 utf8，排序规则为utf8_general_ci，行格式为Dynamic，就可以定义数据表的最后约定了： 1ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic; 修改表结构添加字段，比如我在数据表中添加一个 age 字段，类型为int(11) 1ALTER TABLE player ADD (age int(11)); 修改字段名，将 age 字段改成player_age 1ALTER TABLE player RENAME COLUMN age to player_age 修改字段的数据类型，将player_age的数据类型设置为float(3,1) 1ALTER TABLE player MODIFY (player_age float(3,1)); 删除字段, 删除刚才添加的player_age字段 1ALTER TABLE player DROP COLUMN player_age; 数据表的常见约束当我们创建数据表的时候，还会对字段进行约束，约束的目的在于保证 RDBMS 里面数据的准确性和一致性。下面，我们来看下常见的约束有哪些。 首先是主键约束。 主键起的作用是唯一标识一条记录，不能重复，不能为空，即 UNIQUE+NOT NULL。一个数据表的主键只能有一个。主键可以是一个字段，也可以由多个字段复合组成。在上面的例子中，我们就把 player_id 设置为了主键。 其次还有外键约束。 外键确保了表与表之间引用的完整性。一个表中的外键对应另一张表的主键。外键可以是重复的，也可以为空。比如 player_id 在 player 表中是主键，如果你想设置一个球员比分表即 player_score，就可以在 player_score 中设置 player_id 为外键，关联到 player 表中。 除了对键进行约束外，还有字段约束。 唯一性约束。 唯一性约束表明了字段在表中的数值是唯一的，即使我们已经有了主键，还可以对其他字段进行唯一性约束。比如我们在 player 表中给 player_name 设置唯一性约束，就表明任何两个球员的姓名不能相同。需要注意的是，唯一性约束和普通索引（NORMAL INDEX）之间是有区别的。唯一性约束相当于创建了一个约束和普通索引，目的是保证字段的正确性，而普通索引只是提升数据检索的速度，并不对字段的唯一性进行约束。 NOT NULL 约束。对字段定义了 NOT NULL，即表明该字段不应为空，必须有取值。 DEFAULT，表明了字段的默认值。如果在插入数据的时候，这个字段没有取值，就设置为默认值。比如我们将身高 height 字段的取值默认设置为 0.00，即DEFAULT 0.00。 CHECK 约束，用来检查特定字段取值范围的有效性，CHECK 约束的结果不能为 FALSE，比如我们可以对身高 height 的数值进行 CHECK 约束，必须≥0，且＜3，即CHECK(height&gt;=0 AND height&lt;3)。 设计数据表的原则我们在设计数据表的时候，经常会考虑到各种问题，比如：用户都需要什么数据？需要在数据表中保存哪些数据？哪些数据是经常访问的数据？如何提升检索效率？ 如何保证数据表中数据的正确性，当插入、删除、更新的时候该进行怎样的约束检查？ 如何降低数据表的数据冗余度，保证数据表不会因为用户量的增长而迅速扩张？ 如何让负责数据库维护的人员更方便地使用数据库？ 除此以外，我们使用数据库的应用场景也各不相同，可以说针对不同的情况，设计出来的数据表可能千差万别。那么有没有一种设计原则可以让我们来借鉴呢？这里我整理了一个“三少一多”原则： 1.数据表的个数越少越好 RDBMS 的核心在于对实体和联系的定义，也就是 E-R 图（Entity Relationship Diagram），数据表越少，证明实体和联系设计得越简洁，既方便理解又方便操作。 2.数据表中的字段个数越少越好 字段个数越多，数据冗余的可能性越大。设置字段个数少的前提是各个字段相互独立，而不是某个字段的取值可以由其他字段计算出来。当然字段个数少是相对的，我们通常会在数据冗余和检索效率中进行平衡。 3.数据表中联合主键的字段个数越少越好 设置主键是为了确定唯一性，当一个字段无法确定唯一性的时候，就需要采用联合主键的方式（也就是用多个字段来定义一个主键）。联合主键中的字段越多，占用的索引空间越大，不仅会加大理解难度，还会增加运行时间和索引空间，因此联合主键的字段个数越少越好。 4.使用主键和外键越多越好 数据库的设计实际上就是定义各种表，以及各种字段之间的关系。这些关系越多，证明这些实体之间的冗余度越低，利用度越高。这样做的好处在于不仅保证了数据表之间的独立性，还能提升相互之间的关联使用率。 你应该能看出来“三少一多”原则的核心就是简单可复用。简单指的是用更少的表、更少的字段、更少的联合主键字段来完成数据表的设计。可复用则是通过主键、外键的使用来增强数据表之间的复用率。因为一个主键可以理解是一张表的代表。键设计得越多，证明它们之间的利用率越高。 问题：外键真的越多越好吗？ 首先，外键本身是为了实现强一致性，所以如果需要正确性&gt;性能的话，还是建议使用外键，它可以让我们在数据库的层面保证数据的完整性和一致性。当然不用外键，你也可以在业务层进行实现。不过，这样做也同样存在一定的风险，因为这样，就会让业务逻辑会与数据具备一定的耦合性。也就是业务逻辑和数据必须同时修改。而且在工作中，业务层可能会经常发生变化。 当然，很多互联网的公司，尤其是超大型的数据应用场景，大量的插入，更新和删除在外键的约束下会降低性能，同时数据库在水平拆分和分库的情况下，数据库端也做不到执行外键约束。另外，在高并发的情况下，外键的存在也会造成额外的开销。因为每次更新数据，都需要检查另外一张表的数据，也容易造成死锁。所以在这种情况下，尤其是大型项目中后期，可以采用业务层来实现，取消外键提高效率。不过在SQL学习之初，包括在系统最初设计的时候，还是建议你采用规范的数据库设计，也就是采用外键来对数据表进行约束。因为这样可以建立一个强一致性，可靠性高的数据库结构，也不需要在业务层来实现过多的检查。当然在项目后期，业务量增大的情况下，你需要更多考虑到数据库性能问题，可以取消外键的约束，转移到业务层来实现。而且在大型互联网项目中，考虑到分库分表的情况，也会降低外键的使用。不过在SQL学习，以及项目早期，还是建议你使用外键。在项目后期，你可以分析有哪些外键造成了过多的性能消耗。一般遵循2/8原则，会有20%的外键造成80%的资源效率，你可以只把这20%的外键进行开放，采用业务层逻辑来进行实现，当然你需要保证业务层的实现没有错误。不同阶段，考虑的问题不同。当用户和业务量增大的时候，对于大型互联网应用，也会通过减少外键的使用，来减低死锁发生的概率，提高并发处理能力。 检索数据基本用法SELECT 可以帮助我们从一个表或多个表中进行数据查询。 起别名 1SELECT name AS n, hp_max AS hm, mp_max AS mm, attack_max AS am, defense_max AS dm FROM heros; 起别名只是在查询结构上生效，不改变实际表。 增加常数列 1SELECT &#x27;kinghonor&#x27; as platform, name FROM heros 这里就增加了一列platform，列的值都是固定值kinghonor。（字符串需要加单引号，数字不需要） 去除重复行 1SELECT DISTINCT attack_range FROM heros DISTINCT需要放到所有列名的前面，并且去重是针对一整行，而不是针对某个字段。 对检索排序 ORDER BY使用要点： 排序的列名：ORDER BY 后面可以有一个或多个列名，如果是多个列名进行排序，会按照后面第一个列先进行排序，当第一列的值相同的时候，再按照第二列进行排序，以此类推。 排序的顺序：ORDER BY 后面可以注明排序规则，ASC 代表递增排序，DESC 代表递减排序。如果没有注明排序规则，默认情况下是按照 ASC 递增排序。我们很容易理解 ORDER BY 对数值类型字段的排序规则，但如果排序字段类型为文本数据，就需要参考数据库的设置方式了，这样才能判断 A 是在 B 之前，还是在 B 之后。比如使用 MySQL 在创建字段的时候设置为 BINARY 属性，就代表区分大小写。 非选择列排序：ORDER BY 可以使用非选择列进行排序，所以即使在 SELECT 后面没有这个列名，你同样可以放到 ORDER BY 后面进行排序。 ORDER BY 的位置：ORDER BY 通常位于 SELECT 语句的最后一条子句，否则会报错。 1234SELECT name, hp_max FROM heros ORDER BY hp_max DESC SELECT name, hp_max FROM heros ORDER BY mp_max, hp_max DESC # 当mp_max相同时，按照hp_max递减排序 约束返回结果的数量 1SELECT name, hp_max FROM heros ORDER BY hp_max DESC LIMIT 5 在 MySQL、PostgreSQL、MariaDB 和 SQLite 中使用 LIMIT 关键字，而且需要放到 SELECT 语句的最后面。 执行顺序关键词顺序： 1SELECT ... FROM ... WHERE ... GROUP BY ... HAVING ... ORDER BY ... 这个顺序是指在写SQL语句的时候，要保证的顺序，否则会报语法错误。 SELECT 语句的执行顺序（在 MySQL 和 Oracle 中，SELECT 执行顺序基本相同）： 1FROM &gt; WHERE &gt; GROUP BY &gt; 聚集函数 &gt; HAVING &gt; 计算所有表达式 &gt; SELECT 的字段 &gt; DISTINCT &gt; ORDER BY &gt; LIMIT 首先，SELECT 是先执行 FROM 这一步的。在这个阶段，如果是多张表联查，还会经历下面的几个步骤： 首先先通过 CROSS JOIN 求笛卡尔积，相当于得到虚拟表 vt（virtual table）1-1； 通过 ON 进行筛选，在虚拟表 vt1-1 的基础上进行筛选，得到虚拟表 vt1-2； 添加外部行。如果我们使用的是左连接、右链接或者全连接，就会涉及到外部行，也就是在虚拟表 vt1-2 的基础上增加外部行，得到虚拟表 vt1-3。 当然如果我们操作的是两张以上的表，还会重复上面的步骤，直到所有表都被处理完为止。这个过程得到是我们的原始数据。 当我们拿到了查询数据表的原始数据，也就是最终的虚拟表 vt1，就可以在此基础上再进行 WHERE 阶段。在这个阶段中，会根据 vt1 表的结果进行筛选过滤，得到虚拟表 vt2。 然后进入第三步和第四步，也就是 GROUP 和 HAVING 阶段。在这个阶段中，实际上是在虚拟表 vt2 的基础上进行分组和分组过滤，得到中间的虚拟表 vt3 和 vt4。 当我们完成了条件筛选部分之后，就可以筛选表中提取的字段，也就是进入到 SELECT 和 DISTINCT 阶段。 首先在 SELECT 阶段会提取想要的字段，然后在 DISTINCT 阶段过滤掉重复的行，分别得到中间的虚拟表 vt5-1 和 vt5-2。 当我们提取了想要的字段数据之后，就可以按照指定的字段进行排序，也就是 ORDER BY 阶段，得到虚拟表 vt6。 最后在 vt6 的基础上，取出指定行的记录，也就是 LIMIT 阶段，得到最终的结果，对应的是虚拟表 vt7。 当然我们在写 SELECT 语句的时候，不一定存在所有的关键字，相应的阶段就会省略。 where筛选123456SELECT name, hp_max FROM heros WHERE hp_max &gt; 6000SELECT name, hp_max FROM heros WHERE hp_max BETWEEN 5399 AND 6811# 筛选hp_max在[5399,6811]范围的记录SELECT name, hp_max FROM heros WHERE hp_max IS NULL 逻辑运算符： 123SELECT name, hp_max, mp_max FROM heros WHERE hp_max &gt; 6000 AND mp_max &gt; 1700 ORDER BY (hp_max+mp_max) DESCSELECT name, hp_max, mp_max FROM heros WHERE (hp_max+mp_max) &gt; 8000 OR hp_max &gt; 6000 AND mp_max &gt; 1700 ORDER BY (hp_max+mp_max) DESC 当 WHERE 子句中同时存在 OR 和 AND 的时候，AND 执行的优先级会更高，也就是说 SQL 会优先处理 AND 操作符，然后再处理 OR 操作符。 12345SELECT name, role_main, role_assist, hp_max, mp_max, birthdateFROM heros WHERE (role_main IN (&#x27;法师&#x27;, &#x27;射手&#x27;) OR role_assist IN (&#x27;法师&#x27;, &#x27;射手&#x27;)) AND DATE(birthdate) NOT BETWEEN &#x27;2016-01-01&#x27; AND &#x27;2017-01-01&#x27;ORDER BY (hp_max + mp_max) DESC 你能看到我把 WHERE 子句分成了两个部分。第一部分是关于主要定位和次要定位的条件过滤，使用的是role_main in (&#39;法师&#39;, &#39;射手&#39;) OR role_assist in (&#39;法师&#39;, &#39;射手&#39;)。这里用到了 IN 逻辑运算符，同时role_main和role_assist是 OR（或）的关系。 第二部分是关于上线时间的条件过滤。NOT 代表否，因为我们要找到不在 2016-01-01 到 2017-01-01 之间的日期，因此用到了NOT BETWEEN &#39;2016-01-01&#39; AND &#39;2017-01-01&#39;。同时我们是在对日期类型数据进行检索，所以使用到了 DATE 函数，将字段 birthdate 转化为日期类型再进行比较。 通配符 %表示任何字符出现任意次数，_表示单个字符。 12SELECT name FROM heros WHERE name LIKE &#x27;% 太 %&#x27;SELECT name FROM heros WHERE name LIKE &#x27;_% 太 %&#x27; 通配符可以让我们对文本类型的字段进行模糊查询，不过检索的代价也是很高的，通常都需要用到全表扫描，所以效率很低。 SQL函数算术函数ABS()：取绝对值。MOD()：取余。ROUND()：指定保留小数点位数。 SELECT ABS(-2)，运行结果为 2。 SELECT MOD(101,3)，运行结果 2。 SELECT ROUND(37.25,1)，运行结果 37.3。 字符串函数拼接：SELECT CONCAT(&#39;abc&#39;, 123)，运行结果为 abc123。 计算长度（汉字算三个字符）：SELECT LENGTH(&#39;你好&#39;)，运行结果为 6。 计算长度（汉字算一个字符）：SELECT CHAR_LENGTH(&#39;你好&#39;)，运行结果为 2。 转小写：SELECT LOWER(&#39;ABC&#39;)，运行结果为 abc。 转大写：SELECT UPPER(&#39;abc&#39;)，运行结果 ABC。 替换：SELECT REPLACE(&#39;fabcd&#39;, &#39;abc&#39;, 123)，运行结果为 f123d。 截取：SELECT SUBSTRING(&#39;fabcd&#39;, 1,3)，运行结果为 fab。 日期函数系统当前日期：SELECT CURRENT_DATE()，运行结果为 2022-02-18。 系统当前时间：SELECT CURRENT_TIME()，运行结果为 21:26:34。 系统当前时间（包括日期）：SELECT CURRENT_TIMESTAMP()，运行结果为 2022-02-18 21:26:34。 获取年或月或日：SELECT EXTRACT(YEAR FROM &#39;2022-02-18&#39;)，运行结果为 2022。 获取日期：SELECT DATE(&#39;2022-02-18 12:00:05&#39;)，运行结果为 2022-04-01。 获取年：SELECT YEAR(&#39;2022-02-18 12:00:05&#39;)，运行结果为 2022。 这里需要注意的是，DATE 日期格式必须是 yyyy-mm-dd 的形式。如果要进行日期比较，就要使用 DATE 函数，不要直接使用日期与字符串进行比较。 转换函数SELECT CAST(123.123 AS INT)，运行结果会报错。 SELECT CAST(123.123 AS DECIMAL(8,2))，运行结果为 123.12。 CAST 函数在转换数据类型的时候，不会四舍五入，如果原数值有小数，那么转换为整数类型的时候就会报错。不过你可以指定转化的小数类型，在 MySQL 和 SQL Server 中，你可以用DECIMAL(a,b)来指定，其中 a 代表整数部分和小数部分加起来最大的位数，b 代表小数位数，比如DECIMAL(8,2)代表的是精度为 8 位（整数加小数位数最多为 8 位），小数位数为 2 位的数据类型。所以SELECT CAST(123.123 AS DECIMAL(8,2))的转换结果为 123.12。 聚集函数聚集函数是对一组数据进行汇总的函数，输入的是一组数据的集合，输出的是单个值。 123# 计算总行数SELECT COUNT(*) FROM heros WHERE hp_max &gt; 6000SELECT COUNT(role_assist) FROM heros WHERE hp_max &gt; 6000 有的记录 role_assist 为 NULL，这时COUNT(role_assist)会忽略值为 NULL 的数据行，而 COUNT(*) 只是统计数据行数，不管某个字段是否为 NULL。 12345678910# MAX()取最大值SELECT MAX(hp_max) FROM herosSELECT name, hp_max FROM heros WHERE hp_max = (SELECT MAX(hp_max) FROM heros)# 查询最大值所在行的某些字段# AVG()取平均值SELECT AVG(hp_max), AVG(mp_max), MAX(attack_max) FROM heros WHERE DATE(birthdate)&gt;&#x27;2016-10-01&#x27;# MIN()最小值，SUM()求和SELECT COUNT(*), AVG(hp_max), MAX(mp_max), MIN(attack_max), SUM(defense_max) FROM heros WHERE role_main = &#x27;射手&#x27; or role_assist = &#x27;射手&#x27; AVG、MAX、MIN 等聚集函数会自动忽略值为 NULL 的数据行，MAX 和 MIN 函数也可以用于字符串类型数据的统计，如果是英文字母，则按照 A—Z 的顺序排列，越往后，数值越大。如果是汉字则按照全拼拼音进行排列。 1SELECT MIN(CONVERT(name USING gbk)), MAX(CONVERT(name USING gbk)) FROM heros 需要说明的是，我们需要先把 name 字段统一转化为 gbk 类型，使用CONVERT(name USING gbk)，然后再使用 MIN 和 MAX 取最小值和最大值。 大部分 DBMS 会有自己特定的函数，这就意味着采用 SQL 函数的代码可移植性是很差的，因此在使用函数的时候需要特别注意。 命名规范的建议： 关键字和函数名称全部大写； 数据库名、表名、字段名称全部小写； SQL 语句必须以分号结尾。 数据分组我们在做统计的时候，可能需要先对数据按照不同的数值进行分组，然后对这些分好的组进行聚集统计。对数据进行分组，需要使用 GROUP BY 子句。 12SELECT COUNT(*), role_main FROM heros GROUP BY role_main# 按照role_main进行分组，再计算每组的行数 如果分组的字段中存在NULL，那么这些NULL的会分为一组。 12SELECT COUNT(*) as num, role_main, role_assist FROM heros GROUP BY role_main, role_assist ORDER BY num DESC# 使用多个字段进行分组，并按照num排序 使用多个字段进行分组相当于把这些字段可能出现的所有的取值情况都进行分组。 过滤分组 当我们创建出很多分组的时候，有时候就需要对分组进行过滤。你可能首先会想到 WHERE 子句，实际上过滤分组我们使用的是 HAVING。HAVING 的作用和 WHERE 一样，都是起到过滤的作用，只不过 WHERE 是用于数据行，而 HAVING 则作用于分组。 1SELECT COUNT(*) as num, role_main, role_assist FROM heros GROUP BY role_main, role_assist HAVING num &gt; 5 ORDER BY num DESC 首先我们需要获取的是英雄的数量、主要定位和次要定位，即SELECT COUNT(*) as num, role_main, role_assist。然后按照英雄的主要定位和次要定位进行分组，即GROUP BY role_main, role_assist，同时我们要对分组中的英雄数量进行筛选，选择大于 5 的分组，即HAVING num &gt; 5，然后按照英雄数量从高到低进行排序，即ORDER BY num DESC。 HAVING 支持所有 WHERE 的操作，因此所有需要 WHERE 子句实现的功能，你都可以使用 HAVING 对分组进行筛选。 1SELECT COUNT(*) as num, role_main, role_assist FROM heros WHERE hp_max &gt; 6000 GROUP BY role_main, role_assist HAVING num &gt; 5 ORDER BY num DESC 这里先使用 WHERE 子句对最大生命值大于 6000 的英雄进行条件过滤，然后再使用 GROUP BY 进行分组，使用 HAVING 进行分组的条件判断，然后使用 ORDER BY 进行排序。 子查询子查询就是嵌套在查询中的查询。很多时候，我们无法直接从数据表中得到查询结果，需要从查询结果集中再次进行查询，这个“查询结果集”就是资源查询。 非关联子查询：子查询从数据表中查询了数据结果，这个数据结果只执行一次，然后这个数据结果作为主查询的条件进行执行。 关联子查询：子查询需要执行多次，即采用循环的方式，先从外部查询开始，每次都传入子查询进行查询，然后再将结果反馈给外部，即嵌套的执行方式。 12SELECT player_name, height FROM player WHERE height = (SELECT max(height) FROM player)# 非关联子查询 如果子查询的执行依赖于外部查询，通常情况下都是因为子查询中的表用到了外部的表，并进行了条件关联，因此每执行一次外部查询，子查询都要重新计算一次，这样的子查询就称之为关联子查询。比如我们想要查找每个球队中大于平均身高的球员有哪些，并显示他们的球员姓名、身高以及所在球队 ID。 12SELECT player_name, height, team_id FROM player AS a WHERE height &gt; (SELECT avg(height) FROM player AS b WHERE a.team_id = b.team_id)# 关联子查询 关联子查询通常也会和 EXISTS 一起来使用，EXISTS 子查询用来判断条件是否满足，满足的话为 True，不满足为 False。 1SELECT player_id, team_id, player_name FROM player WHERE EXISTS (SELECT player_id FROM player_score WHERE player.player_id = player_score.player_id) 同样，NOT EXISTS 就是不存在的意思，我们也可以通过 NOT EXISTS 查询不存在于 player_score 表中的球员信息。 1SELECT player_id, team_id, player_name FROM player WHERE NOT EXISTS (SELECT player_id FROM player_score WHERE player.player_id = player_score.player_id) 集合比较子查询：与另一个查询结果集进行比较，我们可以在子查询中使用 IN、ANY、ALL 和 SOME 操作符。 IN和EXISTS的含义一样，但是使用方式不一样。 12SELECT player_id, team_id, player_name FROM player WHERE player_id in (SELECT player_id FROM player_score WHERE player.player_id = player_score.player_id)# 用IN子查询代替EXISTS子查询 两者抽象出来的结构如下： 1234SELECT * FROM A WHERE cc IN (SELECT cc FROM B)# 用到了A 表上cc 列的索引SELECT * FROM A WHERE EXISTS (SELECT cc FROM B WHERE B.cc=A.cc)# 用到了B 表上cc 列的索引 IN是把外表和内表作hash 连接，而EXISTS是对外表作loop 循环，每次loop 循环再对内表进行查询。如果两个表中一个较小，一个是大表，则子查询表大的用EXISTS，子查询表小的用IN。 如果查询语句使用了NOT IN那么内外表都进行全表扫描，没有用到索引；而NOT EXISTS的子查询依然能用到表上的索引。所以无论那个表大，用NOT EXISTS都比NOT IN要快。 ANY表示任何一个。 比印第安纳步行者（对应的 team_id 为 1002）中任何一个球员身高高的球员的信息，并且输出他们的球员 ID、球员姓名和球员身高（只要比印第安纳步行者中的其中一个球员高就行）。 1SELECT player_id, player_name, height FROM player WHERE height &gt; ANY (SELECT height FROM player WHERE team_id = 1002) 比印第安纳步行者（对应的 team_id 为 1002）中所有球员身高都高的球员的信息，并且输出球员 ID、球员姓名和球员身高。 1SELECT player_id, player_name, height FROM player WHERE height &gt; ALL (SELECT height FROM player WHERE team_id = 1002) 查询每个球队的球员数，也就是对应 team 这张表，我需要查询相同的 team_id 在 player 这张表中所有的球员数量是多少。 1SELECT team_name, (SELECT count(*) FROM player WHERE player.team_id = team.team_id) AS player_num FROM team 我们还可以将子查询作为计算字段： 1SELECT team_name, (SELECT count(*) FROM player WHERE player.team_id = team.team_id) AS player_num FROM team 表的连接方式SQL92 中有 5 种连接方式，它们分别是笛卡尔积、等值连接、非等值连接、外连接（左连接、右连接）和自连接。 交叉连接交叉连接又叫笛卡尔积。 1234# SQL92写法SELECT * FROM player, team# SQL99写法SELECT * FROM player CROSS JOIN team A表和B表进行笛卡尔积，表示A表中的数据与B表中的数据任意组合。即从表A中取出任意一条记录与表B中任意一条记录组合为一条记录，这些所有的记录就组合成一个新的表，这个表就是表A和表B的笛卡尔积。 等值连接等值连接又叫自然连接。 12345678910# SQL92写法SELECT player_id, player.team_id, player_name, height, team_name FROM player, team WHERE player.team_id = team.team_id# 简洁写法SELECT player_id, a.team_id, player_name, height, team_name FROM player AS a, team AS b WHERE a.team_id = b.team_id# SQL99写法# USING连接SELECT player_id, team_id, player_name, height, team_name FROM player JOIN team USING(team_id)# 自然连接SELECT player_id, team_id, player_name, height, team_name FROM player NATURAL JOIN team 两张表中存在相同的字段，基于相同的字段，将两张表中该字段相等的记录合并成一条记录，这些所有的记录就组合成一个新的表，这个表就是表A和表B的等值连接。 USING连接和自然连接的区别在于，自然连接会对所有相同字段进行连接，而USING连接则可以指定连接的字段。 非等值连接123SELECT p.player_name, p.height, h.height_levelFROM player AS p, height_grades AS hWHERE p.height BETWEEN h.height_lowest AND h.height_highest 非等值连接可以说是对等值连接的一个扩充，等值连接基于字段相等的条件，而如果需要基于字段之间的其他关系进行连接就是非等值连接。此两者都是基于一个字段之间的关系，对于满足这些关系的记录进行连接。 在SQL92标准中对应的是ON连接。ON连接可以看做是条件连接，这个条件不论是等值条件还是非等值条件都可以。 12345# ON连接（等值条件）SELECT player_id, player.team_id, player_name, height, team_name FROM player JOIN team ON player.team_id = team.team_id# ON连接（非等值条件）SELECT p.player_name, p.height, h.height_level FROM player as p JOIN height_grades as h ON height BETWEEN h.height_lowest AND h.height_highest 外连接外连接与等值连接的不同在于，外连接除了两个表中字段满足等值关系的记录，还包括其中一个表的所有记录，如果这个表是左表就是左外连接，如果是右表就是右外连接。包括所有记录的表叫做主表，另一个经过条件筛选的表叫做从表。 1234567891011# SQL92写法# 左外连接，主表在左SELECT * FROM player, team where player.team_id = team.team_id(+)# 右外连接，主表在右SELECT * FROM player, team where player.team_id(+) = team.team_id# SQL99写法# 左外连接，主表在左SELECT * FROM player LEFT JOIN team on player.team_id = team.team_id# 右外连接，主表在右SELECT * FROM player RIGHT JOIN team on player.team_id = team.team_id SQL99还有全外连接：全外连接的结果 = 左右表匹配的数据 + 左表没有匹配到的数据 + 右表没有匹配到的数据。但是MySQL不支持全外连接 1SELECT * FROM player FULL JOIN team ON player.team_id = team.team_id 自连接将等值连接和非等值连接的两个表换成同一个表就是自连接。即表和它自身的某些字段满足一个关系就将记录连接起来。 1234# SQL92写法SELECT b.player_name, b.height FROM player as a , player as b WHERE a.player_name = &#x27;布雷克 - 格里芬&#x27; and a.height &lt; b.height# SQL99写法SELECT b.player_name, b.height FROM player as a JOIN player as b ON a.player_name = &#x27;布雷克 - 格里芬&#x27; and a.height &lt; b.height 自连接的处理速度要比子查询快得多，建议使用自连接代替子查询。 SQL99 标准的层次性和可读性更强，比如多表连接的时候： 1234SELECT ...FROM table1 JOIN table2 ON table1 和 table2 的连接条件 JOIN table3 ON table2 和 table3 的连接条件 SQL 连接具有通用性，但是不同的 DBMS 在使用规范上会存在差异，在标准支持上也存在不同。 Access、SQLite、MariaDB 等 不支持全外连接，Oracle、DB2、SQL Server 中是支持的。 Oracle 不支持 AS 。SQLite 只支持左连接，不支持右连接。 视图的创建和使用视图是基于表创建的虚拟表，可以看做是数据库查询的一个中间结果，视图可以让我们直接使用一些查询的中间结果，而不必每次重新查询。 创建语法： 1CREATE VIEW view_name AS SELECT column1, column2 FROM table WHERE condition 在 SQL 查询语句的基础上封装成视图 VIEW，基于 SQL 语句的结果集形成一张虚拟表。其中 view_name 为视图名称，column1、column2 代表列名，condition 代表查询过滤条件。 例如： 1234CREATE VIEW player_above_avg_height ASSELECT player_id, heightFROM playerWHERE height &gt; (SELECT AVG(height) from player) 当视图创建之后，它就相当于一个虚拟表，可以直接使用： 1SELECT * FROM player_above_avg_height 我们还可以通过视图来创建视图： 1234CREATE VIEW player_above_above_avg_height ASSELECT player_id, heightFROM playerWHERE height &gt; (SELECT AVG(height) from player_above_avg_height) 修改和删除修改视图的语法是： 1234ALTER VIEW view_name ASSELECT column1, column2FROM tableWHERE condition 你能看出来它的语法和创建视图一样，只是对原有视图的更新。比如我们想更新视图 player_above_avg_height，增加一个 player_name 字段，可以写成： 1234LTER VIEW player_above_avg_height ASSELECT player_id, player_name, heightFROM playerWHERE height &gt; (SELECT AVG(height) from player) 删除视图的语法是： 1DROP VIEW view_name 例如: 1DROP VIEW player_above_avg_height 通过表的连接来创建视图： 1234CREATE VIEW player_height_grades ASSELECT p.player_name, p.height, h.height_levelFROM player as p JOIN height_grades as hON height BETWEEN h.height_lowest AND h.height_highest 利用视图对数据进行格式化： 12CREATE VIEW player_team AS SELECT CONCAT(player_name, &#x27;(&#x27; , team.team_name , &#x27;)&#x27;) AS player_team FROM player JOIN team WHERE player.team_id = team.team_id 我们使用 CONCAT 函数，即CONCAT(player_name, &#39;(&#39; , team.team_name , &#39;)&#39;)，将 player_name 字段和 team_name 字段进行拼接，得到了拼接值被命名为 player_team 的字段名，将它放到视图 player_team 中。 利用视图做数字统计： 12CREATE VIEW game_player_score ASSELECT game_id, player_id, (shoot_hits-shoot_3_hits)*2 AS shoot_2_points, shoot_3_hits*3 AS shoot_3_points, shoot_p_hits AS shoot_p_points, score FROM player_score 视图的优点 安全性：虚拟表是基于底层数据表的，我们在使用视图时，一般不会轻易通过视图对底层数据进行修改，即使是使用单表的视图，也会受到限制，比如计算字段，类型转换等是无法通过视图来对底层数据进行修改的，这也在一定程度上保证了数据表的数据安全性。同时，我们还可以针对不同用户开放不同的数据查询权限，比如人员薪酬是个敏感的字段，那么只给某个级别以上的人员开放，其他人的查询视图中则不提供这个字段。 简单清晰：视图是对 SQL 查询的封装，它可以将原本复杂的 SQL 查询简化，在编写好查询之后，我们就可以直接重用它而不必要知道基本的查询细节。同时我们还可以在视图之上再嵌套视图。这样就好比我们在进行模块化编程一样，不仅结构清晰，还提升了代码的复用率。 另外，我们也需要了解到视图是虚拟表，本身不存储数据，如果想要通过视图对底层数据表的数据进行修改也会受到很多限制，通常我们是把视图用于查询，也就是对 SQL 查询的一种封装。那么它和临时表又有什么区别呢？在实际工作中，我们可能会见到各种临时数据。比如你可能会问，如果我在做一个电商的系统，中间会有个购物车的功能，需要临时统计购物车中的商品和金额，那该怎么办呢？这里就需要用到临时表了，临时表是真实存在的数据表，不过它不用于长期存放数据，只为当前连接存在，关闭连接后，临时表就会自动释放。 存储过程语法存储过程可以看做SQL语句的函数，函数名就是存储过程名，通过调用存储过程可以实现更复杂的数据处理。存储过程可以接收参数，可以包含流程控制。 语法： 1234CREATE PROCEDURE 存储过程名称 ([参数列表])BEGIN 需要执行的语句END 由 BEGIN 和 END 来定义我们所要执行的语句块。和视图一样，我们可以删除已经创建的存储过程，使用的是 DROP PROCEDURE。如果要更新存储过程，我们需要使用 ALTER PROCEDURE。 举例，累加1到n。 12345678910111213CREATE PROCEDURE `add_num`(IN n INT)BEGIN DECLARE i INT; DECLARE sum INT; SET i = 1; SET sum = 0; WHILE i &lt;= n DO SET sum = sum + i; SET i = i +1; END WHILE; SELECT sum;END 当我们需要再次使用这个存储过程的时候，直接使用 CALL add_num(50)，50即统计 1加到50 的积累之和。 默认情况下 SQL 采用;作为结束符，这样会逐条的执行语句，如果要将存储过程当做一个整体来执行，需要用DELIMITER来自定义结束符，新的结束符可以用//或者$$$$。 123456789101112131415DELIMITER //CREATE PROCEDURE `add_num`(IN n INT)BEGIN DECLARE i INT; DECLARE sum INT; SET i = 1; SET sum = 0; WHILE i &lt;= n DO SET sum = sum + i; SET i = i +1; END WHILE; SELECT sum;END //DELIMITER ; 先定义结束符为//，整个存储过程结束后用//结束，告诉SQL可以执行了，然后再将结束符还原成默认的;。 如果使用的是Navicat来编写存储过程，Navicat 会自动设置 DELIMITER 为其他符号，我们不需要再自定义结束符。 参数类型IN类型：用于传入参数，不能够作为返回值返回（默认）。 OUT类型：用于作为返回值返回。 INOUT类型：可传入参数，可作为返回值返回。 例如： 123456789CREATE PROCEDURE `get_hero_scores`( OUT max_max_hp FLOAT, OUT min_max_mp FLOAT, OUT avg_max_attack FLOAT, s VARCHAR(255) )BEGIN SELECT MAX(hp_max), MIN(mp_max), AVG(attack_max) FROM heros WHERE role_main = s INTO max_max_hp, min_max_mp, avg_max_attack;END 调用 12CALL get_hero_scores(@max_max_hp, @min_max_mp, @avg_max_attack, &#x27;战士&#x27;);SELECT @max_max_hp, @min_max_mp, @avg_max_attack; 关键字DECLARE：声明变量，变量必须在最前面声明。 SET：用等号为变量赋值。 SELECT…INTO：用查询结果为变量赋值。 IF…THEN…ENDIF：条件判断语句（ELSE、ELSEIF可用）。 CASE：CASE语法格式： 1234567CASE WHEN expression1 THEN ... WHEN expression2 THEN ... ... ELSE --ELSE 语句可以加，也可以不加。加的话代表的所有条件都不满足时采用的方式。END LOOP：无条件循环。 REPEAT…UNTIL…END REPEAT：相当于c语言的until循环。 WHILE…DO…END WHILE：相当于c语言的while do循环。 LEAVE 和 ITERATE：相当于break和continue。 优缺点优点：存储过程可以一次编译多次使用。存储过程只在创造时进行编译，之后的使用都不需要重新编译，这就提升了 SQL 的执行效率。其次它可以减少开发工作量。将代码封装成模块，实际上是编程的核心思想之一，这样可以把复杂的问题拆解成不同的模块，然后模块之间可以重复使用，在减少开发工作量的同时，还能保证代码的结构清晰。还有一点，存储过程的安全性强，我们在设定存储过程的时候可以设置对用户的使用权限，这样就和视图一样具有较强的安全性。最后它可以减少网络传输量，因为代码封装到存储过程中，每次使用只需要调用存储过程即可，这样就减少了网络传输量。同时在进行相对复杂的数据库操作时，原本需要使用一条一条的 SQL 语句，可能要连接多次数据库才能完成的操作，现在变成了一次存储过程，只需要连接一次即可。 缺点：它的可移植性差，存储过程不能跨数据库移植，比如在 MySQL、Oracle 和 SQL Server 里编写的存储过程，在换成其他数据库时都需要重新编写。其次调试困难，只有少数 DBMS 支持存储过程的调试。对于复杂的存储过程来说，开发和维护都不容易。此外，存储过程的版本管理也很困难，比如数据表索引发生变化了，可能会导致存储过程失效。我们在开发软件的时候往往需要进行版本管理，但是存储过程本身没有版本控制，版本迭代更新的时候很麻烦。最后它不适合高并发的场景，高并发的场景需要减少数据库的压力，有时数据库会采用分库分表的方式，而且对可扩展性要求很高，在这种情况下，存储过程会变得难以维护，增加数据库的压力，显然就不适用了。 事务事务的特性：ACID 原子性（Atomicity），要么完全执行，要么都不执行，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 一致性（Consistency），指一个事务执行之前和执行之后数据库都必须处于一致性状态。如果事务成功地完成，那么系统中所有变化将正确地应用，系统处于有效状态。如果在事务中出现错误，那么系统中的所有变化将自动地回滚，系统返回到原始状态（与之前一致的状态）。 隔离性（Isolation），指的是在并发环境中，当不同的事务同时操纵相同的数据时，每个事务都有各自的完整数据空间。由并发事务所做的修改必须与任何其他并发事务所做的修改隔离。事务查看数据更新时，数据所处的状态要么是另一事务修改它之前的状态，要么是另一事务修改它之后的状态，事务不会查看到中间状态的数据。 持久性（Durability），指的是只要事务成功结束，它对数据库所做的更新就必须永久保存下来。即使发生系统崩溃，重新启动数据库系统后，数据库还能恢复到事务成功结束时的状态。持久性是通过事务日志来保证的，包括回滚日志和重做日志。当通过事务对数据进行修改的时候，首先会将数据库的变化信息记录到重做日志，然后再对数据库进行修改。这样做的好处是，即使数据库系统崩溃，数据库重启后也能找到没有更新到数据库系统中的重做日志，重新执行，从而使事务具有持久性。 事务的常用控制语句： START TRANSACTION 或者 BEGIN，作用是显式开启一个事务。 COMMIT：提交事务。当提交事务后，对数据库的修改是永久性的。 ROLLBACK 或者 ROLLBACK TO [SAVEPOINT]，意为回滚事务。意思是撤销正在进行的所有没有提交的修改，或者将事务回滚到某个保存点。 SAVEPOINT：在事务中创建保存点，方便后续针对保存点进行回滚。一个事务中可以存在多个保存点。 RELEASE SAVEPOINT：删除某个保存点。 SET TRANSACTION，设置事务的隔离级别。 使用事务有两种方式，分别为隐式事务和显式事务。隐式事务实际上就是自动提交，Oracle 默认不自动提交，需要手写 COMMIT 命令，而 MySQL 默认自动提交，当然我们可以配置 MySQL 的参数： 12mysql&gt; set autocommit =0; // 关闭自动提交mysql&gt; set autocommit =1; // 开启自动提交 当我们设置 autocommit=1 时，每条 SQL 语句都会自动进行提交。不过这时，如果你采用 START TRANSACTION 或者 BEGIN 的方式来显式地开启事务，那么这个事务只有在 COMMIT 时才会生效，在 ROLLBACK 时才会回滚。 12345678910// mysql执行CREATE TABLE test(name varchar(255), PRIMARY KEY (name)) ENGINE=InnoDB;BEGIN;INSERT INTO test SELECT &#x27;关羽&#x27;;COMMIT;BEGIN;INSERT INTO test SELECT &#x27;张飞&#x27;;INSERT INTO test SELECT &#x27;张飞&#x27;;ROLLBACK;SELECT * FROM test; 执行的结果是只有“关羽”。只有BEGIN，没有COMMIT，显示开启了事务且未提交所以可以回滚。（name是主键，第二次插入“张飞”会报错。） 123456789// mysql执行CREATE TABLE test(name varchar(255), PRIMARY KEY (name)) ENGINE=InnoDB;BEGIN;INSERT INTO test SELECT &#x27;关羽&#x27;;COMMIT;INSERT INTO test SELECT &#x27;张飞&#x27;;INSERT INTO test SELECT &#x27;张飞&#x27;;ROLLBACK;SELECT * FROM test; 执行的结果是“关羽”和“张飞”。第一次插入MySQL会隐式开启事务并默认自动提交，所以无法回滚。 12345678910// mysql执行CREATE TABLE test(name varchar(255), PRIMARY KEY (name)) ENGINE=InnoDB;SET @@completion_type = 1;BEGIN;INSERT INTO test SELECT &#x27;关羽&#x27;;COMMIT;INSERT INTO test SELECT &#x27;张飞&#x27;;INSERT INTO test SELECT &#x27;张飞&#x27;;ROLLBACK;SELECT * FROM test; 默认情况，completion=0，普通的COMMIT；completion=1表示COMMIT=COMMIT AND CHAIN，提交之后，会开启一个链式事务；completion=2表示 COMMIT=COMMIT AND RELEASE，提交后，会自动与服务器断开连接。 所以上面的代码的执行结果是只有“关羽”。相当于在COMMIT之后开启了一个新的事务。 事务并发过程中可能存在的异常： 脏读（Dirty Read）：A在事务中更新表，且未提交，同时，B查询表，并读到了未提交的结果。 不可重复读（Nnrepeatable Read）：B查询表，此时，A修改了某条记录，B马上又查询一遍，查询到修改后的结果，前后两次查询结果的数据内容发生了变化。 幻读（Phantom Read）： A根据条件查询得到 N 条数据，此时，B更改或者增加了 M 条符合事务 A 查询条件的数据，当A再次进行查询的时候发现会有 N+M条数据，前后两次查询结果的数据量发生了变化。 不可重复读和幻读的区别在于，不可重复读是前后内容的修改，幻读是前后数据的增删。 SQL-92 标准定义了 4 种隔离级别来解决这些异常情况： 读未提交（READ UNCOMMITTED ）：允许读到未提交的数据，这种情况下查询是不会使用锁的，可能会产生脏读、不可重复读、幻读等情况。 读已提交（READ COMMITTED）：只能读到已经提交的内容，可以避免脏读的产生。可能产生不可重复读和幻读。 可重复读（REPEATABLE READ）：可重复读，保证一个事务在相同查询条件下两次查询得到的数据结果是一致的，可以避免不可重复读和脏读，但无法避免幻读。MySQL 默认的隔离级别就是可重复读。 可串行化（SERIALIZABLE）：将事务进行串行化，也就是在一个队列中按照顺序执行，可串行化是最高级别的隔离等级，可以解决事务读取中所有可能出现的异常情况，但是它牺牲了系统的并发性。 游标游标就相当于记录的指针，即行的指针。 定义游标： 1DECLARE cursor_name CURSOR FOR select_statement select_statement 代表的是 SELECT 语句。 12DECLARE cur_hero CURSOR FOR SELECT hp_max FROM heros; 开启游标： 1OPEN cursor_name 读取游标处的数据： 1FETCH cursor_name INTO var_name ... 读取当前行，游标指针指到下一行。 如果游标读取的数据行有多个列名，则在 INTO 关键字后面赋值给多个变量名即可。 关闭游标： 1CLOSE cursor_name 关闭游标之后，我们就不能再检索查询结果中的数据行，如果需要检索只能再次打开游标。 释放游标： 1DEALLOCATE PREPARE 我们一定要养成释放游标的习惯，否则游标会一直存在于内存中，直到进程结束后才会自动释放。当你不需要使用游标的时候，释放游标可以减少资源浪费。 下面是一个循环求和的存储过程，用到了游标来遍历行。 12345678910111213141516171819202122232425262728CREATE PROCEDURE `calc_hp_max`()BEGIN -- 创建接收游标的变量 DECLARE hp INT; -- 创建总数变量 DECLARE hp_sum INT DEFAULT 0; -- 创建结束标志变量 DECLARE done INT DEFAULT false; -- 定义游标 DECLARE cur_hero CURSOR FOR SELECT hp_max FROM heros; -- 指定游标循环结束时的返回值 DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = true; OPEN cur_hero; read_loop:LOOP FETCH cur_hero INTO hp; -- 判断游标的循环是否结束 IF done THEN LEAVE read_loop; END IF; SET hp_sum = hp_sum + hp; END LOOP; CLOSE cur_hero; SELECT hp_sum; DEALLOCATE PREPARE cur_hero;END","categories":[{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://wht6.github.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://wht6.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"理解Docker的文件系统","slug":"理解Docker的文件系统","date":"2022-02-15T08:00:00.000Z","updated":"2022-04-02T09:04:55.141Z","comments":true,"path":"posts/6635.html","link":"","permalink":"http://wht6.github.io/posts/6635.html","excerpt":"","text":"01|Docker存储镜像存储UnionFS（联合文件系统）是一种分层、轻量级并且高性能的文件系统，它支持将对文件系统的修改作为一次提交操作来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下。overlay2 是目前 Docker 默认的存储驱动，Docker支持选择其他的存储驱动。 文件系统在linux主机上只有两层，一个目录在下层，用来保存镜像（docker），另外一个目录在上层，用来存储容器信息。 镜像不包含任何动态数据，其内容在构建之后也不会被改变。Docker 镜像层次结构： base image：基础镜像 image：固化了一个标准运行环境，镜像本身的功能-封装一组功能性的文件，通过统一的方式，文件格式提供出来（只读） container：容器层（读写） 分层结构是 docker 镜像如此轻量的重要原因。当需要修改容器镜像内的某个文件时，只对处于最上方的读写层（容器层）进行变动，不覆写下层已有文件系统的内容，已有文件在只读层中的原始版本仍然存在，但会被读写层中的新版本所隐藏。当使用 docker commit 提交这个修改过的容器文件系统为一个新的镜像时，保存的内容仅为最上层读写文件系统中被更新过的文件。分层达到了在不的容器同镜像之间共享镜像层的效果。 Dockerfile是由一组指令组成的文件 基础镜像信息（指定操作系统镜像是什么镜像、什么版本) 维护者信息(可选填) 镜像操作指令 容器启动时执行指令（启动容器的时候，执行的脚本/命令参数等等) Dockerfile中的每一条指令都会对应于Docker镜像中的每一层。 如下面的Dockerfile： 1234FROM ubuntu:15.04COPY . /appRUN make /appCMD python /app/app.py 该Dockerfile包含四个命令，每个命令创建一个层。FROM语句是从Ubuntu：15.04镜像开始创建第一个层，COPY命令是从Docker客户端的当前目录添加一些文件，RUN命令使用make命令来构建应用程序，最后一层指定要在容器中运行的命令。 可以使用docker history redis命令查看镜像的所有层，从下往上看是生成的顺序。 镜像为容器提供基础文件系统，容器运行时需要依赖镜像中的内容，一般不允许删除正在已存在容器的底层镜像，除非强制删除。 通过docker image inspect nginx查看镜像的详细内容，里面有一个GraphDriver的关键字指示了镜像是在宿主机上存储的路径。通过docker inspect nginxtest查看容器的详细内容，里面的GraphDriver的关键字指示了容器是在宿主机上存储的路径。 1、rootfs： 基础镜像 2、lower： 下层信息（为镜像层，可读） 3、upper： 上层目录（容器信息,可写） 4、merged： “视图层”（容器视图）（镜像基础文件与容器的修改变化的合并的抽象视图） 5、worker： 运行的工作目录 nginx的LowerDir包含四个目录（diff表示只存储不同），生成顺序也是从下往上，最下面的目录就是nginx的底层linux系统根目录，我们进入该目录，通过ls -i查看文件的inode，然后我们用这个nginx镜像以交互的形式启动一个容器，然后再通过ls -i查看文件的inode，会发现inode一模一样，说明是同一个文件，因为容器的基础文件系统就是镜像提供的。通过docker ps -s可以查看容器实际占用的存储，如果是刚启动的容器，其自身并不占用多少存储空间，因为容器用的是镜像的文件。 virtual size：容器使用的用于只读镜像数据的数据量加上容器的可写镜像层大小。 如果容器要对镜像文件进行修改，该如何进行？这就用到了Docker的Copy on Write（写时复制）。 docker 镜像使用了写时复制(copy-on-write)的策略，在多个容器之间共享镜像，每个容器在启动的时候并不需要单独复制一份镜像文件，而是将所有镜像层以只读的方式挂载到一个挂载点，再在上面覆盖一个可读写的容器层。在未更改文件内容时，所有容器共享同一份数据，只有在 docker 容器运行过程中文件系统发生变化时，才会把变化的文件内容写到可读写层，并隐藏只读层中的老版本文件。写时复制配合分层机制减少了镜像对磁盘空间的占用和容器启动时间。 容器挂载容器挂载的原因：比如，在容器中运行着一个MySQL数据库，由于容器中的数据是动态的，当容器损坏，容器中的数据也会随之丢失，这样就保证不了数据的可靠性。容器挂载可以将容器中的文件挂载到主机上的某个目录，当容器损坏，数据将不受影响。 Docker提供三种方式将数据从宿主机挂载到容器中 volumes：Docker管理宿主机文件系统的一部分（/var/lib/docker/volumes） 保存数据的最佳方式 bind mounts：将宿主机上的任意位置的文件或者目录挂载到容器中， 就像软连接一样 tmpfs：挂载存储在主机系统的内存中，而不会写入主机的文件系统（不常用） 区别： volume ： 是docker的宿主机文件系统一部分，只有docker可以进行更改，其他进程不能修改 bind mounts ： 是挂载在宿主机文件系统的任意位置，除了docker所有进程都可以进行修改 管理卷 123456789docker volume create nginx-vol # 创建一个数据卷 nginx-voldocker volume ls # 查看宿主机数据卷信息 docker volume inspect nginx-vol # 查看 nginx-vol 这个数据卷详细信息ls /var/lib/docker/volumes/nginx-vol/_data # 详细信息中会显示 nginx-vol 这个卷实际在宿主机位置docker rm -f $(docker ps -a |awk &#x27;&#123;print $1&#125;&#x27;) # 删除所有容器 bind mounts手动挂载： -v 宿主机绝对路径:容器目录 1234docker run -d -P --name some-nginx -v /some/content:/usr/share/nginx/html:ro --restart=always nginx# 以只读的方式将容器的/usr/share/nginx/html目录挂载到宿主机的/some/content目录（默认是rw读写）# 使用这种方式需要首先在目录中准备好文件，否则可能会出现空挂载问题，挂载的目录为空，没有程序所必须的文件--restart=always #当Docker重启时，容器自动启动。 volumes自动挂载 -v 卷名称:容器目录 （创建一个匿名卷：-v 容器目录） 12docker run -d -P --name some-nginx -v html:/usr/share/nginx/html:ro --restart=always nginx# 以只读的方式将容器的/usr/share/nginx/html目录自动挂载到宿主机上 怎么查看olumes自动挂载的位置，使用docker inspect命令查看mounts字段。 12# 同时挂载配置文件和html页面docker run -d -P -v nginxconf:/etc/nginx/ -v nginxpage:/usr/share/nginx/html --restart=always nginx","categories":[{"name":"云原生","slug":"云原生","permalink":"http://wht6.github.io/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://wht6.github.io/tags/Docker/"},{"name":"容器","slug":"容器","permalink":"http://wht6.github.io/tags/%E5%AE%B9%E5%99%A8/"}]},{"title":"Docker的基本概念和命令","slug":"Docker的基本概念和命令","date":"2022-02-14T03:00:00.000Z","updated":"2022-04-04T10:26:52.301Z","comments":true,"path":"posts/865e.html","link":"","permalink":"http://wht6.github.io/posts/865e.html","excerpt":"","text":"01|基本概念1、Docker架构&nbsp;&nbsp; Client： 客户端；操作docker服务器的客户端（命令行或者界面） Docker_Host：Docker主机；安装Docker服务的主机 Docker_Daemon：后台进程；运行在Docker服务器的后台进程 Containers：容器；在Docker服务器中的容器（一个容器一般是一个应用实例，容器间互相隔离） Images：镜像、映像、程序包；Image是只读模板，其中包含创建Docker容器的说明。容器是由Image运行而来，Image固定不变。 Registries：仓库；存储Docker Image的地方。官方远程仓库地址： https://hub.docker.com/search &nbsp; Docker用Go编程语言编写，并利用Linux内核的多种功能来交付其功能。 Docker使用一种称为命名空间的技术来提供容器的隔离工作区。 运行容器时，Docker会为该容器创建一组命名空间。 这些命名空间提供了一层隔离。 容器的每个方面都在单独的命名空间中运行，并且对其的访问仅限于该命名空间。 &nbsp; 镜像和容器的关系（一个类可以创建多个实例）（镜像和容器也可以比作软件安装包和软件，一个安装包可以在多个机器上安装多个软件）&nbsp; Docker 面向对象 镜像（Image） 类 容器（Container） 对象（实例） &nbsp;容器与虚拟机&nbsp; &nbsp; 这个图并不是很合理，可以参考博客中的关于Docker的其他文章。 2、Docker隔离原理&nbsp;namespace 6项隔离 （资源隔离）&nbsp; namespace 系统调用参数 隔离内容 UTS CLONE_NEWUTS 主机和域名 IPC CLONE_NEWIPC 信号量、消息队列和共享内存 PID CLONE_NEWPID 进程编号 Network CLONE_NEWNET 网络设备、网络栈、端口等 Mount CLONE_NEWNS 挂载点(文件系统) User CLONE_NEWUSER 用户和用户组 &nbsp;cgroups资源限制 （资源限制）&nbsp;cgroup提供的主要功能如下：&nbsp; 资源限制：限制任务使用的资源总额，并在超过这个 配额 时发出提示 优先级分配：分配CPU时间片数量及磁盘IO带宽大小、控制任务运行的优先级 资源统计：统计系统资源使用量，如CPU使用时长、内存用量等 任务控制：对任务执行挂起、恢复等操作 &nbsp;cgroup资源控制系统，每种子系统独立地控制一种资源。功能如下：&nbsp; 子系统 功能 cpu 使用调度程序控制任务对CPU的使用。 cpuacct(CPU Accounting) 自动生成cgroup中任务对CPU资源使用情况的报告。 cpuset 为cgroup中的任务分配独立的CPU(多处理器系统时)和内存。 devices 开启或关闭cgroup中任务对设备的访问 freezer 挂起或恢复cgroup中的任务 memory 设定cgroup中任务对内存使用量的限定，并生成这些任务对内存资源使用 情况的报告 perf_event(Linux CPU性能探测器) 使cgroup中的任务可以进行统一的性能测试 net_cls(Docker未使 用) 通过等级识别符标记网络数据包，从而允许Linux流量监控程序(Trawic Controller)识别从具体cgroup中生成的数据包 &nbsp; 3、Docker安装&nbsp; 以下以centos为例；更多其他安装方式，详细参照文档： https://docs.docker.com/engine/install/centos/ &nbsp; 1、移除旧版本&nbsp;1sudo yum remove docker*&nbsp; 2、设置docker yum源&nbsp;1234sudo yum install -y yum-utilssudo yum-config-manager \\--add-repo \\https://download.docker.com/linux/centos/docker-ce.repo&nbsp; 3、安装最新docker engine&nbsp;1sudo yum install docker-ce docker-ce-cli containerd.io&nbsp; 4、安装指定版本docker engine&nbsp; 1、在线安装&nbsp;12345678#找到所有可用docker版本列表yum list docker-ce --showduplicates | sort -r# 安装指定版本，用上面的版本号替换&lt;VERSION_STRING&gt;sudo yum install docker-ce-&lt;VERSION_STRING&gt;.x86_64 docker-ce-cli-&lt;VERSION_STRING&gt;.x86_64 containerd.io#例如：#yum install docker-ce-3:20.10.5-3.el7.x86_64 docker-ce-cli-3:20.10.5-3.el7.x86_64 containerd.io#注意加上 .x86_64 大版本号&nbsp; 2、离线安装&nbsp;https://download.docker.com/linux/centos/7/x86_64/stable/Packages/&nbsp;123rpm -ivh xxx.rpm# 可以下载 tar# 解压启动即可&nbsp; https://docs.docker.com/engine/install/binaries/#install-daemon-and-client-binaries-on-linux &nbsp; 5、启动服务&nbsp;123systemctl start dockersystemctl enable dockerdocker version # 查看Docker的版本&nbsp; 6、镜像加速&nbsp;12345678910sudo mkdir -p /etc/dockersudo cat &gt;&gt; /etc/docker/daemon.json &lt;&lt;EOF&#123;&quot;registry-mirrors&quot;: [&quot;https://82m9ar63.mirror.aliyuncs.com&quot;]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker#以后docker下载直接从阿里云拉取相关镜像&nbsp; /etc/docker/daemon.json 是Docker的核心配置文件。 &nbsp; 7、可视化界面-Portainer&nbsp; 1、什么是Portainer&nbsp;https://documentation.portainer.io/&nbsp;Portainer社区版2.0拥有超过50万的普通用户，是功能强大的开源工具集，可让您轻松地在Docker，Swarm，Kubernetes和Azure ACI中构建和管理容器。 Portainer的工作原理是在易于使用的GUI后面隐藏使管理容器变得困难的复杂性。通过消除用户使用CLI，编写YAML或理解清单的需求，Portainer使部署应用程序和解决问题变得如此简单，任何人都可以做到。 Portainer开发团队在这里为您的Docker之旅提供帮助；&nbsp; 2、安装&nbsp;123456789# 服务端部署docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v/var/run/docker.sock:/var/run/docker.sock -v portainer_data:/dataportainer/portainer-ce# 访问 9000 端口即可#agent端部署docker run -d -p 9001:9001 --name portainer_agent --restart=always -v/var/run/docker.sock:/var/run/docker.sock -v/var/lib/docker/volumes:/var/lib/docker/volumes portainer/agent&nbsp; 02|Docker命令1、常见命令&nbsp;所有Docker命令手册https://docs.docker.com/engine/reference/commandline/docker/&nbsp; 命令 作用 attach 绑定到运行中容器的 标准输入, 输出,以及错误流（这样似乎也能进入容器内容，但是 一定小心，他们操作的就是控制台，控制台的退出命令会生效，比如redis,nginx…） build 从一个 Dockerfile 文件构建镜像 commit 把容器的改变 提交创建一个新的镜像 cp 容器和本地文件系统间 复制 文件/文件夹 create 创建新容器，但并不启动（注意与docker run 的区分）需要手动启动。start\\stop diff 检查容器里文件系统结构的更改【A：添加文件或目录 D：文件或者目录删除 C：文件或者目录更改】 events 获取服务器的实时事件 exec 在运行时的容器内运行命令 export 导出容器的文件系统为一个tar文件。commit是直接提交成镜像，export是导出成文件方便在本地移动。 history 显示镜像的历史 images 列出所有镜像 import 导入tar的内容创建一个镜像，但是导入进来的镜像无法直接启动容器。必须使用导出之前镜像的完整启动命令去启动它。用docker ps --no-trunc查看完整启动命令。 info 显示系统信息 inspect 获取docker对象的底层信息 kill 杀死一个或者多个容器 load 从 tar 文件加载镜像 login 登录Docker registry logout 退出Docker registry logs 获取容器日志 pause 暂停一个或者多个容器 port 列出容器的端口映射 ps 列出所有容器 pull 从registry下载一个image 或者repository push 给registry推送一个image或者repository rename 重命名一个容器 restart 重启一个或者多个容器 rm 移除一个或者多个容器 rmi 移除一个或者多个镜像 run 创建并启动容器 save 把一个或者多个镜像保存为tar文件 search 去docker hub寻找镜像 start 启动一个或者多个容器 stats 显示容器资源的实时使用状态 stop 停止一个或者多个容器 tag 给源镜像创建一个新的标签，变成新的镜像 top 显示正在运行容器的进程 unpause pause的反操作 update 更新一个或者多个docker容器配置 version Show the Docker version information container 管理容器 image 管理镜像 network 管理网络 volume 管理卷 &nbsp;Docker命令的关系图&nbsp;&nbsp;根据正在运行的容器制作出相关的镜像：反向&nbsp;根据镜像启动一个容器：正向&nbsp;下载镜像并启动一个容器：&nbsp;1、先去软件市场搜镜像：docker hub 2、下载镜像 docker pull xxx 3、启动软件 docker run 镜像名；&nbsp;通过命令docker image --help 查看镜像的所有管理操作。&nbsp; 123456789101112131415161718192021222324252627282930313233docker pull redis == docker pull redis:latest（最新版）# 阿里云的镜像是从docker hub来的，我们配置了加速，默认是从阿里云（缓存）下载docker images # 查看本地所有镜像docker rmi 镜像名 # 删除某个镜像 （-f为强制删除）docker rm 容器id # 删除某个容器docker ps # 列出所有容器 （-a则会将已经停止的容器也列出）docker rmi -f $(docker images -aq) #删除全部镜像docker rm -f $(docker ps -aq) # 删除所有容器docker image prune #移除游离镜像 dangling：游离镜像（没有镜像名字的）docker tag 原镜像:标签 新镜像名:标签 #起别名docker kill 容器名 # 是强制关闭docker stop 容器名 # 可以允许优雅停机(当前正在运行中的程序处理完所有事情后再停止)docker logs mynginx # -d运行前台没有输出，通过logs命令查看日志(可以加-f来持续追踪日志)docker attach # 绑定的是控制台. 可能导致容器停止，所有一般不用这个命令，而用execdocker container inspect 容器名 = docker inspect 容器名 # 查看容器的详细信息docker inspect image /network/volume ....docker cp index.html mynginx:/usr/share/nginx/html # 复制文件docker cp mynginx:/etc/nginx/nginx.conf nginx.confdocker diff 容器名 # 检查容器里文件系统结构的更改,即容器的改变docker commit -a 作者名 -m &quot;信息&quot; mynginx4 mynginx:v4 # mynginx:v4是给镜像起的名字docker save -o busybox.tar busybox:latest # 把busybox镜像保存成tar文件docker load -i busybox.tar # 把压缩包里面的内容直接导成镜像 &nbsp; 容器的状态 &nbsp; Created（新建）、Up（运行中）、Pause（暂停）、Exited（退出）&nbsp;docker hub的一个镜像的完整路径&nbsp; 对于官方镜像：docker.io/library/仓库名/标签 对于非官方镜像：docker.io/仓库名/标签 &nbsp;推送镜像的过程：&nbsp; 注册docker hub并登录 可以创建一个仓库，选为public 登录远程docker仓库 ，docker login，输入用户名和密码。（判断是否登录，使用命令cat ~/.docker/config.json 有 auth的值说明登录了 ） 给需要上传的镜像改名为：你的用户名/镜像名/标签，即docker tag [镜像名/标签] [你的用户名/镜像名/标签] docker push docker.io/带用户名的镜像名/标签（docker.io/可以省略） &nbsp;制作镜像的过程：&nbsp; 新建Dockerfile文件 编写docker file中的内容 docker build构建镜像 &nbsp;例如：&nbsp;Dockerfile的内容：&nbsp; 12FROM busyboxping baidu.com &nbsp; 构建镜像的命令： &nbsp;1docker build -t mybusy66:v6 -f Dockerfile ./&nbsp; 2、典型命令&nbsp; 1、docker run&nbsp;常用关键参数 OPTIONS 说明：&nbsp; -d: 后台运行容器，并返回容器ID； -i: 以交互模式运行容器，通常与 -t 同时使用； -P: 随机端口映射，容器内部端口随机映射到主机的端口 -p:指定端口映射，格式为：主机(宿主)端口:容器端口 -t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用 --name=&quot;nginx-lb&quot;:为容器指定一个名称； --dns 8.8.8.8: 指定容器使用的DNS服务器，默认和宿主一致； --dns-search example.com: 指定容器DNS搜索域名，默认和宿主一致； -h &quot;mars&quot;: 指定容器的hostname； -e username=&quot;ritchie&quot;: 设置环境变量； --env-file=[]: 从指定文件读入环境变量； --cpuset=&quot;0-2&quot; or --cpuset=&quot;0,1,2&quot;: 绑定容器到指定CPU运行； -m:设置容器使用内存最大值； --net=&quot;bridge&quot;: 指定容器的网络连接类型，支持 bridge/host/none/container: 四种类型； --link=[]: 添加链接到另一个容器； --expose=[]: 开放一个端口或一组端口； --restart, 指定重启策略，可以写—restart=awlays 总是故障重启 --volume , -v: 绑定一个卷。一般格式 主机文件或文件夹:虚拟机文件或文件夹 &nbsp;使用Docker部署组件的步骤：&nbsp; 1、先去找组件的镜像 2、查看镜像文档，了解组件的可配置内容 3、docker run进行部署 &nbsp;12docker run --name myredis2 -p 6379:6379 -p 8888:6379 redis -d # 宿主机的很多端口绑定容器的一个端口是允许的，宿主机的一个端口绑定容器的多个端口是行不通的，因为从主机端口来的数据它不知道给谁。默认是前台启动的，一般加上-d 让他后台悄悄启动docker run -d == docker create + docker start&nbsp; 注意：直接用docker run busybox这个命令并不能启动busybox，这是因为busybox运行后开启任何应用进程，启动后直接就退出了。所以我们需要手动开启一个应用进程，docker run -it busybox（开启终端），docker run -d busybox ping baidu.com（开启ping程序）。 &nbsp; 1、部署Nginx&nbsp;12345# 注意 外部的/nginx/conf下面的内容必须存在，否则挂载会覆盖docker run --name nginx-app \\-v /app/nginx/html:/usr/share/nginx/html:ro \\-v /app/nginx/conf:/etc/nginx-d nginx&nbsp; 2、部署MySQL&nbsp;12345678910111213141516# 5.7版本docker run -p 3306:3306 --name mysql57-app \\-v /app/mysql/log:/var/log/mysql \\-v /app/mysql/data:/var/lib/mysql \\-v /app/mysql/conf:/etc/mysql/conf.d \\-e MYSQL_ROOT_PASSWORD=123456 \\-d mysql:5.7#8.x版本,引入了 secure-file-priv 机制，磁盘挂载将没有权限读写data数据，所以需要将权限透传，或者chmod -R 777 /app/mysql/data# --privileged 特权容器，容器内使用真正的root用户docker run -p 3306:3306 --name mysql8-app \\-v /app/mysql/conf:/etc/mysql/conf.d \\-v /app/mysql/log:/var/log/mysql \\-v /app/mysql/data:/var/lib/mysql \\-e MYSQL_ROOT_PASSWORD=123456 \\--privileged \\-d mysql&nbsp; 3、部署Redis&nbsp;123456789# 提前准备好redis.conf文件，创建好相应的文件夹。如：port 6379appendonly yes#更多配置参照 https://raw.githubusercontent.com/redis/redis/6.0/redis.confdocker run -p 6379:6379 --name redis \\-v /app/redis/redis.conf:/etc/redis/redis.conf \\-v /app/redis/data:/data \\-d redis:6.2.1-alpine3.13 \\redis-server /etc/redis/redis.conf --appendonly yes&nbsp; 4、部署ElasticSearch&nbsp;123456789101112#准备文件和文件夹，并chmod -R 777 xxx#配置文件内容，参照https://www.elastic.co/guide/en/elasticsearch/reference/7.5/node.name.html 搜索相关配置# 考虑为什么挂载使用esconfig ...docker run --name=elasticsearch -p 9200:9200 -p 9300:9300 \\-e &quot;discovery.type=single-node&quot; \\-e ES_JAVA_OPTS=&quot;-Xms300m -Xmx300m&quot; \\-v /app/es/data:/usr/share/elasticsearch/data \\-v /app/es/plugins:/usr/shrae/elasticsearch/plugins \\-v esconfig:/usr/share/elasticsearch/config \\-d elasticsearch:7.12.0&nbsp; 5、部署Tomcat&nbsp;12345# 考虑，如果我们每次 -v 都是指定磁盘路径，是不是很麻烦？docker run --name tomcat-app -p 8080:8080 \\-v tomcatconf:/usr/local/tomcat/conf \\-v tomcatwebapp:/usr/local/tomcat/webapps \\-d tomcat:jdk8-openjdk-slim-buster&nbsp; 6、重启策略&nbsp; no，默认策略，在容器退出时不重启容器 on-failure，在容器非正常退出时（退出状态非0），才会重启容器 on-failure:3，在容器非正常退出时重启容器，最多重启3次 always，在容器退出时总是重启容器 unless-stopped，在容器退出时总是重启容器，但是不考虑在Docker守护进程启动时就已经停止了的容器 &nbsp; 2、docker exec&nbsp;1docker exec -it -u 0:0 mynginx4 /bin/bash # -i表示交互模式，-t表示新的终端 -u 0:0表示用户的属主和属组（换成--privileged表示以root身份进入容器），/bin/bash是控制台程序（可以通过exit退出bash终端）&nbsp; 3、docker create&nbsp;123456docker create [OPTIONS] IMAGE [COMMAND] [ARG...]docker create [设置项] 镜像名 [启动命令] [启动参数...]docker create redis # 按照redis:latest镜像启动一个容器docker create --name myredis -p 6379（主机的端口）:6379（容器的端口） redis # --name表示给容器起名，-p是端口绑定 （-p port1:port2） port1到port2是主机端口到容器端口的映射。docker start myredis # 创建之后启动容器 &nbsp;","categories":[{"name":"云原生","slug":"云原生","permalink":"http://wht6.github.io/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://wht6.github.io/tags/Docker/"},{"name":"容器","slug":"容器","permalink":"http://wht6.github.io/tags/%E5%AE%B9%E5%99%A8/"}]},{"title":"阿里云重要知识点","slug":"阿里云重要知识点","date":"2022-01-21T02:00:00.000Z","updated":"2022-03-26T02:43:09.873Z","comments":true,"path":"posts/fd04.html","link":"","permalink":"http://wht6.github.io/posts/fd04.html","excerpt":"","text":"健康检查失败时间窗=响应超时时间×不健康阈值+检查间隔×（不健康阈值-1），健康检查成功时间窗= （健康检查成功响应时间×健康阈值）+检查间隔×（健康阈值-1）。 在阿里云上开通安骑士功能可以减少密码被暴力破解的可能。 DDoS基础防护阀值只有5GB，无法保证业务需要保持在99.9%以上，高防IP可以。 公网IP是购买负载均衡SLB时候系统分配的，或者你单独购买EIP后单独给负载均衡SLB绑定。并不是在你在负载均衡的配置和管理里面可以设定的。 “最新创建的实例”不管是自动创建还是手动创建，都会被移出，而“最早伸缩配置对应的实例”会优先移除系统自动创建的实例，跳过手动创建的实例。 VPC实例的状态变为Available之后,表示VPC创建成功,可以进行下一步的管理操作 在OSS的控制台的属性设置里面，是没有设置OSS文件被访问时的HTTP头（Header）这个操作 ECS外部系统可以通过API在请求时传入参数来指定返回的数据格式,默认XML格式 防敏感信息泄露是WAF的功能，收费。 SLB的UploadServerCertificate接口一次只能上传一份服务器证书和对应的私钥。 OSS目前不具备OCR文字识别的处理能力。 单表的有效最大表尺寸通常受限于操作系统的文件尺寸限制，而不是受MySQL内部机制的限制。 由于RDS MySQL实例的最大尺寸为 2TB， 因此单表的最大尺寸为略小于2TB（因为会有些元数据等的开销）。若RDS MySQL实例有多张表，多张表的总和也不能超过2TB。单表的最大记录数在2000万条以内。 阿里云CDN支持对同名更新的实时更新，用户只要做相关配置后，在用户不主动提交请求的时候，CDN不会自动去实现刷新请求。 开通ECS实例后，ddos基础防护和阿里绿网和安骑士都是免费的，服务器安全托管是要收费的。 OPDS即MaxCompute不能创建为专有网络类型。 ECS创建之后，系统盘的设备名应该是/dev/xvda，所以挂载到数据盘不能再使用这个名字。 OSS支持文件读取，创建和删除，不支持修改。 同一个负载均衡SLB实例的后端服务器池中可以包含多个伸缩组。 Channel 是IMG（图片处理服务）上的命名空间，也是计费、权限控制、日志记录等高级功能的管理实体。IMG名称在整个图片处理服务中具有全局唯一性，且不能修改。一个用户最多可创建10个Channel，但每个Channel中存放的object的数量没有限制，所以每个Channel的容量是没有上限的。 将VPC中ECS服务器切换/迁移到同VPC下的其他交换机的步骤：1）打开云服务器管理控制台；2）找到对应的需要切换/迁移的云服务器；3）修改云服务器的私网地址；4）选择您所需的交换机,同时指定新交换机下的IP。 安骑士可以进行木马文件检查、高危漏洞修复、防密码暴力破解和异地登录报警，但是不能防web应用系统密码破解。 ECS支持强制停止，但是会丢失内存中的数据。 如果在配置阿里云的负载均衡SLB实例的监听时,开启了“获取真实访问IP”,针对7层服务可以通过http头部中的X-Forwarded-For字段获取来访者真实IP。 ECS带宽临时升级：可以按天进行升级,升级后如果云服务器ECS续费,仍然按照原基础带宽进行续费。 目前阿里云的负载均衡SLB只支持PEM格式的证书。 用户在阿里云以外的服务器上安装“安骑士客户端”后,通过“在管理控制台生成的安装验证key”的方式与指定的阿里云官网帐号关联。 弹性伸缩目前只支持ECS及其数据盘的弹性伸缩,不支持云数据库RDS、负载均衡等云产品的弹性伸缩。 修改云服务器ECS的私网IP地址需要云服务器ECS处于停止的状态。 ECS只有在稳定( 运行中, 已停止) 状态才能挂载磁盘, 其他状态都不行。 HTTPDNS是面向移动开发者推出的一款域名解析产品,具有域名防劫持、精准调度等特性。主要是用在客户端的时候请求URL防止域名劫持的情况。 升级云服务器ECS实例的CPU和内存后,必须通过阿里云的管理控制台重启ECS实例才能生效,在ECS实例内重启无效。 回滚磁盘必须要求云盘必须已经挂载到某台ECS实例上,而且已经停止实例。 用户可以卸载阿里云的云服务器ECS实例上的云盾安骑士客户端,在需要的时候可以再次安装。 阿里云对象存储OSS的存储空间Bucket支持删除操作,在删除Bucket前必须先删除Bucket中的所有文件,包括未完成的分片文件。 在安全管理方面，云计算服务网络安全管理的基本要求，提出了“安全管理责任不变，数据归属关系不变，安全管理标准不变，敏感信息不出境”四条基本要求。不包含”运维管理方式不变”。 使用阿里云OSS产品实现在线的音视频内容直播时,必须要和阿里云的多媒体转码服务MTS产品一起配合实现。 AccessKeySecret是用于加密签名字符串和服务器端验证签名字符串的密钥，AccessKeySecret并不支持通过控制台直接查看，AccessKeySecret只能一次生成，后续不支持再次查看。如果忘记AccessKeySecret，只能在授权登录后在控制台重新生成。 您可以将不希望被移出伸缩组的ECS实例转为保护状态，处于保护状态的ECS实例负载均衡权重不受影响。AS不会检查处于保护状态的ECS实例健康状态，也不会释放ECS实例。需要用户手动管理该台ECS实例生命周期。 扩容云盘只是扩大存储容量，但不会扩容ECS实例的文件系统（应该是扩容的那部分容量并没有以磁盘的形式进行挂载）。您还需要登录实例，然后进行扩容文件系统的操作。 Windows系统盘扩容后磁盘可用空间反而变少：因为虚拟内存被开启且设置的是系统自动管理。 linux系统的服务器不支持开启Selinux服务,如果开启了Selinux服务,会导致系统异常并无法启动。 在使用部署集之前，您需要注意：部署集之间不支持相互合并。部署集内不能创建抢占式实例。部署集不支持创建专有宿主机。 四层是通过SLB中设定的转发策略和规则和报文中的目标IP地址和端口分发流量，七层是通过报文中真正有意义的应用层内容和负载均衡SLB中设定的转发策略和规则分发流量。负载均衡SLB的四层服务的会话保持是基于源IP实现的，七层服务的会话保持是基于Cookie实现的。 存储空间创建成功后，其名称、所处地域、存储类型不能修改。 在阿里云上创建专有网络VPC时，VPC会自动为用户创建1条系统路由，这条路由的作用是用于专有网络内的云产品实例访问专有网络外的云服务（该云服务支持VPC内的云产品实例直接访问）。 阿里云的云盾数据风控可以很好地解决WEB应用中常见的垃圾注册、刷库等业务风险识别的难题， 要想使用这项服务首先得进行业务数据的采集，对于WEB应用系统，可以采用JavaScript方式来采集信息。（数据风控目前只支持在Web中插入指定的JS代码，其他的暂时还不支持） 出于改善性能的考虑，将正在使用的1核2G云服务器的配置提升至2核8G，这种操作在云计算中被称为垂直扩展。升级ECS的配置，所以称之为垂直扩展，通过弹性伸缩来完成的拓展，称之为水平扩展。 定时任务独立于伸缩组存在，不依赖伸缩组的生命周期管理，删除伸缩组不会删除定时任务；云监控报警任务独立于伸缩组存在，不依赖伸缩组的生命周期管理，删除伸缩组不会删除报警任务。 伸缩组中包含的云服务器ECS实例有两种类型，一种是根据用户的伸缩配置和伸缩规则由弹性伸缩服务自动创建的云服务器ECS实例，另外一种是由用户手工添加到伸缩组中的ECS实例。当ECS实例被弹性伸缩从伸缩组中移出时，对于自动创建的ECS实例会停止和释放，对于手工添加的ECS实例则不会停止和释放。 在使用阿里云弹性伸缩(Auto Scaling)时,希望能根据计算资源的使用情况来增加或者减少云服务器ECS实例,如当CPU利用率小于等于30%时则减少一台云服务器ECS实例,首先配置伸缩规则为“减少1台ECS”,然后还需要（创建报警任务）来实现。 自动快照是保存在OSS里面，但是是保存在独立于用户自己的OSS Bucket里面，而不能保存在用户的Bucket里面。 Aliyun Linux和Centos都属于RedHat一家的。兼容Red Hat。其中包括（Red Hat Enterprise Linux， Fedora，CentOS，Scientific Linux， Oracle Linux） OSS每小时结算一次调用 OSS API 的请求费用。目前只支持按量付费：每万次请求的单价 * 每小时实际请求次数/10000。 阿里云负载均衡服务SLB支持用户可以同时设置实例维度上的“后端服务器”、监听维度上的“虚拟服务器组”和“转发规则”，那么当用户流量经过负载均衡某端口时，我们首先判断其是否能够匹配上某条“转发规则”，如果匹配，则将流量转发到该规则的后端服务器组上；若不匹配并且在该监听上设置了虚拟服务器组，那么将流量转发到该虚拟服务器组上；若用户没有在该监听上设置虚拟服务器组，即将流量转发到实例级别添加的各后端服务器中。 如果您使用OSS自带域名，如http://bucketname.oss.aliyuncs.com/objcet访问静态文件时（文件类型包括：txt、html、htm、图片格式、视频格式、音频格式等），均限制在浏览器中以“另存为”下载的方式打开文件，而不能直接浏览该文件。因此您需要将自定义的域名访问绑定在属于自己的Bucket上面，即CNAME。域名绑定成功后，为了使用域名正常访问 OSS，还需要添加 CNAME 记录指向存储空间对应的外网域名。 在使用阿里云负载均衡SLB时后端服务器可以设置主备服务器组,当主机工作正常时，流量将直接走主机；当主机宕机时，流量将走到备机。主备服务器组是在监听维度上维护的，并且只支持四层监听。 健康检查四层是通过端口检查是否超时，七层则是检查服务器端返回的状态码。 专用网络VPC下面的ECS，不管是否绑定了EIP，在设置安全组的时候，只可以设置内网规则，外网规则是不可以选的。因为对于其来说，内网规则等同于外网规则。 伸缩活动中，最后一个ECS实例加入或移出完成后，整个伸缩组冷却时间才开始计时，伸缩活动结束并不代表最后一个ECS实例已经加入或移出完成。 数据风控由阿里聚安全提供，是基于阿里大数据计算能力，通过业内领先的风险决策引擎，解决企业账号、活动、交在的易等关键业务环节存欺诈威胁，降低企业经济损失。 云服务器ECS实例与OSS之间的请求次数，不分内外网都会计费。走内网流量是不收费的，但是每次的请求次数还是收费。 当文件所在的Bucket的读写权限为“私有”时，OSS分享链接采用的安全机制是在管理控制台中获取文件访问URL时设置分享链接有效的时间，超过设定时间就无法下载。出于安全考虑，目前控制台针对私有bucket的获取URL链接签名操作做了优化，使用的AccessKey是临时生成的密钥对，主账号用户最长有效时间是64800秒（18小时），RAM子账号用户以及sts用户最长有效时间是3600秒（1小时），超时后链接就无法访问了。 云监控的http提交指令为post。 停用伸缩组以后，之前的冷却时间即失效。所以后面伸缩活动不会受到冷却时间的影响。 管理云盾加密服务的密钥时，必须通过身份卡(USB Key)方式的认证。 针对7层（HTTP协议）服务，由于采取替换HTTP头文件IP地址的方式来进行请求转发，所以后端云服务器看到的访问IP是SLB系统的本地IP而不是实际来访者的真实IP。所以系统支持用户采用X-Forwarded-For的方式获取访问者真实IP，前提是用户必须在配置7层（HTTP协议）服务监听时开启了“获取真实访问IP”功能。 先知计划为企业提供的是为企业收集情报（漏洞）服务。 操作系统级别监控指标包含内存使用率、平均负载、磁盘 IO 读/写、磁盘使用率、TCP 连接数、进程总数等。请注意CPU使用率不属于操作系统级别监控。 安全组只允许公网TCP 80端口和25端口访问并不能达到禁止用户pingECS是否在线的效果。选项”先将ECS IP解析到一个不常用的四级域名，然后将对外推广的域名通过CNAME指向以上四级域名”可以隐藏了ECS的真实IP地址，所以一定程度上可以防止ECS被ping到。 阿里云云安全中心提供的是一种SaaS类型的服务。 加速域名日志中记录的流量数据，是我们应用层日志统计出的流量，但是实际产生的网络流量却要比应用层统计到的流量要高出7%-15%；这个主要的原因有两个：1、TCP/IP包头的消耗，2、TCP重传。 在创建伸缩组时候可以配置RDS的实例，但是并不能指定RDS的实例的配置，RDS的实例的配置需要在购买RDS的时候来指定的。在创建伸缩组时只支持指定ECS的指定配置，不支持指定ECS的最大配置。 OSS中，用户只有通过URL签名或者匿名访问Object时，才会做防盗链验证。请求Header中有Authorization字段时，不会做防盗链验证。 若SLB未结合弹性伸缩一起使用，后端ECS实例宕机SLB会将其隔离而不是移除（不再给其分发请求），直到该ECS健康检查恢复正常后，还会从隔离区移出，继续分发请求。 回滚磁盘必须要求云盘必须已经 挂载到某台ECS实例上，而且已经 停止实例。 SLB中的四层负载均衡采用开源软件LVS，并根据云计算的需求对其官方进行了定制化，下列哪些属于阿里云在LVS官方问题上进行了定制化？答：DDoS攻击防御功能+采用LVS集群部署方式+优化keepalived性能 简单上传适用情况：上传文件最大不超过5GB+在上传单个文件过程中可以携带meta信息+基于PUT方式或POST方式的http请求上传。简单上传指的是使用OSS API中的PutObject方法上传单个文件（Object）。简单上传不需要支持断点上传，也就是断点续传功能一定不是简单上传。简单上传适用于一次HTTP请求交互即可完成上传的场景，比如小文件（小于5GB）的上传。上传文件时支持设置Object Meta。 阿里云弹性伸缩（Auto Scaling）的伸缩配置（Scaling Configuration）中支持设置的镜像类型包含自定义镜像+公共镜像+共享镜像。弹性伸缩设置ECS配置的时候，不支持使用云市场的镜像，主要是因为部分云市场的镜像是收费的，阿里云控制台没有云市场镜像的选项。 在删除阿里云弹性伸缩(Auto Scaling)的伸缩组时有两种模式：强制删除和非强制删除。在非强制删除模式下，必须满足“伸缩组没有任何伸缩活动正在执行+伸缩组当前的ECS实例数量为0”的条件才可以删除。 负载均衡SLB不提供CNAME地址，需要直接将域名解析到负载约衡SLB提供的服务IP地址上。 CDN节点系统关键组件：LVS做四层均衡负载；Tengine做七层负载均衡；Swift做HTTP缓存。 .阿里云负载均衡服务SLB提供性能保障型实例，该类型实例提供了可保障的性能指标, 那么它的关键指标有：最大连接数-Max Connection、每秒新建连接数-Connection Per Second (CPS)、每秒查询数-Query Per Second (QPS)。 一台ECS实例最多能挂载16块云盘作数据盘用，且云盘只能挂载到同一地域下同一可用区内的实例上，不能跨可用区挂载。 自定义监控：用户可以对自己关心的业务进行监控，将采集到监控数据上报至云监控，由云监控进行数据的处理，并根据结果进行报警。自定义的监控项的数量是没有上限的，上报监控数据的程序可以部署在阿里云的云服务器以外的机器。 虽然用户是可以自己上传本地镜像的，但是上传的本地镜像需要用户把本地镜像制作为自定义镜像，所以在创建云服务器ECS的时候是没有用户上传的镜像的选项的。 云服务器ECS系统资源监控DashBoard页面内容有CPU使用率+磁盘IO+网络带宽。DashBoard不支持对TPS的监控 伸缩组中手工加入ECS实例的步骤：1）判断伸缩组的健康状态、边界条件和 ECS 实例的状态、类型。2）分配 ActivityId 和执行伸缩活动。3）加入 ECS 实例。4）修改 Total Capacity。5）添加 RDS 白名单。6）挂载负载均衡，将权重设为当前伸缩组中已激活的伸缩配置上指定的“负载均衡权重”。此处使用了伸缩配置上指定的“负载均衡权重”。7）伸缩活动完成，启动 cooldown。 “访客真实IP”保存在HTTP协议的“X—Forwarded-For”Header中，可以在Apache和Nginx的自定义LOG中直接获取到 ， Windows平台下，如果使用IIS，需要安装一个“F5XForwardedFor”的扩展模块，才能在日志里看到“访客真实IP” 。 CDN仅支持PEM格式的证书，更新HTTPS证书1分钟后全网生效。 查看CDN节点是否生效：方法一：通过ping或dig的方式查看所添加的加速域名；方法二：在CDN控制台中查看节点是否生效；方法三：获取对应加速域名资源的response查看节点是否生效。 OSS 提供服务端加密和客户端加密。服务端加密方式：使用KMS托管密钥进行加解密，使用OSS完全托管加密。客户端加密方式：使用KMS托管用户主密钥，使用用户自主管理密钥。客户端不支持OSS完全托管加密。 OSS跨区域复制可以指定文件前缀进行同步，可以进行写同步（增/改），可以进行增/删/改同步。不可以指定文件后缀进行同步。 网络ACL的规则是无状态的，即设置入方向规则的允许请求后，需要同时设置相应的出方向规则，否则可能会导致请求无法响应。网络ACL无任何规则时，会拒绝所有出入方向的访问。新创建的网络ACL，默认会在出方向和入方向分别生成一条规则，表示允许所有出、入方向流量。您可以删除默认规则。 阿里云的交换机是三层交换机，绑定路由表。 SLB后端ECS实例的两种状态区别：保护状态和备用状态的相同点是都会实现“弹性伸缩不会检查ECS实例健康状态,也不会释放ECS实例”，不同点是保护状态的ECS还会继续提供服务，备用状态的ECS的权重会被设置为0，不对外提供服务。 OSS的存储费用支持包年包月和按量付费，API请求费用只有按量付费。 普通快照是存储在OSS中，属于跨地域容灾，创建速度慢，本地快照是存储在云盘中，属于同城容灾，创建速度快。自动快照主要强调周期性，固定时间等，对时间有要求。 实例健康状况侧重于监视实例的健康状态，如异常活动、网络和软硬件问题，可以配合云监控使用。自助诊断系统是在你的资源出现问题的时候可以一键诊断，提交结果等。 性能保障型实例的关键指标：最大连接数，每秒新建连接数，每秒查询数。 专有网络VPC（Virtual Private Cloud）的高级功能包括网络ACL、自定义路由表和DHCP选项集。如果VPC中创建了ECS实例规格族限制中的ECS实例，则该VPC不支持使用高级功能。 伸缩组支持关联传统型负载均衡CLB（原SLB）实例，前提条件：1）您持有一个或多个处于运行中状态的CLB实例。2）CLB实例和伸缩组必须位于同一地域。3）如果CLB实例和伸缩组的网络类型均为专有网络，则必须位于同一专有网络。4）当CLB实例的网络类型为经典网络，伸缩组的网络类型为专有网络时，如果CLB实例的后端服务器组中包含专有网络ECS实例，该ECS实例必须与伸缩组位于同一专有网络。5）CLB实例配置至少一个监听。6）CLB实例必须开启健康检查。 TCP监听支持HTTP和TCP两种健康检查方式：1）TCP协议健康检查通过发送SYN握手报文，检测服务器端口是否存活。2）HTTP协议健康检查通过发送HEAD或GET请求，模拟浏览器的访问行为来检查服务器应用是否健康。 交换机创建完成之后,无法修改CIDRBlock,且新建交换机所使用的CIDRBlock不可以与已经存在的交换机的CIDRBlock重复。 伸缩配置包括：标签，SSH密钥对，实例RAM角色，实例自定义数据 伸缩组具有以下三种状态:Active、Inacitve和Deleting，伸缩配置具有以下两种状态:Active和Inacitve。 OSS图片处理限制：1）图片格式只支持JPG、PNG、BMP、GIF、WebP、TIFF。2）原图大小不能超过20 MB。3）图片旋转对应的原图高或者宽不能超过4,096 px。4）原图高或者宽不能超过30,000 px。","categories":[{"name":"云原生","slug":"云原生","permalink":"http://wht6.github.io/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"}],"tags":[{"name":"阿里云","slug":"阿里云","permalink":"http://wht6.github.io/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"}]},{"title":"阿里云的一些重要概念","slug":"阿里云的一些重要概念","date":"2022-01-09T02:00:00.000Z","updated":"2022-03-26T02:42:45.847Z","comments":true,"path":"posts/da88.html","link":"","permalink":"http://wht6.github.io/posts/da88.html","excerpt":"","text":"弹性计算服务器ECSECS是阿里云的云服务器，单个云服务器被称为实例，一个实例即一个虚拟服务器，具有CPU、内存、操作系统、网络和磁盘。 地域和可用区阿里云在不同的地域建设许多机房，每个地域划分成多个可用区，一个可用区可以看成一个机房。地域内的所有可用区内网互联，地域之间通信要经过公网。跨地域部署会有网络延迟和经费问题，所以ECS的虚拟化是针对单个地域的，可用区之间可以迁移，但是ECS创建后不可以跨地域。 同样，阿里云的负载均衡、对象存储和数据库服务都是针对单个地域部署的，负载均衡只能在地域内做，对象存储创建好的Budget不能切换地域，对象存储和数据库跨地域迁移都要计费。 块存储块存储可以说就是虚拟磁盘，分为本地盘和云盘，本地盘应该就是物理服务器自带的磁盘虚拟化的结果，而云盘是将一堆磁盘放一起用来提供存储服务，类似于NAS。本地盘直接连接在主板上，所以访问速度快，云盘即使是内网连接也需要经过网线，所以稍微慢一些。 阿里云还提供了一个共享块存储，支持多个ECS实例并发读写访问，这个主要用于共享访问的场景，可以防止单点故障。 云盘分为普通云盘、高效云盘、SSD云盘、ESSD云盘，速度由慢到快。 安全组安全组是一组实例集合，这些实例使用同样规则的虚拟防火墙，用于网络访问控制。创建一个安全组，配置一套ACL规则，将实例加入该安全组，那么这个实例的就会配置一个具有上述规则的防火墙。 虚拟专有网络VPCVPC底层用到的是网络虚拟化和软件定义网络SDN，目的是提供隔离的网络环境。每一个VPC都由一个私有网段、一个路由器和至少一个交换机组成。 弹性IP阿里云的弹性IP简称EIP，购买之后会得到一个公网IP，弹性提现在该公网IP不与实例绑定而是与账号绑定，就是可以把该公网IP切换到任意一个实例上，具体怎么做到的咱也不知道。 云企业网云企业网（Cloud Enterprise Network）的作用就是提供一个跨越公网的安全环境，比如两个不同地域的VPC的通信，以及混合云的部署。VPN网关也可以做到。 负载均衡SLB 监听监听支持的协议有TCP、UDP、HTTP和HTTPS，目的是监听这些基于这四种协议的请求，比如HTTP的请求，每次监听到HTTP的一个请求，就做一次转发，基于配置负载均衡的策略转发到特定的服务器。还可以根据cookie还进行会话保持，将相同cookie的请求转发到同一个服务器。 弹性伸缩AS使用弹性伸缩（Auto Scaling）可以根据业务需求和策略设置伸缩规则，在业务需求增长时自动为您增加ECS实例以保证服务端计算能力，在业务需求下降时自动减少ECS实例数量以节省成本。例如触发条件：平均vCPU使用率 &gt; 80%时，添加一台实例。平均vCPU使用率 &lt; 30%时，减少一台实例。 伸缩组这个概念有点类似于安全组，也是将一组实例划分成一个伸缩组，但是安全组中的安全规则是针对每个实例的，而伸缩组的规则是针对所有实例的。伸缩组包括伸缩配置，伸缩规则和伸缩活动，伸缩配置就是针对每个实例的配置，伸缩规则是具体扩展或收缩的规则，伸缩活动就是实际伸缩情况，伸缩规则成功触发后就会产生一条伸缩活动。 除了自定义规则来触发伸缩之外，还可以任务触发，如定时任务，报警任务。 对象存储服务OSS对象（Object）由元信息(Metadata)、用户数据(Data)和文件名(Key)组成。对象由存储空间内部唯一的Key来标识。对象元信息是是一组键值对，表示了对象的一些属性，比如最后修改时间、大小等信息，同时也可以在元信息中存储一些自定义的信息。 存储空间存储空间（Budget）是用户用于存储对象(Object)的容器，所有的对象都必须隶属于某个存储空间。存储空间具有各种配置属性，包括地域、访问权限、存储类型等。每个存储空间属于某个地域，存储空间创建之后不能修改地域。 生命周期对象的生命周期是从上传到删除为止。可以创建生命周期规则来批量转换或批量删除。 访问域名和访问密钥Endpoint（访问域名）：OSS对外服务的访问域名。不同地域下的域名不一样。内网和外网的域名也不一样。 AccessKey（访问密钥）：用于身份验证。 下载方式OSS中的对象可以在控制台下载，也可以用URL下载。 防盗链 对象存储OSS是按照使用量收费的服务，为了减少存储于OSS的数据被其他人盗链而产生额外费用，OSS支持设置基于HTTP和HTTPS header中表头字段Referer的防盗链方法。可以通过控制台为存储空间设置Referer字段的白名单和是否允许Refer字段为空的请求访问。 参数：Refer白名单：仅允许指定域名访问OSS资源。是否允许空Referer：如果不允许空Referer，则只有HTTP和HTTPS header中包含Referer字段的请求才能访问OSS资源。 云数据库RDS阿里云关系型数据库(Relational Database Service，简称RDS)是一种稳定可靠的、可弹性伸缩的在线数据库服务。支持市场上主流的数据库系统和引擎。 读写分离主实例和只读实例都有独立的连接地址，当开启读写分离功能后，系统会额外提供一个读写分离地址，联动主实例及其下的所有只读实例，实现了自动的读写分离。应用程序连接读写分离地址，读写分离模块会自动将写入请求发往主实例，将读请求发往只读实例。 主要满足特定的业务场景，有些业务场景写很少，大部分都是读，读写分离主要适用于这种场景。 内容分发网络CDN阿里云内容分发网络(Alibaba Cloud Content Delivery Network，简称CDN)将用户源站资源缓存到阿里云遍布全球的加速节点上。当终端用户请求访问和获取这些资源时，无需回源，系统将就近调用CDN节点上已经缓存的资源。 加速域名你自己首先有个域名，开通CDN服务后，会得到一个加速域名，然后你在域名服务器上添加CNAME记录，将你的域名映射到加速域名，那么别人访问你的域名的时候就会解析到CDN服务器，CDN服务器再通过一定的策略将距离用户最近的CDN节点的IP地址返回。 回源策略回源就是不去访问CDN节点，而去直接访问你的服务器。回源策略是什么情况下回源，回源的时候访问哪个IP（对于你有多个IP域名的情况）。","categories":[{"name":"云原生","slug":"云原生","permalink":"http://wht6.github.io/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"}],"tags":[{"name":"阿里云","slug":"阿里云","permalink":"http://wht6.github.io/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"}]},{"title":"Linux系统配置","slug":"Linux系统配置","date":"2021-12-30T02:00:00.000Z","updated":"2022-10-16T08:12:59.318Z","comments":true,"path":"posts/a302.html","link":"","permalink":"http://wht6.github.io/posts/a302.html","excerpt":"","text":"环境变量 环境变量的配置文件： /etc/profile和/etc/profile.d/，作用域：所有用户。 ~/.bash_profile（~/.profile），作用域：当前用户。 /etc/bashrc，作用域：所有用户shell。 ~/.bashrc，作用域：当前用户shell。 生效环境变量的方式： source &lt;配置文件&gt; . &lt;配置文件&gt; 重新登录或重启shell systemd Systemd是一个Linux操作系统下的系统和服务管理器。 systemctl命令管理systemd的资源Unit： start：立刻启动后面接的 unit。 stop：立刻关闭后面接的 unit。 restart：立刻关闭后启动后面接的 unit，亦即执行 stop 再 start 的意思。 reload：不关闭 unit 的情况下，重新载入配置文件，让设置生效。 enable：设置下次开机时，后面接的 unit 会被启动。 disable：设置下次开机时，后面接的 unit 不会被启动。 status：目前后面接的这个 unit 的状态，会列出有没有正在执行、开机时是否启动等信息。 is-active：目前有没有正在运行中。is-enable：开机时有没有默认要启用这个 unit。 kill ：向运行 unit 的进程发送信号。 show：列出 unit 的配置。 mask：注销 unit，注销后你就无法启动这个 unit 了。 unmask：取消对 unit 的注销。 系统启动级别 init一共有7个级别： 0：停机或者关机（默认运行级别不能设置为0） 1：单用户模式，只root用户进行维护 2：多用户模式，不支持NFS 3：完整的多用户模式（标准的运行级别），有NFS，登陆后进入命令行控制台。 4：未使用，系统保留 5：图形化模式（即图形界面） 6：重启（默认运行级别不能设置为6） 12345678# Default runlevel. The runlevels used by RHS are: # 0 - halt (Do NOT set initdefault to this) # 1 - Single user mode # 2 - Multiuser, without NFS (The same as 3, if you do not havenetworking) # 3 - Full multiuser mode # 4 - unused # 5 - X11 # 6 - reboot (Do NOT set initdefault to this)","categories":[{"name":"Linux","slug":"Linux","permalink":"http://wht6.github.io/categories/Linux/"}],"tags":[{"name":"系统配置","slug":"系统配置","permalink":"http://wht6.github.io/tags/%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE/"}]},{"title":"LAMP服务配置","slug":"Linux网站服务","date":"2021-12-28T02:00:00.000Z","updated":"2022-10-06T12:17:52.723Z","comments":true,"path":"posts/c49a.html","link":"","permalink":"http://wht6.github.io/posts/c49a.html","excerpt":"","text":"网站架构LAMP：Linux+Apache+MySQL+PHP。 Apache对应的软件包是httpd，http协议端口80，https协议端口443。 配置文件：/etc/httpd/conf/httpd.conf，子配置文件：/etc/httpd/conf.d/*.conf。 123456# 启动Apachesystemctl start httpdsystemctl status httpdsystemctl enable httpdsystemctl stop firewalldsetenforce 0 firewalld是保护互联网对服务器的影响，selinux是保护服务器内部程序对内部文件的访问，selinux限制了每个程序只能访问特定的文件夹。getenforce，查看selinux状态（enforcing开启，permissive放行（临时关闭），disable永久关闭）。永久关闭方法：vim /etc/selinux/config，enforcing改成disable。 Apache的网页默认主目录是/var/www/html/（网站源码默认位置） Apache支持虚拟主机（VirtualHost），即在一台物理服务器上运行多个网站。配置过程如下： 1234567891011121314151617181920212223准备网站源码（网页）目录：mkdir /var/www/html/a.org，vim /var/www/html/a.org/index.html。创建a.org的网站配置文件，vim /etc/httpd/conf.d/a.org.conf。写入：&lt;VirtualHost *:80\\&gt; ServerName www.a.org DocumentRoot /var/www/html/a.org&lt;/VirtualHost&gt;httpd -t，检查服务器是否配置有错误，syntax ok表示没有语法错误。重启更新配置：systemctl restart httpd。客户端上需要配置域名解析，vim /etc/hosts ，写入：192.168.142.138 www.a.org。然后用浏览器访问www.a.org或用elink访问www.a.org。配置完成第一个网站后可以继续配置第二个网站，mkdir /b.org，vim /b.org/index.html。创建b.org的网站配置文件，vim /etc/httpd/conf.d/b.org.conf。写入：(因为路径不在网页主目录下面，所以需要配置Directory选项，对目录进行授权)&lt;VirtualHost *:80\\&gt; ServerName www.b.org DocumentRoot /b.org&lt;/VirtualHost&gt;&lt;Directory &quot;/b.org&quot;&gt; Require all granted&lt;/Directory&gt;httpd -t，检查服务器是否配置有错误，syntax ok表示没有语法错误。重启更新配置：systemctl restart httpd。客户端上需要配置域名解析，vim /etc/hosts ，写入：192.168.142.138 www.b.org。然后用浏览器访问www.b.org或用elink访问www.b.org。 通过discuz配置配置简易的动态站点： 12345678910111213141516安装LAMP(用mariadb代替MySQL，gd是图形库)：yum install -y httpd mariadb-server mariadb php php-mysql gd php-gd。启动httpd和mariaDB，systemctl start httpd mariaDB。开机自启：systemctl enable httpd mariaDB。wget下载开源论坛源码discuz。新建一个文件夹，并把压缩包解压进去。mkdir -p /webroot/discuz，unzip xxx.zip，cp -rf upload/* /webroot/discuz/，给Apache授权，chown -R apache.apache /webroot/discuz。vim /etc/httpd/conf.d/discuz.conf。写入：&lt;VirtualHost *:80\\&gt; ServerName www.discuz.com DocumentRoot /webroot/discuz&lt;/VirtualHost&gt;&lt;Directory &quot;/webroot/discuz&quot;&gt; Require all granted&lt;/Directory&gt;httpd -t，检查服务器是否配置有错误，syntax ok表示没有语法错误。重启更新配置：systemctl restart httpd。准备数据库：命令行输入mysql进入数据库，输入exit退出数据库。create database discuz;， show database; ，\\q退出。添加域名解析，vim /etc/hosts ，写入：192.168.142.138 www.discuz.com。然后用浏览器访问www.discuz.com，出现discuz的安装向导，然后一步一步安装discuz，同意，全新安装，配置数据库（localhost,discuz,root,没有密码，创建站长帐号）","categories":[{"name":"Linux","slug":"Linux","permalink":"http://wht6.github.io/categories/Linux/"}],"tags":[{"name":"网站搭建","slug":"网站搭建","permalink":"http://wht6.github.io/tags/%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/"}]},{"title":"Linux网络管理","slug":"Linux网络管理","date":"2021-11-11T02:00:00.000Z","updated":"2022-10-06T12:32:48.547Z","comments":true,"path":"posts/f714.html","link":"","permalink":"http://wht6.github.io/posts/f714.html","excerpt":"","text":"Linux网络配置 传统上，Linux网络接口被枚举为eth0、eth1、eth2，eth表示以太网。 现在的命令规则是以太网以en开头，WLAN接口以wl开头，WWLAN接口以ww开头，接着下一个字符表示适配器的类型，o表示板载，s表示热插拔，p表示PCI。 /etc/sysconfig/network-scripts目录中是关于网卡的配置文件。 网络管理器（NetworkManager）是一个动态网络的控制器与配置系统，作用是当网络设备可用时，保持设备和连接开启并激活。systemctl status NetworkManager是查看NetworkManager的运行状态，network是NetworkManager的一个子程序，查看network的状态用systemctl status network。 查看主机上的所有网卡：nmcli device。n是网络，m是管理。 用命令打开网卡配置的图形界面：nm-connection-editor，简易图形界面：nmtui。 网卡配置文件的字段含义： ONBOOT表示是否启动该设备，如果为no表示禁用该设备。 BOOTPROTO表示启动的协议，协议有三种情况，dhcp自动、none手动和static静态。 IPADDR、NETMASK表示手动配置IP地址和子网掩码。 GATEWAY表示手动配置网关地址。 DNS表示域名服务器地址（正常是DNS1=xxx，DNS2=xxx）。 NAME是网卡的名字。 UUID是网卡的ID。 查看IP。ip a。 查看主机名。hostname。 配置主机名。hostnamectl set-hostname wht.example.com，主机名是写在文件/etc/hostname中的，所以也可以用vim /etc/hostname来修改主机名。配置之后重启生效。 查看网关。ip route。 查看邻居。ip neigh。 交换机和路由器配置 下面以CPT（Cisco Packet Tracer）为例，介绍交换机2960的配置过程。 VLAN（虚拟局域网）： 配置的方法是首先创建VLAN，VLAN的序号是0~1024。直接输入命令VLAN 10就创建了一个序号为10的VLAN。然后是接口与VLAN关联，F0/1~F0/10 VLAN10，表示将接口F0/1~F0/10加入VLAN 10。 具体步骤： enable进入特权模式(exit退出)， config t进入配置模式(exit退出)， VLAN 10创建VLAN， interface fastethernet0/1(简写int f0/1)进入接口， switch access VLAN 10将接口加入VLAN。 特权模式下show vlan可以查看vlan。 （一个从未创建vlan的交换机连接的所有主机默认属于VLAN 1。） VLAN如果需要跨越交换机，需要设置trunk(干道)，truck是连接两台交换机的一条线缆，需要将线缆两端的接口配置为trunk口，trunk可以识别帧中的vlan标签，发送到确定的vlan。 配置trunk： int f0/3进入接口， 然后switch mode trunk。 (vlan标签只存在于trunk线路，从交换机的trunk口出加vlan标签，进入一台交换机的turnk口，先识别vlan标签，再去除vlan标签。) 路由器需要给接口配置本网段IP地址，路由器接口默认不通电，需要手动打开。命令行输入show ip route查看路由表。 添加静态路由信息：ip route 192.168.2.0 255.255.255.0 f0/1。 域名解析 hosts文件：Linux：/etc/hosts，Windows：C:\\Windows\\System32\\drivers\\etc\\hosts。hosts文件是Linux系统上一个负责ip地址与域名快速解析的文件，hosts文件是DNS没出现之前就已经使用的方式。 优先级：DNS缓存&gt;hosts文件&gt;DNS服务。DNS缓存在内存中，关机就没。 配置DNS服务器地址，可以在 /etc/sysconfig/network-scripts/ifcfg-ens33里面配置，也可以在/etc/resolv.conf配置（真正起作用的是/etc/resolv.conf）。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://wht6.github.io/categories/Linux/"}],"tags":[{"name":"网络配置","slug":"网络配置","permalink":"http://wht6.github.io/tags/%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/"}]},{"title":"Linux日志管理","slug":"Linux日志管理","date":"2021-11-05T02:00:00.000Z","updated":"2022-10-07T06:28:36.068Z","comments":true,"path":"posts/5580.html","link":"","permalink":"http://wht6.github.io/posts/5580.html","excerpt":"","text":"rsyslogd CentOS中rsyslogd负责对Linux中的日志进行管理，配置文件为 /etc/rsyslog.conf（主配置）和/etc/sysconfig/rsyslog（属性配置） Linux中的日志文件统一存放在/var/log下 Rsyslog在/etc/rsyslog.conf 文件中配置日志过滤规则： 格式： filter的格式：. Facility(设施)，用来定义日志消息的来源，不同的字段表示不同类型的日志。类型如下： SYSLOG（日志系统自身消息） AUTH（认证系统消息） AUTHPRIV（权限系统消息） CRON（定时任务消息） MAIL（邮件系统消息） USER（用户级消息） DEAMON（系统服务消息） FTP（文件服务器） KERN（内核消息） LPR（打印机） LOCAL0 到 LOCAL7（用户自定义设备） 看一个程序属于什么设施，例如，grep Facility /etc/ssh/sshd_config Priority(日志级别)，用于定义不同日志的优先级。级别从高到低： EMERG（紧急，致命） ALERT（报警，需立即处理） CRIT（严重行为） ERR（错误） WARNING（警告） NOTICE（正常信息，但是较为重要） INFO（正常信息） DEBUG（调试信息） 符号： =表明只有该优先级的消息会被捕获 !表明除了该优先级的消息之外的优先级会被捕获 *符号表示任意或所有 例如： kern.* 表示选择所有优先级的内核日志 mail.crit 表示选择所有mail 的优先级高于crit的日志 cron.!info,!debug 表示选择除了 info 和 debug 优先级的 cron 日志 aciton常用： 用户名，表示将日志发送到特定的用户。例如：kern.=crit user1 路径，表示日志写入的位置。例如：cron.* /var/log/cron.log 网络传输日志，表示将日志发送出去。@表示用UDP发送，@@表示用TCP发送，后边根IP和端口。 ~，表示丢弃日志。 完整格式： ;&lt;模板名&gt; &lt;模板名&gt;表示使用该模板格式化日志。 logrotate logrotate是用于日志切割和轮替的日志管理工具。 日志记录了程序产生的各种信息，但是磁盘空间有限，为了节省空间和方便，日志文件通常需要按时间或大小等分成多份，删除时间久远的日志文件。日志轮转就是只记录最近一段时间的发生的事，系统会根据你的要求删除你不想要的日志。 logrotate的主配置文件/etc/logrotate.conf，子配置目录/etc/logrotate.d/* 日志轮替规则的字段含义： weekly表示按周轮转，monthly表示按月轮转。 rotate 4表示保留4份日志 create表示轮转后创建新文件 dateext表示以时间作为文件的扩展名 compress表示压缩 missingok表示丢失不执行 notifempty表示空文件不轮转 maxsize 30k表示只要到了30k就轮转。 举例： 12345678910 /var/log/wtmp&#123; monthly minsize=1M create 0664 root utmp rotate 1 &#125;# 对单个日志文件的轮转规则配置# 日志文件wtmp按月且最小达到1M才轮转# create是创建新日志文件并设置文件权限，其中root是属主，utmp是属组，rotate 1表示只保留一份 logrotate 是基于 crontab 运行的，时间点满足时进行固定的日志切割和轮替操作。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://wht6.github.io/categories/Linux/"}],"tags":[{"name":"日志","slug":"日志","permalink":"http://wht6.github.io/tags/%E6%97%A5%E5%BF%97/"}]},{"title":"Linux计划任务","slug":"Linux计划任务","date":"2021-11-02T02:00:00.000Z","updated":"2022-10-07T08:19:04.397Z","comments":true,"path":"posts/6b0e.html","link":"","permalink":"http://wht6.github.io/posts/6b0e.html","excerpt":"","text":"一次性计划任务 at工具用于执行一次性的计划任务。 命令格式：at TIMESPEC时间示例： now +5min，从现在开始往后5分钟。 teatime tomorrow，teatime是16:00，明天的下午茶。 noon +4day，第四天中午。 5pm august 3 2022、4:00 2020-11-27等。 使用方法： 1）输入at now +2min回车，此时进入了一个交互命令模式。 2）输入2min后要执行的命令回车。如果有第二条命令继续输入并回车。 3）Ctrl+D，输入完毕，提交任务。 查询任务。输入atq查询。第一列是任务编号，第二列是时间，第三列是执行的用户。 循环任务 crontab（循环任务表）工具用于执行循环的计划任务。 所有的crontab任务存储在/var/spool/cron目录下。 创建循环任务：crontab -e（为当前用户创建，如果指定其他用户加-u username） 然后会自动打开一个文件，在这个文件中编辑命令，一条命令占一行，编辑完成保存。 命令格式如下图所示： 时间字段举例： 5 1 15 3 *标识每年的3月15日1时5分都会执行 5 1 15 * *表示每月的15日1时5分都会执行 5 1 * * *表示每天的1时5分都会执行 5 * * * *表示每小时的5分都会执行 */5 * * * *表示每隔5分钟执行一次。 0 2 1,4,6 * *表示每月的1日4日6日的2点执行。 0 2 5-9 * *表示每月的5日至9日的2点执行。 * * * * *表示每分钟执行一次。0 * * * *表示整点执行。 特别的： 0 2 * * 5表示每周5的2点执行（周里边0和7表示周日） 0 2 * 6 5表示6月每周五的2点执行。 查看任务计划表：crontab -l。或者crontab -u username -l。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://wht6.github.io/categories/Linux/"}],"tags":[{"name":"计划任务","slug":"计划任务","permalink":"http://wht6.github.io/tags/%E8%AE%A1%E5%88%92%E4%BB%BB%E5%8A%A1/"}]},{"title":"Linux软件包管理","slug":"Linux软件安装","date":"2021-11-01T02:00:00.000Z","updated":"2022-10-07T02:49:45.298Z","comments":true,"path":"posts/6b92.html","link":"","permalink":"http://wht6.github.io/posts/6b92.html","excerpt":"","text":"RPM包管理 RPM Package Manager（原Ret Hat Package Manager，是一个递归缩写）。RPM包是二进制包，无需编译，可直接使用。缺点是无法进行个人设置，也就是无法修改。 命名格式：软件包名+版本号+发布平台+系统平台+文件后缀。zip-3.0-11.el7.x86_64.rpm，3.0-11是版本号，el7是发布平台enterprise Linux 7。 RPM包管理工具：YUM工具和RPM工具。 yum YUM全称yellow dog updater modified，基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，能够自动处理依赖关系，并且一次安装所有依赖的软件包。 yum仓库配置： 配置本地仓库。目的：通知Linux服务器，通过本机的系统光盘获得软件包，并安装软件。 12345678910111213首先找到YUM仓库配置目录，/etc/yum.repos.d/。然后新建一个repo文件，vim /ect/yum.repos.d/dvd.repo，下面是文件中需要写入的内容。 [dvd] name=dvd123 baseurl=file:///mnt/cdrom gpgcheck=0 enable=1方括号是库的名字，name是描述，baseurl是下载地址，file://表示从本机上找，/mnt/cdrom表示路径。gpgcheck是包校验，等于0表示关闭校验。enable=1表示开启该源。将光盘连接到服务器上。准备文件夹，mkdir /mnt/cdrom。挂载光盘，mount /dev/cdrom /mnt/cdrom。(这里的这个光盘是centos的镜像文件，在虚拟机创建系统时写入了虚拟光盘)挂载之后你会看到/mnt/cdrom下有一个Packages的文件夹，这个文件夹下有四千多个RPM包。 配置网络仓库： 12345配置网络源。上面YUM是配置的本地源，但是有时候本地源没有就需要配置网络的上的源。1）清理原有YUM配置。mv * /tmp。2）找到阿里巴巴开源镜像站官网配置，找centos和epel，里面一个wget获取repo的命令。3）下载YUM仓库。一共两个包一个基础包，centos-base.repo，一个扩展包，epel.repo。4）更新YUM，执行安装。yum makecache。查看安装的源，yum repolist。 yum安装：yum install -y &lt;包名&gt; yum重装：yum reinstall -y &lt;包名&gt; yum更新软件：yum update -y &lt;包名&gt; yum列出相关包：yum list &lt;包名&gt;（如果这个包已经安装，那么前边会有一个@） yum卸载：yum remove -y &lt;包名&gt; 清除yum缓存的软件包：yum clean all 查看yum仓库：yum repolist all rpm rpm是从本地安装，需要提前准备好安装包和相关依赖。 rpm安装：rpm -ivh &lt;包名&gt;.rpm（i是install，v是可视，h是百分比） rpm卸载：rpm -evh &lt;包名&gt;.rpm rpm查询： rpm -qf [程序文件]：查询某个程序属于哪个软件包。 rpm -ql [包名]：查询软件包的文件列表。 rpm -qi [包名]：查询软件包的详细信息。 rpm -qc [包名]：查询软件包的配置文件。 源码包管理 源码包，source code，需要经过GCC，C++编译环境编译才能运行，可以进行个人设置，修改源码参数。缺点是配置过程复杂。 源码包命令格式：包名+版本号+压缩格式，如nginx-1.81.tar.gz。 下面以Tengine的安装为例展示源码包的安装过程。 下载Tengine包，解压。tar xf tengine-2.3.3.tar.gz。进入cd tengine-2.3.3。 配置。可以配置也可以不配置。用到./configure。具体配置命令：./configure —user=www —group=www —prefix=/usr/local/nginx，(最后是安装位置) 编译，将源码翻译成二进制语言。make。 安装。make install。 测试。如果开启了httpd先把它关了，因为它会和Tengine起冲突。systemctl stop httpd。运行主程序：/usr/local/nginx/sbin/nginx。访问验证：http://127.0.0.1（如果无法访问，关闭防火墙systemctl stop firewalld。）","categories":[{"name":"Linux","slug":"Linux","permalink":"http://wht6.github.io/categories/Linux/"}],"tags":[{"name":"包管理","slug":"包管理","permalink":"http://wht6.github.io/tags/%E5%8C%85%E7%AE%A1%E7%90%86/"}]},{"title":"Linux磁盘管理","slug":"Linux磁盘管理","date":"2021-10-29T02:00:00.000Z","updated":"2022-10-07T10:16:25.389Z","comments":true,"path":"posts/9232.html","link":"","permalink":"http://wht6.github.io/posts/9232.html","excerpt":"","text":"/dev是设备文件目录，里面存放这块设备文件，例如，sda，s代表SATA就是串口（SCSI），d代表disk磁盘，a表示第一块，第二块、第三块就是sdb、sdc。 两种常用的分区方式：MBR和GPT。MBR叫主引导记录，位于磁盘最前边，记录分区的结构信息。MBR支持的最大磁盘容量是2T，最多支持划分4个主分区。GPT支持大于2T的硬盘，支持128个主分区。 磁盘分区 查看所有磁盘分区：lsblk或df -hT 查看某个磁盘分区：fdisk -l /dev/sdb fdisk是基于MBR分区的分区工具。 MBR最多支持4个主分区，超过4个分区就要用到逻辑分区。操作系统只能装在主分区，而不能装在逻辑分区。 划分逻辑的分区的方法：就要放弃一个主分区，一般选第四个主分区，然后把它划分为扩展分区。再将扩展分区划分成N个逻辑分区。扩展分区只是抽象的分区，包括多个逻辑分区，逻辑分区是真正可以格式化并存数据的地方。 fidisk创建分区的过程： 启动分区工具。fdisk /dev/sdb。进入会话模式： 1）欢迎界面，n（# new）回车开始分区。 2）问你是划分主分区还是扩展分区，p回车选择主分区。 3）设置分区号。MBR分区只能划分四个分区。从小到大设置，方便理解。1回车。 4）问你起始扇区的位置，每次默认都是从上个分区结束的地方开始划分，默认回车。【可以发现分区的时候并不是从0号扇区开始分的，而是从2048号扇区开始分的，因为前2048个扇区记录了MBR的信息】 5）问你分区结束的扇区。可以写扇区号，但是没必要，有更简单的方法。直接写+2G回车，就是给分区分2G，它会自动帮你算扇区。 6）告诉你完成了2G分区的记录，但未生效。w回车，表示确认保存分区信息并将分区列表写入磁盘，然后自动会退出分区工具。也可以继续n回车，表示继续进行分区。（没有分区的部分不能使用） 刷新分区表：partprobe /dev/sdb。 如果需要创建逻辑分区： 进入fdisk后，按n，再按e，选择所有剩余空间划分成扩展分区。再按n，就直接问你逻辑分区的起始和结束了，这时候再设置逻辑分区。 创建文件系统 创建文件系统（分区格式化）：mkfs.ext4 分区名:/dev/sdb1 mk是make，fs是文件系统，ext4是一种文件系统的类型（扩展文件系统第四代）。微软的文件系统叫NTFS，早期是FAT16、FAT32。 挂载分区 临时挂载分区：mount -t ext4 分区名:/dev/sdb1 &lt;挂载目录&gt;，-t是文件系统类型（可以省略，会自动识别系统文件系统类型） 同一个目录只能挂载一个分区，一个分区只能挂载到一个目录。 卸载分区：umount 分区名:/dev/sdb1 永久挂载分区：需要将挂载信息写入配置文件/etc/fstab，开启启动的时候系统会自动加载/etc/fstab进行磁盘的挂载。先通过blkid查看分区的UUID（分区设备号），然后vim /etc/fstab或者直接echo “UUID=e943fbb7-020a-4c64-a48a-2597eb2496df /home ext4 defaults 0 0” &gt;&gt; /etc/fstab。 UUID=e943fbb7-020a-4c64-a48a-2597eb2496df /home ext4 defaults 0 0 要挂载的分区设备号 挂载点 文件系统类型 挂载选项 是否备份 是否检测 逻辑卷管理LVM 普通的磁盘分区无法随意扩容，如果容量满了只能备份数据再重新分区。 而逻辑卷管理LVM是一种磁盘管理方式，逻辑卷的特点是可以随意扩容，相当于把磁盘变成了抽象的分区。 物理卷（PV），卷组（VG），逻辑卷（LV）。 物理卷即物理分区，一个卷组包含多个物理卷，在卷组之上创建逻辑卷，卷组可以随时扩展物理卷，逻辑卷可以抽调卷组的空间。所以如果卷组中没有空间了，只需要购买新的物理硬盘变成物理卷加入到卷组，在扩展逻辑卷即可。 创建逻辑卷的流程： 1）将分区创建为物理卷。 pvcreate &lt;分区名&gt;，分区名变为物理卷名 2）创建卷组并将物理卷加入卷组。 vgcreate &lt;卷组名:vg1&gt; &lt;物理卷名&gt; 3）在卷组中抽调空间来创建逻辑卷。 lvcreate -L &lt;大小:4G&gt; -n &lt;物理卷名:lv1&gt; &lt;卷组名:vg1&gt; 4）格式化。 mkfs.ext4 /dev/vg1/lv1 5）挂载。mount /dev/vg1/lv1 /mnt/lv1 查看所有物理卷：pvs Fmt是版本，Attr是属性（a表示可用），PSize是大小，PFree是空闲 查看所有卷组：vgs PV是物理卷的个数，#LV是逻辑卷的个数，VSize是卷组总的大小，VFree是卷组总的空闲。 给逻辑卷扩容的流程： 1）先给卷组扩容。vgextend &lt;卷组名:vg1&gt; &lt;物理卷名&gt; 2）再给逻辑卷扩容。lvextend -L +4G /dev/vg1/lv1。 3）刷新目录。resize2fs /dev/vg1/lv1，重置文件系统大小。 交换分区管理Swap Swap分区即交换分区，也是一种普通分区，只是在功能上有些特殊。交换分区的作用是“提升”内存的容量，防止OOM（Out Of Memory）。Swap分区是由硬盘提供的交换区，当物理内存不够用的时候，操作系统才会把暂时不用的数据放到Swap中。SWAP分区通常大小设置为内存大小的2倍。 free -m，查看SWAP分区。 更改一个分区为SWAP分区的流程： 进入fdisk工具，按p查看已有分区，按t更改一个分区的ID，（swap分区对应的ID是82，可以按L查看所有ID），输入82，按w写入保存。 partprobe /dev/sdc刷新，mkswap /dev/sdc1格式化交换分区，swapon /dev/sdc1挂载交换分区 （这里其实不改分区的ID，只执行后面的格式化挂载操作也是可以的） swapoff /dev/sdc2，卸载swap分区。 文件系统 文件系统的类型：Windows：FAT16、FAT32、NTFS。Linux：EXT3、EXT4、XFS。这些都是索引（index）文件系统。 文件系统将磁盘分成一个个的小格子，这个小格子可以存也可以取，就像超市的储物柜。这个小格子叫块（block），每个块的大小是固定的4096字节，它的作用是存储文件的实际数据。 一个大于4k的文件在存储是会占用多个块，一般情况下是一个块不够，依次存储到后面的块。但是有一种情况是，有些块存储数据之后又擦除数据了，这时候空闲块的分布是不连续的，在去存储一个大文件是可能会导致这个文件在实际存储位置上不连续，这就是文件碎片。文件碎片就是同一个文件在存储在不连续的存储位置，但是文件碎片对于实际使用是没有影响的。 文件系统还需要一个东西去记录一个文件所有的块，它就是索引节点（inode）。每个文件有一个索引节点，一个inode的大小是128比特，它记录着文件的元数据（metadata），metadata就是文件属性，包括大小、权限、属主、属组、链接数、块数量、块的编号。 文件系统还有一个东西是超级块(superblock)，记录了所有块和索引节点的信息。 每次分区格式化都创建了一个独立的文件系统，所以一台主机上往往不只一个文件系统。 查看文件的inode信息：ll -i file1.txt，文件属性中的第一段数字就是inode号。 查看一个分区可以存储inode的数量：df -i，Inode就是可以存储inode的总量。 inode的数量决定了文件存储到个数，只要innode满了，不论你的存储空间有没有占满都不能再新建文件了，但是只要block没有满就可以往已有的文件里继续写东西。 总结：磁盘空间的限制取决于两方面，inode和block，inode决定了文件的数量，block决定了文件的大小。 链接文件 软链接(symbolic link)，可以让我们快速的找到一个文件，Windows中叫快捷方式。软链接可以存在不同位置，通过访问软链接就能访问原来的文件（文件夹也是一样）。 创建文件的软链接文件：ln -s /file1 /root/桌面/file11，ln就是link，-s是symbolic，/file1是原文件，/root/桌面/file11是file1的软链接。软链接的文件类型是l。 扩展（硬链接用的很少）：创建硬链接，ln /file2 /file2-h1，修改和查看硬链接同样是修改和查看原文件。但是硬链接与软链接的区别在于，如果删除原文件，软链接就没用了，读不到原来的文件内容，而硬链接还保留着原文件的内容，还可以读。可以针对目录做软链接，但是无法针对目录做硬链接。并且硬链接只能在同分区做。可以把硬链接看成文件同步。 RAID RAID（独立磁盘冗余阵列），作用是冗余备份和提升磁盘IO的读写速度。 RAID0 2块以上磁盘并行IO，读写速度快，不容错。它提高存储性能的原理是把连续的数据分散到多个磁盘上存取 ，这样，系统有数据请求就可以被多个磁盘并行的执行。 RAID1 又叫镜像卷，2块磁盘，容量是50%，提供数据的完整备份，高容错，写入慢。 RAID4 至少3块磁盘，数据分成n-1份分别放入n-1块磁盘中，剩余的一块磁盘放这n-1份数据的校验信息，如果一块数据盘损坏了，通过校验公式可以逆推出原数据。 RAID5 RAID5校验数据分布在阵列中的所有磁盘上，而没有采用专门的校验磁盘。RAID5 不存在 RAID4 中的并发写操作时的校验盘性能瓶颈问题。另外， RAID5 还具备很好的扩展性。当阵列磁盘 数量增加时，并行操作量的能力也随之增长，可比 RAID4 支持更多的磁盘，从而拥有更高的容量以及更高的性能。读写快，容错一般。 RAID6 引入双重校验的概念，它可以保护阵列中同时出现两个磁盘失效时，阵列仍能够继续工作，不会发生数据丢失。RAID6 实现代价很高，控制器的设计也比其他等级更复杂、更昂贵。 RAID10 是由两组硬盘先做 RAID1，多个RAID1在做 RAID0。至少需要四块硬盘才能构建，它的优点是同时拥有 RAID0 的高IO速度和 RAID1 的高数据可靠性，但是磁盘的利用率比较低。 RAID01 是由多组硬盘先做 RAID0，两组RAID0在做 RAID1。RAID01的缺点是多块盘只能同时坏在同一个RAID0 RAID分为硬RAID和软RAID，硬RAID是需要RAID卡的，有自己的cpu，处理速度快，而软RAID是通过操作系统上的软件实现的。软RAID不常用。 实践：给根目录扩容 前提：装系统的时候，挂载到根目录的主分区需要是lvm逻辑分区，否则无法对根目录进行扩容。 1、查看磁盘空间大小，使用df -hT 命令。查看所有可用块设备的信息，使用lsblk。查看所有卷组，使用pvs。查看所有物理卷，使用pvscan命令。 2、通过lsblk可以看到尚未被使用的块设备。（Type的类型：disk是已连接但未使用的磁盘，part是磁盘分区，rom是只读存储，lvm是逻辑卷） 3、fdisk /dev/sda，先对块设备进行分区。 4、pvcreate /dev/sda1，根据分区创建物理卷。并通过pvdisplay查看新建的物理卷。 5、vgextend centos /dev/sda1，将物理卷加入centos卷组。pvs查看是否成功。 6、lvextend -L +100G /dev/mapper/centos-root，给系统主分区所在的逻辑卷扩容。 7、xfs_growfs /dev/mapper/centos-root，扩容文件系统。 8、df -hT，查看结果。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://wht6.github.io/categories/Linux/"}],"tags":[{"name":"磁盘管理","slug":"磁盘管理","permalink":"http://wht6.github.io/tags/%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86/"}]},{"title":"Linux进程管理","slug":"Linux进程管理","date":"2021-10-27T02:00:00.000Z","updated":"2022-10-07T07:39:39.443Z","comments":true,"path":"posts/630d.html","link":"","permalink":"http://wht6.github.io/posts/630d.html","excerpt":"","text":"进程查看方式 psps（process status），显示当前进程相关信息的快照。 标准格式：ps -ef UID用户名 PID进程ID PPID父进程ID C进程占用CPU的百分比 STIME是进程启动到现在的时间 TTY该进程运行终端，若与终端无关，则显示? ，tty终端，pts伪终端 CMD命令的名称和参数 BSD格式：ps -aux（a是all，显示所有的程序。u是user，以用户为主的格式来显示程序状况。x表示不以终端划分。） USER（运行进程的用户） PID（进程ID） %CPU（cpu占用率） %MEM（内存占用率） VSZ（占用虚拟内存大小） RSS（占用实际内存大小） TTY（进程运行的终端） STAT（状态） START（进程启动时间） TIME（进程占用CPU的总时间） COMMAND（进程文件，进程名称） 排序： ps aux —sort %cpu，根据cpu占用率升序排序。ps aux —sort -%cpu，根据cpu占用率降序排序。 进程状态及含义如下表所示： STAT 含义 D 不可中断的睡眠状态(TASK_UNINTERRUPTIBLE) S 可中断的睡眠状态(TASK_INTERRUPTIBLE) R 正在运行，或在队列中的进程(TASK_RUNNING) T 停止状态(TASK_STOPPED) t 被跟踪状态(TASK_TRACED) Z 僵尸状态(TASK_DEAD - EXIT_ZOMBIE) X 退出状态，进程即将被销毁(TASK_DEAD - EXIT_DEAD) &lt; 高优先级 N 低优先级 s 包含子进程 + 位于后台的进程组 L 有些页被锁进内存 l 多线程，克隆线程 top top第一行： 1top - 20:41:08 up 18 days, 5:24, 2 users, load average: 0.04, 0.03, 0.05 top：当前时间 up：机器运行了多少时间 users：当前有多少用户 load average：分别是过去1分钟，5分钟，15分钟的cpu平均负载。 top第二行： 1Tasks: 216 total, 1 running, 215 sleeping, 0 stopped, 0 zombie Tasks：当前有多少进程 running：正在运行的进程 sleeping：正在休眠的进程 stopped：停止的进程 zombie：僵尸进程 top第三行： 1%Cpu(s): 0.2 us, 0.1 sy, 0.0 ni, 99.8 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st us：用户进程占CPU的使用率 sy：系统进程占CPU的使用率 ni：用户进程空间改变过优先级 id：空闲CPU占用率 wa：等待输入输出的CPU时间百分比 hi：硬件的中断请求 si：软件的中断请求 st：steal time top第四行&amp;第五行： 12KiB Mem : 65810456 total, 30324416 free, 9862224 used, 25623816 buff/cacheKiB Swap: 7999484 total, 7999484 free, 0 used. 54807988 avail Mem total：内存总量 free：空闲内存 used：使用的 buffer/cache： 写缓存/读缓存 【Swap分区是由硬盘提供的交换区，当物理内存不够用的时候，操作系统才会把暂时不用的数据放到Swap中。所以当这个数值变高的时候，说明内存是真的不够用了。】 第五行往下： 1234 PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 19868 root 20 0 19.733g 369980 15180 S 0.7 0.6 129:53.91 java 19682 root 20 0 19.859g 5.766g 22252 S 0.3 9.2 139:42.81 java 54625 100 20 0 50868 33512 4104 S 0.3 0.1 0:04.68 fluentd PID：进程id USER：进程所有者 PR：系统优先级。数值越大优先级越高 NI：nice值，负值表示高优先级，正值表示低优先级 VIRT：进程使用的虚拟内存总量 RES：进程使用的、未被换出的物理内存大小 SHR：共享内存大小 S：进程状态。D表示不可中断的睡眠状态；R表示运行；S表示睡眠；T表示跟踪/停止；Z表示僵尸进程。 %CPU：上次更新到现在的CPU占用百分比 %MEM：进程使用的物理内存百分比 TIME+：进程使用的CPU时间总计，单位1/100秒 COMMAND：程序 快捷键： 按z设置彩色，&lt;&gt;向前翻页和向后翻页，q退出，p按CPU排序，m按内存排序。 top参数： top -d 3表示每3秒刷新一次。（top默认是每秒刷新一次） top -p 2160 只显示PID为2160的这一个进程的实时状态信息。 kill传递信号 查看所有的信号：kill -l。 传递信号给某个进程：kill -&lt;数字&gt; 数字含义; 1-SIGHUP 重新加载配置。 2-SIGINT 键盘中断Ctrl+C。 3-SIGQUIT键盘退出Ctrl+\\，类似SIGINT。 20-SIGTSTP 键盘暂停Ctrl+Z。 9-SIGKILL 强制终止，无条件。 15-SIGTERM 终止（正常结束），缺省信号。 19-SIGSTOP 暂停。 20-SIGCONT 继续。 进程优先级nice 系统会给每个进程分配一个默认的优先级，优先级高的进程获得更多的资源。 优先级nice值得范围是-20~19，一共40个数，供用户使用。nice值越大，优先级越低。 系统优先级的范围大于nice值得范围。nice值转化成系统优先级需要加20。 进程按优先级排序：ps axo pid,command,nice —sort=-nice，查看nice，并根据nice排序。 设定程序启动后进程的优先级：nice -n - 修改正在运行的进程的优先级：renice - 作业job 作业(Job)是shell管理的进程(每个job都有一个关联的PID)，每个作业会被分配一个线性job ID。有两种形式的作业: Foreground: 当你在终端窗口输入命令，这个命令将会占据终端窗口，直到命令执行完成， 这是一个前台Job Background: 当你在命令后面添加&amp; 符号，命令将不会占据终端窗口(你可在shell prompt继续输入)，这是一个后台Job 查看后台正在运行的程序：jobs。第一列是作业序号，旁边+表示最新的，-表示最老的。 fg 4，将序号为4的job调到前台运行，bg 4，将序号为4的job调到后台运行。 Ctrl+C是直接终止一个作业，Ctrl+Z是暂停作业。 kill %4，终止序号为4的job。 procfs procfs 是进程文件系统的缩写，包含一个伪文件系统（启动时动态生成的文件系统），用于通过内核访问进程信息。这个文件系统通常被挂载到 /proc 目录。 由于 /proc 不是一个真正的文件系统，它也就不占用存储空间，只是占用有限的内存。/proc中的文件可以被修改，但一般不可以被删除。 概念区分 1.终端 在 UNIX 系统中，用户通过终端登录系统后得到一个 shell 进程，这个终端成为 shell 进程的控制终端（Controlling Terminal），进程中，控制终端是保存在 PCB 中的信息，而 fork() 会复制 PCB 中的信息，因此由 shell 进程启动的其它进程的控制终端也是这个终端。 默认情况下（没有重定向），每个进程的标准输入、标准输出和标准错误输出都指向控制终端，进程从标准输入读也就是读用户的键盘输入，进程往标准输出或标准错误输出写也就是输出到显示器上。 在控制终端输入一些特殊的控制键可以给前台进程发信号，例如 Ctrl + C会产生 SIGINT 信号，Ctrl + \\会产生 SIGQUIT 信号。 2.进程组 进程组和会话在进程之间形成了一种两级层次关系：进程组是一组相关进程的集合，会话是一组相关进程组的集合。进程组和会话是为支持 shell 作业控制而定义的抽象概念，用户通过 shell 能够交互式地在前台或后台运行命令。 进程组由一个或多个共享同一进程组标识符（PGID）的进程组成。一个进程组拥有一个进程组首进程，该进程是创建该组的进程，其进程 ID 为该进程组的 ID，新进程会继承其父进程所属的进程组 ID。 进程组拥有一个生命周期，其开始时间为首进程创建组的时刻，结束时间为最后一个成员进程退出组的时刻。一个进程可能会因为终止而退出进程组，也可能会因为加入了另外一个进程组而退出进程组。进程组首进程无需是最后一个离开进程组的成员 3.会话 会话是一组进程组的集合。会话首进程是创建该新会话的进程，其进程 ID 会成为会话 ID。新进程会继承其父进程的会话 ID。 一个会话中的所有进程共享单个控制终端。控制终端会在会话首进程首次打开一个终端设备时被建立。一个终端最多可能会成为一个会话的控制终端。 前台进程组（只能有一个）享有控制终端的操作权力 4.守护进程 守护进程（Daemon Process），也就是通常说的 Daemon 进程（精灵进程），是Linux 中的后台服务进程。它是一个生存期较长的进程，通常独立于控制终端并且周期性地执行某种任务或等待处理某些发生的事件。一般采用以 d 结尾的名字。 守护进程具备下列特征： 生命周期很长，守护进程会在系统启动的时候被创建并一直运行直至系统被关闭。 它在后台运行并且不拥有控制终端。没有控制终端确保了内核永远不会为守护进程自动生成任何控制信号以及终端相关的信号（如 SIGINT、SIGQUIT）。 Linux 的大多数服务器就是用守护进程实现的。比如，Internet 服务器 inetd， Web 服务器 httpd 等。 守护进程的开启服务过程： 执行一个 fork()，之后父进程退出，子进程继续执行。 （防止终端显示shell提示符) 子进程调用 setsid() 开启一个新会话。 (目的是为了脱离控制终端) 子进程去创建新的会话，他有自己新的ID，与之前没有冲突 清除进程的 umask 以确保当守护进程创建文件和目录时拥有所需的权限。 修改进程的当前工作目录，通常会改为根目录（/）。 关闭守护进程从其父进程继承而来的所有打开着的文件描述符。 在关闭了文件描述符0、1、2之后，守护进程通常会打开/dev/null 并使用dup2()使所有这些描述符指向这个设备。 运行核心业务逻辑","categories":[{"name":"Linux","slug":"Linux","permalink":"http://wht6.github.io/categories/Linux/"}],"tags":[{"name":"进程","slug":"进程","permalink":"http://wht6.github.io/tags/%E8%BF%9B%E7%A8%8B/"}]},{"title":"Linux用户权限","slug":"Linux用户权限","date":"2021-10-25T02:00:00.000Z","updated":"2022-10-06T10:10:39.338Z","comments":true,"path":"posts/187d.html","link":"","permalink":"http://wht6.github.io/posts/187d.html","excerpt":"","text":"chown更改文件身份文件身份表明了文件的归属。 u：用户，g：组，o：其他人； a：所有人，a=u+g+o。 更改文件的属主 ： chown &lt;用户名&gt; &lt;文件&gt; 更改文件的属主与属组： chown &lt;用户名&gt;.&lt;组名&gt; &lt;文件&gt; 更改文件的属组：chown .&lt;组名&gt; &lt;文件&gt; 更改目录身份：chown -R &lt;用户名&gt; &lt;目录&gt; chmod更改文件基本权限权限字段：rw-r—r— 三个一组，分别表示u、g、o的文件基本权限。 基本权限类型：r、w、x（读、写、执行）（目录也是特殊的文件，r表示可以查看目录内的文件列表（ls），w表示可以对目录内的文件进行增删、复制、剪切，x表示可以进入目录（cd）） rwx赋权： chmod [augo][+-=][rwx] filename 数字赋权： 利用数字代替权限，读：r=4，写：w=2，执行：x=1。 chmod &lt;三个数字&gt; filename 递归授权，-R ACL权限ACL（Access Control List）访问控制列表， ACL权限用于针对某个用户或组进行专门授权。 【ACL用于文件的三种身份不够用的情况，你需要针对某个用户或某个组单独设置一个与UGO都不一样的权限的时候就用到了ACL】 查看文件的acl权限：getfacl &lt;文件名&gt; 设置文件的acl权限：setfacl -m [augo]:&lt;用户名或组名&gt;:[rwx] &lt;文件名&gt; 12setfacl -m g:hr:rwx /home/file1setfacl -m o::rw /home/test.txt 给文件设置ACL之后，文件属性的基本权限字段后面的.变成+ -x 清除权限 给目录设置默认权限：-d是保证后续新建文件拥有ACL权限。 给目录设置递归权限：-R是为了让已存在的文件拥有ACL权限。 umask：默认权限umask用于计算文件和目录的默认权限，新建文件、目录的默认权限都通过umask计算。 查看当前用户的umask值：umask 四位完整的权限位：特殊位，属主位，属组位，其他位。（特殊位里边的421并不表示rwx，其中的4就是suid。） 【root用户的默认umask是0022，真正的默认权限值的计算方式是0777-umask=0755。在创建目录和文件的时候默认权限是755，但是我们看到文件默认是644，这是因为系统为了保护自己去掉了所有的执行权限。】 临时更改umask：umask &lt;四个数字&gt; 特殊权限：SUID、SGID、SBITSUID是一个针对文件设置的权限，功能是使调用文件的用户，临时具备文件属主的权限。suid也是权限像rwx那样，不过它是一个特殊权限，加上这个权限之后，调用文件的用户就临时具备了文件属主的权限。 【举例说明：cat的权限是：rwxr-xr-x，所有用户都有执行的权限，但是有些文件无法查看，原因就是cat在调用其他文件的时候，该用户不具备被调文件的权限，所以无法执行。如果给cat程序开启suid权限，一个普通用户就可以通过cat查看root目录下的文件。底层原理是在执行cat程序的时候直接将用户uid该成了root的uid。】 开启suid位：chmod u+s &lt;文件名&gt; 清除suid位：chmod u-s &lt;文件名&gt; SGID与SUID类似，SGID是使调用文件的用户，临时具备文件属组的权限。 SBIT 权限是针对目录的，对目录设定了 SBIT 权限，则用户在此目录下创建的文件或目录，就只有自己和 root 才有权利修改或删除该文件。 SUID和SGID的权限风险很大，一般不设置此类权限。 文件属性权限文件属性权限用于给文件上锁。 设置i属性： chattr +i &lt;文件名&gt; ：禁止对文件的任何修改（包括文件数据，文件名，且无法删除和移动） chattr +i &lt;目录名&gt; ：禁止对目录的任何修改（包括目录下的文件，目录下的文件名，无法删除和移动目录下的文件），但是可以修改目录下文件的数据 设置a属性： chattr +a &lt;文件名&gt; ：禁止对文件的任何修改，但是允许向文件中追加数据 chattr +a &lt;目录名&gt; ：禁止对目录的任何修改，但是允许目录中添加文件 查看文件的属性权限：lsattr sudo提权sudo用于普通临时提升权限指root来执行命令。 sudo是需要root进行授权的，授权文件是/etc/sudoers 授权格式：&lt;被授权者&gt; &lt;可执行主机&gt;=(&lt;被授权者借用身份&gt;) &lt;可执行命令&gt; 12345# 授予最大权限user01 ALL=(ALL) ALL# 禁止某些命令的执行echo &quot;usr02 localhost=(root) !/sbin/reboot,!/sbin/init,!/sbin/poweroff,!/sbin/shutdown,!/usr/bin/passwd,/usr/bin/passwd [A-Za-z]*,!/usr/bin/passwd root,!/bin/su&quot; &gt;&gt; /etc/sudoers 注意：授权时尽量授予具体的命令选项参数，以免出现过度授权，权限溢出问题 ；配置文件中编写授权时，命令要写绝对路径 。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://wht6.github.io/categories/Linux/"}],"tags":[{"name":"权限管理","slug":"权限管理","permalink":"http://wht6.github.io/tags/%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/"}]},{"title":"Linux用户管理","slug":"Linux用户管理","date":"2021-10-24T02:00:00.000Z","updated":"2022-10-06T10:11:18.977Z","comments":true,"path":"posts/e894.html","link":"","permalink":"http://wht6.github.io/posts/e894.html","excerpt":"","text":"用户基本信息文件passwd12root:x:0:0:root:/root:/bin/bash# passwd是一个7列的表格数据，用:来分割每一列。 字段含义：[用户名] [x] [uid] [gid] [描述] [HOME] [shell] x是密码占位符（具体的内容在/etc/shadow中） uid是用户的身份id（系统约定 uid:0是特权用户，uid:1~499是系统用户，uid:1000+是普通用户） gid是组id。 描述是补充一些用户信息，或起到说明的作用。 HOME是家目录。 shell是登录程序：/bin/bash是登录的shell命令解释器，如果是/sbin/nologin指的是不允许登录shell。 用户密码信息文件shadow/etc/shadow存放的是用户的密码信息 字段含义：[用户名] [密文] [修改时间] [最小间隔时间] [最大间隔时间] [警告时间] [不活动时间] [账号寿命] [保留字段] 两次修改密码的时间不能小于最小间隔时间。 最大间隔时间是密码的有效期，超过有效期密码就失效了。 警告时间的单位是天，失效之间多少天警告你密码快失效了，赶快修改密码。 不活动时间是用户不登录的天数。 用户组信息文件group/etc/group存放的是用户的组信息。 字段含义：[组名] [组密码] [组id] [组成员] 系统在创建用户的时候，同时也会创建一个与用户同名的组。 同一个组里的用户共享组的权限。 用户管理创建用户：useradd -m &lt;用户名&gt;（-m创建对应的用户家目录） 更改用户登录shell：usermod -s &lt;用户名&gt; 更改用户家目录：usermod -s &lt;目录&gt; 修改用户密码：passwd &lt;用户名&gt;（修改自己的密码可以省略用户名） 删除用户：userdel -r &lt;用户名&gt;（-r将家目录一块删除） 将用户加入组：usermod -G groupname username 组管理同一个组中的用户共享组的权限，通过对组授权，来对权限进行统一管理。 创建组：groupadd &lt;组名&gt; 删除组：groupdel &lt;组名&gt; 查看用户的id和组信息：id &lt;用户&gt; 将用户移出组：gpasswd -d &lt;用户名&gt; &lt;组名&gt; 组分为基本组和附加组。基本组随用户创建而创建，组名与用户同名，但是后期可以修改。用户加入的除基本组以外的其他组就叫附加组。附加组可以有多个，而基本组只能有一个。/etc/passwd中记录的GID是用户的基本组的ID。 对于useradd和usermod，-g是为用户制定基本组，-G是为用户制定附加组。 用户至少有一个基本组","categories":[{"name":"Linux","slug":"Linux","permalink":"http://wht6.github.io/categories/Linux/"}],"tags":[{"name":"用户管理","slug":"用户管理","permalink":"http://wht6.github.io/tags/%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86/"}]},{"title":"Linux文件管理","slug":"Linux文件基本操作","date":"2021-10-22T02:00:00.000Z","updated":"2022-10-06T12:06:07.587Z","comments":true,"path":"posts/f51f.html","link":"","permalink":"http://wht6.github.io/posts/f51f.html","excerpt":"","text":"根目录结构 一级目录是根目录，用“/”表示。 二级目录包括： bin（二进制文件（可执行的命令、用户密码等））、boot（系统启动文件）、dev（设备驱动文件）、etc（配置文件、控制台文件）、run（临时运行文件）、root（管理员家目录）、home（用户家目录）、tmp（临时文件）、var（日志、邮件等）、sbin（二进制特权命令）、usr（应用程序目录）。 文件操作查看目录下内容：ls &lt;目录名&gt;，ll &lt;目录名&gt; 创建文件：touch &lt;文件名&gt; 创建目录：mkdir &lt;目录&gt;，-p多级目录。 拷贝文件：cp &lt;源文件&gt; &lt;目录&gt;，-r拷贝目录，-i提示是否覆盖，-f直接覆盖不提示。 移动文件：mv &lt;源文件&gt; &lt;目录&gt; 删除文件：rm &lt;文件&gt;，-rf表示强制删除。 查看文件内容：cat、more、less、head、tail shell快捷键：tab：补全，两次tab：打印所有补全内容。ctr+L：清屏。ctr+shift++：放大。 修改文件：重定向&gt;，vim vim技巧： yy复制光标当前所在行，p是粘贴到光标所在行的下一行（换行粘贴），dd删除光标所在行。3yy是复制三行的内容，从光标所在行开始往下数。4dd是删除4行内容，也是从光标所在行往下数。u是撤销。x删除光标处的文字。 set nu是显示行数，set list是显示控制字符，像换行那种。 按v进行可视化模式与命令行模式的切换，进入可视化模式之后，通过上下左右可以进行选中，y是复制，p是不换行粘贴，d是删除。 G将光标移动到页尾，gg将光标移动到页首。6G将光标移动到第6行。 /&lt;字符串&gt;是查找文档中所有的字符串。按n是查找下一个，按N是查找上一个。 替换：&lt;范围&gt; s/&lt;原内容&gt;/&lt;新内容&gt;/g，g表示全局 文件服务FTP，FTP（File Transfer Protocol，文件传输协议） FTP在Linux中对应的软件包是vsftpd。控制端口：21，数据端口20，基于tcp。 FTP服务器的默认共享目录：/var/ftp 浏览器访问FTP服务器：ftp://192.168.43.129。 安装lftp客户端访问：lftp 192.168.43.129 下载文件：get，下载目录：mirror ，上传文件：put 通过wget下载： 12wget ftp://192.168.43.129/abc.txt -O /home/444.txt #-O表示另存为wget -m ftp://192.168.43.129/pub # 这里m意思是mirror，表示下载文件夹 客户端将文件上传到服务器需要在FTP服务器上开启上传权限。 NFS，NFS（Network File System，网络文件系统），NFS的客户端主要是Linux，支持多点同时挂载以及并发写入。NFS是NAS的一种。 文件查找which查找命令对应的程序文件：which &lt;命令&gt; locate通过数据库查找文件：locate &lt;文件名&gt;（刷新数据库，updatedb） find查找文件命令格式：find &lt;目录&gt; &lt;查找方式&gt; &lt;参数&gt; -name 根据文件名查找，-iname表示不区分大小写。 -size 根据文件大小查找。 123find /etc -size +5M # 查找5M以上的文件find /etc -size 5M # 查找5M左右的文件find /etc -size -5M # 查找5M以下的文件 -maxdepth 更加路径深度查找（从/开始数，/是一级目录） -user 按属主查找文件，-group 按属组查找文件 -type 根据文件的类型查找文件（f普通文件，l链接文件，b块设备文件，d目录文件） perm 根据权限查找文件 文件压缩和解压将目录打包压缩：tar -czf &lt;包名.tar.gz&gt; &lt;目录&gt;（-c表示create，-f表示filename，-z表示gzip，gzip是一种开源的压缩包工具） 将压缩包解压某个目录：tar -xvf &lt;包名&gt; -C &lt;目录&gt;（x表示解压，v是可视化，f文件名） zip压缩包解压：unzip -d &lt;目录&gt; &lt;包名&gt; 文件输入输出重定向标准输出重定向&gt;或1&gt;和&gt;&gt;或1&gt;&gt; 标准输入重定向&lt;或0&lt;和&lt;或1&lt;&lt; 错误输出重定向2&gt;和2&gt;&gt; 0是标准输入stdin，1是标准输出stdout，2是错误输出stderr 12345678910# cat配合重定向向文件中写入多行内容cat &gt; file.txt &lt;&lt;EOFhelloworldEOFcat &gt;&gt; file.txt &lt;&lt;EOFhelloworldEOF","categories":[{"name":"Linux","slug":"Linux","permalink":"http://wht6.github.io/categories/Linux/"}],"tags":[{"name":"文件系统","slug":"文件系统","permalink":"http://wht6.github.io/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"}]},{"title":"魔法函数","slug":"魔法函数","date":"2021-09-19T02:00:00.000Z","updated":"2022-03-30T11:46:01.733Z","comments":true,"path":"posts/bc85.html","link":"","permalink":"http://wht6.github.io/posts/bc85.html","excerpt":"","text":"&nbsp; Python里面的魔法函数，是以双下划线开头和结尾的函数。魔法函数是Python预定义好的一些函数接口，我们可以根据这些接口来定制类的一些属性。&nbsp;12345678class Company(object): def __init__(self, employee_list): self.employee = employee_listcompany = Company([&quot;tom&quot;, &quot;bob&quot;, &quot;jane&quot;])for em in company.employee: print(em)&nbsp; 这里的__init__就是一个魔法函数，是专门用于对象初始化操作的。 &nbsp;12345678910class Company(object): def __init__(self, employee_list): self.employee = employee_list def __getitem__(self, item): return self.employee[item]company = Company([&quot;tom&quot;, &quot;bob&quot;, &quot;jane&quot;])for em in company: print(em)&nbsp; 当我们定义了__getitem__这个函数之后，对象company就变成了可迭代的类型。我们并没有显式的调用__getitem__方法，但是在我们用for语句对它进行遍历的时候，解释器会自动的调用它。 &nbsp;魔法函数是Python的数据模型，一个主要的特征是对方法的隐式调用。它既不属于定义它的类，也不属于它所继承的类，可以理解为对类型的增强，就如同Python的内置类型一样，我们在使用内置类型的时候也没有方法的调用。或者可以说这些魔法函数的使用使得该类可以像内置类型一样使用。&nbsp;还是接着上面的代码：&nbsp;123company1= company[:2]print(company)print(len(company))&nbsp; 定义了__getitem__这个函数之后，我们就可以像使用list一样使用Company。 &nbsp;我们也可以不定义__getitem__方法来定义__len__方法来获取对象的长度。在使用len(company)来获取对象长度的时候，Python的解释器会先去找__len__方法，如果__len__没有，会去找__getitem__方法，如果__getitem__也没有，就去找其他可以获取长度的方法，如果都没有，程序就会崩溃报错。&nbsp;下面再举几个例子：&nbsp;123456class Company(object): def __init__(self, employee_list): self.employee = employee_listcompany = Company([&quot;tom&quot;, &quot;bob&quot;, &quot;jane&quot;])print(company) &nbsp; 这里只会打印company的地址。 &nbsp;123456789class Company(object): def __init__(self, employee_list): self.employee = employee_list def __str__(self): return &quot;,&quot;.join(self.employee)company = Company([&quot;tom&quot;, &quot;bob&quot;, &quot;jane&quot;])print(company)&nbsp; 这段代码打印的结果是：tom,bob,jane。 &nbsp;12345678910111213class MyVector(object): def __init__(self, x, y): self.x = x self.y = y def __add__(self, other): re_vector = MyVector(self.x+other.x, self.y+other.y) return re_vector def __str__(self): return &quot;x:&#123;x&#125;, y:&#123;y&#125;&quot;.format(x=self.x, y=self.y)vec1 = MyVector(1, 2)vec2 = MyVector(2, 3)print(vec1+vec2)&nbsp; 这段代码打印的结果是：x:3, y:5。","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Python语言","slug":"Python语言","permalink":"http://wht6.github.io/tags/Python%E8%AF%AD%E8%A8%80/"}]},{"title":"一切皆对象","slug":"一切皆对象","date":"2021-09-10T02:00:00.000Z","updated":"2022-03-30T11:44:56.516Z","comments":true,"path":"posts/d24b.html","link":"","permalink":"http://wht6.github.io/posts/d24b.html","excerpt":"","text":"一切皆对象&nbsp; Python的设计理念是“一切皆对象”，例如在元类编程和猴子补丁中就很好的贯彻了这一思想。&nbsp;Java中对象（object）是类（class）的一个实例，而python的面向对象更彻底，Python中类和函数也是对象，代码和模块也是对象。&nbsp;Python的类和函数具有以下特性：&nbsp; 可以赋值给一个变量 可以添加到集合对象中 可以作为参数传递给函数 可以当做函数的返回值 &nbsp; type、object和class的关系&nbsp;下面看一个例子：&nbsp;123456a=1b=&quot;abc&quot;print(type(1))print(type(int))print(type(b))print(type(str))&nbsp; 这段代码的输出： &nbsp;1234&lt;class &#39;int&#39;&gt;&lt;class &#39;type&#39;&gt;&lt;class &#39;str&#39;&gt;&lt;class &#39;type&#39;&gt;&nbsp;1是int类实例化的对象，而int是type类实例化的对象。同样，&quot;abc&quot;是str类实例化的对象，str是type类实例化的对象。 &nbsp;1234567class Student: passstu = Student()print(type(stu))print(type(Student))print(Student.__bases__)&nbsp; 这段代码的输出： &nbsp;123&lt;class &#39;__main__.Student&#39;&gt;&lt;class &#39;type&#39;&gt;(&lt;class &#39;object&#39;&gt;,)&nbsp; stu是Student类实例化的对象，而Student是type类实例化的对象。可以看出type类是用来生成类的。 &nbsp;object类是所有类都要继承的一个基础类，它是最顶层基类。由于Student类没有指名继承的类，那么默认继承object类。&nbsp;123print(type.__bases__)print(object.__bases__)print(type(object))&nbsp; 这段代码的输出： &nbsp;123(&lt;class &#39;object&#39;&gt;,)()&lt;class &#39;type&#39;&gt;&nbsp; type的基类是object，object的基类为空，object的类型为type。 &nbsp;&nbsp;可以看到list、str、dict、tuple、object这些对象都是由type创建出来的，同时这些对象又可以作为类来创建新的对象。type继承了object，同时object是type的一个实例，而type也是自身的一个实例。也就是说type可以把一切变成对象，包括它自己，这样就实现了”一切皆对象“。至于为什么type可以实例化自身，这和内部使用指针来实现有关，指针指向一块内存，同时指针也用一块内存来存储。&nbsp;因为Python中一切都是对象，所以一切都是可修改的。Java中类是不能修改的。&nbsp; Python中常见的内置类型&nbsp;对象的三个特征：身份、类型、值。对象的身份，就是就是对象的指针地址，可以通过id(a)来查看，其中a为任意对象。&nbsp;下面是常见的内置类型：&nbsp;None全局唯一。&nbsp;123a = Noneb = Noneprint(id(a) == id(b))&nbsp; 这段代码的输出结果是：Ture。说明a和b指向同一个对象None。 &nbsp;数值类型：int、float、complex、bool。&nbsp;迭代类型。&nbsp;序列类型：list、bytes、bytearray、memoryview、range、tuple、str、array。&nbsp;映射类型dict。&nbsp;集合：set、frozenset。&nbsp;上下文管理类型with。&nbsp;其他类型：模块类型、class和实例、函数类型、方法类型、代码类型、object对象、type类型、ellipsis类型、notimplemented类型。&nbsp;Python是动态语言，一切皆对象的设计理念使得Python使用起来更加的灵活方便，但是缺失了静态语言所具备的严谨性，比如Java、C++等在编译的时候就可以做一些类型检查，而Python没有编译过程，导致许多错误只能在代码运行起来之后才能发现。","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Python语言","slug":"Python语言","permalink":"http://wht6.github.io/tags/Python%E8%AF%AD%E8%A8%80/"}]},{"title":"Web页面请求的历程","slug":"Web页面请求的历程","date":"2021-09-01T08:00:00.000Z","updated":"2022-03-29T13:35:32.234Z","comments":true,"path":"posts/fc8e.html","link":"","permalink":"http://wht6.github.io/posts/fc8e.html","excerpt":"","text":"————《计算机网络：自顶向下方法》 场景： 一名学生Bob将他的便携机与学校的以太网交换机相连， 下载一个Web页面（比如说www. google, com主页） 。 假定Bob启动他的便携机， 然后将其用一根以太网电缆连接到学校的以太网交换机， 交换机又与学校的路由器相连。学校的这台路由器与一个ISP连接，本例中ISP为comcast. net。在本例中，comcast.net为学校提供了 DNS服务； 所以， DNS服务器驻留在Comcast网络中而不是学校网络中。 我们将假设DHCP服务器运行在路由器中， 就像常见情况那样。 当Bob首先将其便携机与网络连接时， 没有IP地址他就不能做任何事情（例如下载一个Web网页） 。 所以， Bob的便携机所采取的一个网络相关的动作是运行DHCP协议,以从本地DHCP服务器获得一个IP地址以及其他信息。 1） Bob便携机上的操作系统生成一个DHCP请求报文， 并将这个报文放入具有目的端口 67 （DHCP服务器） 和源端口 68 （DHCP客户） 的UDP报文段。该UDP报文段则被放置在一个具有广播IP目的地址（255.255.255.255）和源IP地址0. 0. 0. 0的IP数据报中 ， 因为Bob的便携机还没有一个IP地址。 2）包含DHCP请求报文的IP数据报则被放置在以太网帧中 。 该以太网帧具有目的MAC地址FF:FF:FF:FF:FF:FF,使该帧将广播到与交换机连接的所有设备（如果顺利的话也包括DHCP服务器） ； 该帧的源MAC地址是Boh便携机的MAC地址00:16:D:23:68:8A。 3）包含DHCP请求的广播以太网帧是第一个由Bob便携机发送到以太网交换机的帧。该交换机在所有的出端口广播入帧， 包括连接到路由器的端口。 4）路由器在它的具有MAC地址OO：22：6B：45：1F的接口接收到该广播以太网帧， 该帧中包含DHCP请求， 并且从该以太网帧中抽取出IP数据报。 该数据报的广播IP目的地址指示了这个IP数据报应当由在该节点的高层协议处理， 因此该数据报的载荷（一个UDP报文段） 被分解 向上到达UDP, DHCP请求报文从此UDP报文段中抽取出来。 此时DHCP服务器有了 DHCP请求报文。 5）我们假设运行在路由器中的DHCP服务器能够以CIDR 块68. 85. 2. 0/24分配IP地址。 所以本例中， 在学校内使用的所有IP地址都在Comcast的地址块中。 我们假设DHCP服务器分配地址68. 85. 2. 101给Bob的便携机。 DHCP服务器生成包含这个IP地址以及DNS服务器的1P地址（68.87.71.226）、 默认网关路由器的IP地址（6&amp; 85. 2. 1） 和子网块（68.85.2.0/24）（等价为“网络掩码” ） 的一个DHCP ACK报文 。 该DHCP报文被放入一个UDP报文段中， UDP报文段被放入一个IP数据报中， IP数据报再被放入一个以太网帧中。 这个以太网帧的源MAC地址是路由器连到归属网络时接口的MAC地址（OO：22：6B：45：1F：1B）,目的MAC地址是Bob便携机的MAC地址（00： 16： D3：23：68：8A）。 6） 包含DHCP ACK的以太网帧由路由器发送给交换机。 因为交换机是自学习的 ， 并且先前从Bob便携机收到（包含DHCP请求的） 以太网帧， 所以该交换机知道寻址到00： 16：D3：23：68：8A的帧仅从通向Bob便携机的输岀端口转发。 7） Bob便携机接收到包含DHCP ACK的以太网帧， 从该以太网帧中抽取IP数据报,从IP数据报中抽取UDP报文段， 从UDP报文段抽取DHCP ACK报文。 Bob的DHCP客户则记录下它的IP地址和它的DNS服务器的IP地址。 它还在其IP转发表中安装默认网关的地址 。 Bob便携机将向该默认网关发送目的地址为其子网68. 85. 2. 0/24以外的所有数据报。 此时， Bob便携机已经初始化好它的网络组件， 并准备开始处理Web网页获取。 当Bob将www. google, com的URL键入其Web浏览器时， 他开启了一长串事件， 这将导致谷歌主页最终显示在其Web浏览器上。 Bob的Web浏览器通过生成一个TCP套接字 开始了该过程， 套接字用于向www. google, com发送HTTP请求。 为了生成该套接字， Bob便携机将需要知道www. google, com的IP地址。 我们在2. 4节中学过,使用DNS协议提供这种名字到IP地址的转换服务。 8） Bob便携机上的操作系统因此生成一个DNS查询报文 ， 将字符串WWW. google, com放入DNS报文的问题段中。 该DNS报文则放置在一个具有53号（DNS服务器） 目的端口的UDP报文段中。 该UDP报文段则被放入具有IP目的地址68. 87. 71. 226 （在 第5步中DHCP ACK返回的DNS服务器地址） 和源IP地址68. 85. 2. 101的IP数据报中。 9） Bob便携机则将包含DNS请求报文的数据报放入一个以太网帧中。 该帧将发送（在链路层寻址） 到Bob学校网络中的网关路由器。 然而， 即使Bob便携机经过上述第5步中的DHCP ACK报文知道了学校网关路由器的IP地址,但仍不知道该网关路由器的MAC地址。 为了获得该网关路由器的MAC地址， Bob便携机将需要使用ARP协议。 10） Bob便携机生成一个具有目的IP地址68. 85. 2. 1 （默认网关） 的ARP查询报文,将该ARP报文放置在一个具有广播目的地址（FF： FF：FF：FF：FF：FF）的以太网帧中， 并向交换机发送该以太网帧， 交换机将该帧交付给所有连接的设备， 包括网关路由器。 11）网关路由器在通往学校网络的接口上接收到包含该ARP查询报文的帧， 发现在ARP报文中目标IP地址68. 85. 2. 1匹配其接口的IP地址。 网关路由器因此准备一个ARP回答， 指示它的MAC地址OO：22：6B：45：1F：1B对应IP地址68. 85. 2. 1 o它将ARP回答放在一个以太网帧中， 其目的地址为00：16：D3：23：68：8A （Bob便携机） ， 并向交换机发送该帧， 再由交换机将帧交付给Bob便携机。 12） Bob便携机接收包含ARP回答报文的帧， 并从ARP回答报文中抽取网关路由器的 MAC 地址（00：22：6B：45： IF： 1B）。 13） Boh便携机现在（最终！ ） 能够使包含DNS查询的以太网帧寻址到网关路由器的MAC地址。 注意到在该帧中的IP数据报具有IP目的地址68. 87.71.226 （ DNS服务器） ,而该帧具有目的地址OO：22：6B：45：1F：1B （网关路由器） 。 Bob便携机向交换机发送该帧,交换机将该帧交付给网关路由器。 14） 网关路由器接收该帧并抽取包含DNS查询的IP数据报。 路由器查找该数据报的目的地址（68.87.71.226）,并根据其转发表决定该数据报应当发送到图中Comcast网络中最左边的路由器。 IP数据报放置在链路层帧中， 该链路适合将学校路由器连接到最左边Comcast路由器， 并且该帧经这条链路发送。 15） 在Comcast网络中最左边的路由器接收到该帧， 抽取IP数据报， 检查该数据报的目的地址（68. 87. 71.226）,并根据其转发表确定出接口， 经过该接口朝着DNS服务器转发数据报， 而转发表已根据Comcast的域内协议（如RIP、OSPF或IS・IS, 5.3节） 以及因特网的域间协议BGP所填写。 16） 最终包含DNS查询的IP数据报到达了 DNS服务器。 DNS服务器抽取出DNS查询报文， 在它的DNS数据库中查找名字www. google, com ， 找到包含对应WWW. google, com的IP地址（64.233.169.105）的DNS源记录° （假设它当前缓存在DNS服务器中。 ） 前面讲过这种缓存数据源于google, com的权威DNS服务器（2.4.2节） 。 该DNS服务器形成了一个包含这种主机名到IP地址映射的DNS回答报文， 将该DNS回答报文放入UDP报文段中， 该报文段放入寻址到Bob便携机（68. 85. 2. 101）的IP数据报中。该数据报将通过Comcast网络反向转发到学校的路由器， 并从这里经过以太网交换机到Bob便携机。 17） Bob便携机从DNS报文抽取出服务器www. google, com的IP地址。 最终， 在大量工作后， Bob便携机此时准备接触www. google, com服务器！ 18）既然Bob便携机有了 www. google, com的IP地址， 它能够生成TCP套接字 ， 该套接字将用于向www. google, com发送HTTP GET报文。 当Bob生成TCP套接字时， 在Bob便携机中的TCP必须首先与www. google, com中的TCP执行三次握手 。 Bob便携机因此首先生成一个具有目的端口 80 （针对HTTP的） 的TCP SYN报文段， 将该TCP报文段放置在具有目的IP地址64. 233. 169. 105 （www. google, com）的IP数据报中， 将该数据报放置在MAC地址为OO：22：6B：45：1F：1B （网关路由器） 的帧中,并向交换机发送该帧。 19） 在学校网络、 Comcast网络和谷歌网络中的路由器朝着www. google, com转发包含TCP SYN的数据报， 使用每台路由器中的转发表， 如前面步骤14〜16那样。 前面讲过支配分组经Comcast和谷歌网络之间域间链路转发的路由器转发表项， 是由BGP协议决定的 。20） 最终， 包含TCP SYN的数据报到达WWw.googole,com。从数据报抽取出TCP SYN报文并分解到与端口 80相联系的欢迎套接字。 对于谷歌HTTP服务器和Bob便携机之间的TCP连接生成一个连接套接字（2.7节） 。 产生一个TCP SYNACK （3.5.6节） 报文段,将其放入向Bob便携机寻址的一个数据报中， 最后放入链路层帧中， 该链路适合将WWW. google, com连接到其第一跳路由器。 21） 包含TCP SYNACK报文段的数据报通过谷歌、 Comcast和学校网络， 最终到达Bob便携机的以太网卡。 数据报在操作系统中分解到步骤18生成的TCP套接字， 从而进入连接状态。 22） 借助于Bob便携机上的套接字， 现在（终于！ ） 准备向www. google, com发送字节了, Bob的浏览器生成包含要获取的URL的HTTP GET报文 。 HTTP GET报文则写入套接字， 其中GET报文成为一个TCP报文段的载荷。 该TCP报文段放置进一个数据报中， 并交付到WWW. google, com,如前面步骤18-20所述。 23） 在www. google, com的HTTP服务器从TCP套接字读取HTTP GET报文， 生成一个HTTP响应报文， 将请求的Web页内容放入HTTP响应体中， 并将报文发送进TCP套接字中。 24）包含HTTP回答报文的数据报通过谷歌、 Comcast和学校网络转发， 到达Bob便携机。 Bob的Web浏览器程序从套接字读取HTTP响应， 从HTTP响应体中抽取Web网页的html,并最终（终于！ ） 显示了 Web网页。 上述例子看起来是尽可能详尽， 我们已经忽略了一些可能的附加协议（例如， 运行在学校网关路由器中的NAT,到学校网络的无线接入， 接入学校网络或对报文段或数据报加密的安全协议， 网络管理协议） ， 以及人们将会在公共因特网中遇到的一些考虑（Web缓存， DNS等级体系） 。","categories":[{"name":"网络","slug":"网络","permalink":"http://wht6.github.io/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[]},{"title":"网络服务与程序性能分析","slug":"访问网络服务","date":"2021-08-10T10:00:00.000Z","updated":"2022-04-10T10:33:17.596Z","comments":true,"path":"posts/235e.html","link":"","permalink":"http://wht6.github.io/posts/235e.html","excerpt":"","text":"访问网络服务socket 与 IPC人们常常会使用 Go 语言去编写网络程序（当然了，这方面也是 Go 语言最为擅长的事情）。说到网络编程，我们就不得不提及 socket。 &nbsp; socket，常被翻译为套接字，它应该算是网络编程世界中最为核心的知识之一了。关于 socket，我们可以讨论的东西太多了，因此，在这里只围绕着 Go 语言向你介绍一些关于它的基础知识。&nbsp;所谓 socket，是一种 IPC 方法。IPC 是 Inter-Process Communication 的缩写，可以被翻译为进程间通信。顾名思义，IPC 这个概念（或者说规范）主要定义的是多个进程之间，相互通信的方法。&nbsp;这些方法主要包括：系统信号（signal）、管道（pipe）、套接字 （socket）、文件锁（file lock）、消息队列（message queue）、信号灯（semaphore，有的地方也称之为信号量）等。现存的主流操作系统大都对 IPC 提供了强有力的支持，尤其是 socket。&nbsp;你可能已经知道，Go 语言对 IPC 也提供了一定的支持。&nbsp;比如，在os代码包和os/signal代码包中就有针对系统信号的 API。&nbsp;又比如，os.Pipe函数可以创建命名管道，而os/exec代码包则对另一类管道（匿名管道）提供了支持。对于 socket，Go 语言与之相应的程序实体都在其标准库的net代码包中。&nbsp;毫不夸张地说，在众多的 IPC 方法中，socket 是最为通用和灵活的一种。与其他的 IPC 方法不同，利用 socket 进行通信的进程，可以不局限在同一台计算机当中。&nbsp;实际上，通信的双方无论存在于世界上的哪个角落，只要能够通过计算机的网卡端口以及网络进行互联，就可以使用 socket。&nbsp;支持 socket 的操作系统一般都会对外提供一套 API。跑在它们之上的应用程序利用这套 API，就可以与互联网上的另一台计算机中的程序、同一台计算机中的其他程序，甚至同一个程序中的其他线程进行通信。&nbsp;例如，在 Linux 操作系统中，用于创建 socket 实例的 API，就是由一个名为socket的系统调用代表的。这个系统调用是 Linux 内核的一部分。 所谓的系统调用，你可以理解为特殊的 C 语言函数。它们是连接应用程序和操作系统内核的桥梁，也是应用程序使用操作系统功能的唯一渠道。 &nbsp;在 Go 语言标准库的syscall代码包中，有一个与这个socket系统调用相对应的函数。这两者的函数签名是基本一致的，它们都会接受三个int类型的参数，并会返回一个可以代表文件描述符的结果。&nbsp;但不同的是，syscall包中的Socket函数本身是平台不相关的。在其底层，Go 语言为它支持的每个操作系统都做了适配，这才使得这个函数无论在哪个平台上，总是有效的。&nbsp;Go 语言的net代码包中的很多程序实体，都会直接或间接地使用到syscall.Socket函数。&nbsp;比如，我们在调用net.Dial函数的时候，会为它的两个参数设定值。其中的第一个参数名为network，它决定着 Go 程序在底层会创建什么样的 socket 实例，并使用什么样的协议与其他程序通信。&nbsp;net.Dial函数的第一个参数network有哪些可选值？&nbsp;net.Dial函数会接受两个参数，分别名为network和address，都是string类型的。&nbsp;参数network常用的可选值一共有 9 个。这些值分别代表了程序底层创建的 socket 实例可使用的不同通信协议，罗列如下。 &quot;tcp&quot;：代表 TCP 协议，其基于的 IP 协议的版本根据参数address的值自适应。 &quot;tcp4&quot;：代表基于 IP 协议第四版的 TCP 协议。 &quot;tcp6&quot;：代表基于 IP 协议第六版的 TCP 协议。 &quot;udp&quot;：代表 UDP 协议，其基于的 IP 协议的版本根据参数address的值自适应。 &quot;udp4&quot;：代表基于 IP 协议第四版的 UDP 协议。 &quot;udp6&quot;：代表基于 IP 协议第六版的 UDP 协议。 &quot;unix&quot;：代表 Unix 通信域下的一种内部 socket 协议，以 SOCK_STREAM 为 socket 类型。 &quot;unixgram&quot;：代表 Unix 通信域下的一种内部 socket 协议，以 SOCK_DGRAM 为 socket 类型。 &quot;unixpacket&quot;：代表 Unix 通信域下的一种内部 socket 协议，以 SOCK_SEQPACKET 为 socket 类型。 为了更好地理解这些可选值的深层含义，我们需要了解一下syscall.Socket函数接受的那三个参数。&nbsp;我在前面说了，这个函数接受的三个参数都是int类型的。这些参数所代表的分别是想要创建的 socket 实例通信域、类型以及使用的协议。&nbsp;Socket 的通信域主要有这样几个可选项：IPv4 域、IPv6 域和 Unix 域。&nbsp;Unix 域，指的是一种类 Unix 操作系统中特有的通信域。在装有此类操作系统的同一台计算机中，应用程序可以基于此域建立 socket 连接。&nbsp;以上三种通信域分别可以由syscall代码包中的常量AF_INET、AF_INET6和AF_UNIX表示。&nbsp;Socket 的类型一共有 4 种，分别是：SOCK_DGRAM、SOCK_STREAM、SOCK_SEQPACKET以及SOCK_RAW。syscall代码包中也都有同名的常量与之对应。前两者更加常用一些。&nbsp;SOCK_DGRAM中的“DGRAM”代表的是 datagram，即数据报文。它是一种有消息边界，但没有逻辑连接的非可靠 socket 类型，我们熟知的基于 UDP 协议的网络通信就属于此类。&nbsp;有消息边界的意思是，与 socket 相关的操作系统内核中的程序（以下简称内核程序）在发送或接收数据的时候是以消息为单位的。&nbsp;你可以把消息理解为带有固定边界的一段数据。内核程序可以自动地识别和维护这种边界，并在必要的时候，把数据切割成一个一个的消息，或者把多个消息串接成连续的数据。如此一来，应用程序只需要面向消息进行处理就可以了。&nbsp;所谓的有逻辑连接是指，通信双方在收发数据之前必须先建立网络连接。待连接建立好之后，双方就可以一对一地进行数据传输了。显然，基于 UDP 协议的网络通信并不需要这样，它是没有逻辑连接的。&nbsp;只要应用程序指定好对方的网络地址，内核程序就可以立即把数据报文发送出去。这有优势，也有劣势。&nbsp;优势是发送速度快，不长期占用网络资源，并且每次发送都可以指定不同的网络地址。但是，无法保证传输的可靠性，不能实现数据的有序性，以及数据只能单向进行传输。&nbsp;而SOCK_STREAM这个 socket 类型，恰恰与SOCK_DGRAM相反。它没有消息边界，但有逻辑连接，能够保证传输的可靠性和数据的有序性，同时还可以实现数据的双向传输。众所周知的基于 TCP 协议的网络通信就属于此类。 这样的网络通信传输数据的形式是字节流，而不是数据报文。字节流是以字节为单位的。内核程序无法感知一段字节流中包含了多少个消息，以及这些消息是否完整，这完全需要应用程序自己去把控。 不过，此类网络通信中的一端，总是会忠实地按照另一端发送数据时的字节排列顺序，接收和缓存它们。所以，应用程序需要根据双方的约定去数据中查找消息边界，并按照边界切割数据，仅此而已。 &nbsp;syscall.Socket函数的第三个参数用于表示 socket 实例所使用的协议。&nbsp;通常，只要明确指定了前两个参数的值，我们就无需再去确定第三个参数值了，一般把它置为0就可以了。这时，内核程序会自行选择最合适的协议。&nbsp;比如，当前两个参数值分别为syscall.AF_INET和syscall.SOCK_DGRAM的时候，内核程序会选择 UDP 作为协议。&nbsp;又比如，在前两个参数值分别为syscall.AF_INET6和syscall.SOCK_STREAM时，内核程序可能会选择 TCP 作为协议。 不过，你也看到了，在使用net包中的高层次 API 的时候，我们连那前两个参数值都无需给定，只需要把前面罗列的那些字符串字面量的其中一个，作为network参数的值就好了。&nbsp;调用net.DialTimeout函数时给定的超时时间意味着什么？&nbsp;简单来说，这里的超时时间，代表着函数为网络连接建立完成而等待的最长时间。这是一个相对的时间。它会由这个函数的参数timeout的值表示。&nbsp;开始的时间点几乎是我们调用net.DialTimeout函数的那一刻。在这之后，时间会主要花费在“解析参数network和address的值”，以及“创建 socket 实例并建立网络连接”这两件事情上。&nbsp;不论执行到哪一步，只要在绝对的超时时间达到的那一刻，网络连接还没有建立完成，该函数就会返回一个代表了 I/O 操作超时的错误值。&nbsp;值得注意的是，在解析address的值的时候，函数会确定网络服务的 IP 地址、端口号等必要信息，并在需要时访问 DNS 服务。&nbsp;另外，如果解析出的 IP 地址有多个，那么函数会串行或并发地尝试建立连接。但无论用什么样的方式尝试，函数总会以最先建立成功的那个连接为准。&nbsp;同时，它还会根据超时前的剩余时间，去设定针对每次连接尝试的超时时间，以便让它们都有适当的时间执行。&nbsp;再多说一点。在net包中还有一个名为Dialer的结构体类型。该类型有一个名叫Timeout的字段，它与上述的timeout参数的含义是完全一致的。实际上，net.DialTimeout函数正是利用了这个类型的值才得以实现功能的。&nbsp;net.Dialer类型值得你好好学习一下，尤其是它的每个字段的功用以及它的DialContext方法。&nbsp;在你调用了net.Dial等函数之后，如果成功就会得到一个代表了网络连接的net.Conn接口类型的值。&nbsp;怎样在net.Conn类型的值上正确地设定针对读操作和写操作的超时时间？&nbsp;net.Conn类型有 3 个可用于设置超时时间的方法，分别是：SetDeadline、SetReadDeadline和SetWriteDeadline。&nbsp;这三个方法的签名是一模一样的，只是名称不同罢了。它们都接受一个time.Time类型的参数，并都会返回一个error类型的结果。其中的SetDeadline方法是用来同时设置读操作超时和写操作超时的。&nbsp;有一点需要特别注意，这三个方法都会针对任何正在进行以及未来将要进行的相应操作进行超时设定。&nbsp;因此，如果你要在一个循环中进行读操作或写操作的话，最好在每次迭代中都进行一次超时设定。&nbsp;否则，靠后的操作就有可能因触达超时时间而直接失败。另外，如果有必要，你应该再次调用它们并传入time.Time类型的零值来表达不再限定超时时间。&nbsp; HTTP协议通信用net.Dial或net.DialTimeout函数来访问基于 HTTP 协议的网络服务是完全没有问题的，因为HTTP 协议是基于 TCP/IP 协议栈的，并且它也是一个面向普通文本的协议。&nbsp;原则上，我们使用任何一个文本编辑器，都可以轻易地写出一个完整的 HTTP 请求报文。只要你搞清楚了请求报文的头部（header）和主体（body）应该包含的内容，这样做就会很容易。所以，在这种情况下，即便直接使用net.Dial函数，你应该也不会感觉到困难。&nbsp;不过，不困难并不意味着很方便。如果我们只是访问基于 HTTP 协议的网络服务的话，那么使用net/http代码包中的程序实体来做，显然会更加便捷。&nbsp;其中，最便捷的是使用http.Get函数。我们在调用它的时候只需要传给它一个 URL 就可以了，比如像下面这样：&nbsp;123456789url1 := &quot;http://google.cn&quot;fmt.Printf(&quot;Send request to %q with method GET ...\\n&quot;, url1)resp1, err := http.Get(url1)if err != nil &#123; fmt.Printf(&quot;request sending error: %v\\n&quot;, err)&#125;defer resp1.Body.Close()line1 := resp1.Proto + &quot; &quot; + resp1.Statusfmt.Printf(&quot;The first line of response:\\n%s\\n&quot;, line1)&nbsp; http.Get函数会返回两个结果值。第一个结果值的类型是*http.Response，它是网络服务给我们传回来的响应内容的结构化表示。 &nbsp;第二个结果值是error类型的，它代表了在创建和发送 HTTP 请求，以及接收和解析 HTTP 响应的过程中可能发生的错误。&nbsp;http.Get函数会在内部使用缺省的 HTTP 客户端，并且调用它的Get方法以完成功能。这个缺省的 HTTP 客户端是由net/http包中的公开变量DefaultClient代表的，其类型是*http.Client。它的基本类型也是可以被拿来使用的，甚至它还是开箱即用的。下面的这两行代码：&nbsp;12var httpClient1 http.Clientresp2, err := httpClient1.Get(url1)&nbsp; 与前面的这一行代码resp1, err := http.Get(url1)是等价的。 &nbsp;http.Client是一个结构体类型，并且它包含的字段都是公开的。之所以该类型的零值仍然可用，是因为它的这些字段要么存在着相应的缺省值，要么其零值直接就可以使用，且代表着特定的含义。&nbsp;http.Client类型中的Transport字段代表着什么？&nbsp;http.Client类型中的Transport字段代表着：向网络服务发送 HTTP 请求，并从网络服务接收 HTTP 响应的操作过程。也就是说，该字段的方法RoundTrip应该实现单次 HTTP 事务（或者说基于 HTTP 协议的单次交互）需要的所有步骤。&nbsp;这个字段是http.RoundTripper接口类型的，它有一个由http.DefaultTransport变量代表的缺省值（以下简称DefaultTransport）。当我们在初始化一个http.Client类型的值（以下简称Client值）的时候，如果没有显式地为该字段赋值，那么这个Client值就会直接使用DefaultTransport。&nbsp;顺便说一下，http.Client类型的Timeout字段，代表的正是前面所说的单次 HTTP 事务的超时时间，它是time.Duration类型的。它的零值是可用的，用于表示没有设置超时时间。&nbsp;下面，我们再通过该字段的缺省值DefaultTransport，来深入地了解一下这个Transport字段。&nbsp;DefaultTransport的实际类型是*http.Transport，后者即为http.RoundTripper接口的默认实现。这个类型是可以被复用的，也推荐被复用，同时，它也是并发安全的。正因为如此，http.Client类型也拥有着同样的特质。&nbsp;http.Transport类型，会在内部使用一个net.Dialer类型的值（以下简称Dialer值），并且，它会把该值的Timeout字段的值，设定为30秒。&nbsp;也就是说，这个Dialer值如果在 30 秒内还没有建立好网络连接，那么就会被判定为操作超时。在DefaultTransport的值被初始化的时候，这样的Dialer值的DialContext方法会被赋给前者的DialContext字段。&nbsp;http.Transport类型还包含了很多其他的字段，其中有一些字段是关于操作超时的。 IdleConnTimeout：含义是空闲的连接在多久之后就应该被关闭。 DefaultTransport会把该字段的值设定为90秒。如果该值为0，那么就表示不关闭空闲的连接。注意，这样很可能会造成资源的泄露。 ResponseHeaderTimeout：含义是，从客户端把请求完全递交给操作系统到从操作系统那里接收到响应报文头的最大时长。DefaultTransport并没有设定该字段的值。 ExpectContinueTimeout：含义是，在客户端递交了请求报文头之后，等待接收第一个响应报文头的最长时间。在客户端想要使用 HTTP 的“POST”方法把一个很大的报文体发送给服务端的时候，它可以先通过发送一个包含了“Expect: 100-continue”的请求报文头，来询问服务端是否愿意接收这个大报文体。这个字段就是用于设定在这种情况下的超时时间的。注意，如果该字段的值不大于0，那么无论多大的请求报文体都将会被立即发送出去。这样可能会造成网络资源的浪费。DefaultTransport把该字段的值设定为了1秒。 TLSHandshakeTimeout：TLS 是 Transport Layer Security 的缩写，可以被翻译为传输层安全。这个字段代表了基于 TLS 协议的连接在被建立时的握手阶段的超时时间。若该值为0，则表示对这个时间不设限。DefaultTransport把该字段的值设定为了10秒。 &nbsp;此外，还有一些与IdleConnTimeout相关的字段值得我们关注，即：MaxIdleConns、MaxIdleConnsPerHost以及MaxConnsPerHost。&nbsp;无论当前的http.Transport类型的值（以下简称Transport值）访问了多少个网络服务，MaxIdleConns字段都只会对空闲连接的总数做出限定。而MaxIdleConnsPerHost字段限定的则是，该Transport值访问的每一个网络服务的最大空闲连接数。&nbsp;每一个网络服务都会有自己的网络地址，可能会使用不同的网络协议，对于一些 HTTP 请求也可能会用到代理。Transport值正是通过这三个方面的具体情况，来鉴别不同的网络服务的。&nbsp;MaxIdleConnsPerHost字段的缺省值，由http.DefaultMaxIdleConnsPerHost变量代表，值为2。也就是说，在默认情况下，对于某一个Transport值访问的每一个网络服务，它的空闲连接数都最多只能有两个。&nbsp;与MaxIdleConnsPerHost字段的含义相似的，是MaxConnsPerHost字段。不过，后者限制的是，针对某一个Transport值访问的每一个网络服务的最大连接数，不论这些连接是否是空闲的。并且，该字段没有相应的缺省值，它的零值表示不对此设限。&nbsp;DefaultTransport并没有显式地为MaxIdleConnsPerHost和MaxConnsPerHost这两个字段赋值，但是它却把MaxIdleConns字段的值设定为了100。&nbsp;换句话说，在默认情况下，空闲连接的总数最大为100，而针对每个网络服务的最大空闲连接数为2。注意，上述两个与空闲连接数有关的字段的值应该是联动的，所以，你有时候需要根据实际情况来定制它们。&nbsp;当然了，这首先需要我们在初始化Client值的时候，定制它的Transport字段的值。定制这个值的方式，可以参看DefaultTransport变量的声明。&nbsp;最后，我简单说一下为什么会出现空闲的连接。我们都知道，HTTP 协议有一个请求报文头叫做“Connection”。在 HTTP 协议的 1.1 版本中，这个报文头的值默认是“keep-alive”。&nbsp;在这种情况下的网络连接都是持久连接，它们会在当前的 HTTP 事务完成后仍然保持着连通性，因此是可以被复用的。&nbsp;既然连接可以被复用，那么就会有两种可能。一种可能是，针对于同一个网络服务，有新的 HTTP 请求被递交，该连接被再次使用。另一种可能是，不再有对该网络服务的 HTTP 请求，该连接被闲置。&nbsp;显然，后一种可能就产生了空闲的连接。另外，如果分配给某一个网络服务的连接过多的话，也可能会导致空闲连接的产生，因为每一个新递交的 HTTP 请求，都只会征用一个空闲的连接。所以，为空闲连接设定限制，在大多数情况下都是很有必要的，也是需要斟酌的。&nbsp;如果我们想彻底地杜绝空闲连接的产生，那么可以在初始化Transport值的时候把它的DisableKeepAlives字段的值设定为true。这时，HTTP 请求的“Connection”报文头的值就会被设置为“close”。这会告诉网络服务，这个网络连接不必保持，当前的 HTTP 事务完成后就可以断开它了。&nbsp;如此一来，每当一个 HTTP 请求被递交时，就都会产生一个新的网络连接。这样做会明显地加重网络服务以及客户端的负载，并会让每个 HTTP 事务都耗费更多的时间。所以，在一般情况下，我们都不要去设置这个DisableKeepAlives字段。&nbsp;顺便说一句，在net.Dialer类型中，也有一个看起来很相似的字段KeepAlive。不过，它与前面所说的 HTTP 持久连接并不是一个概念，KeepAlive是直接作用在底层的 socket 上的。&nbsp;它的背后是一种针对网络连接（更确切地说，是 TCP 连接）的存活探测机制。它的值用于表示每间隔多长时间发送一次探测包。当该值不大于0时，则表示不开启这种机制。DefaultTransport会把这个字段的值设定为30秒。&nbsp;好了，以上这些内容阐述的就是，http.Client类型中的Transport字段的含义，以及它的值的定制方式。这涉及了http.RoundTripper接口、http.DefaultTransport变量、http.Transport类型，以及net.Dialer类型。&nbsp;http.Server类型的ListenAndServe方法都做了哪些事情？&nbsp;http.Server类型与http.Client是相对应的。http.Server代表的是基于 HTTP 协议的服务端，或者说网络服务。&nbsp;http.Server类型的ListenAndServe方法的功能是：监听一个基于 TCP 协议的网络地址，并对接收到的 HTTP 请求进行处理。这个方法会默认开启针对网络连接的存活探测机制，以保证连接是持久的。同时，该方法会一直执行，直到有严重的错误发生或者被外界关掉。当被外界关掉时，它会返回一个由http.ErrServerClosed变量代表的错误值。&nbsp;这个ListenAndServe方法主要会做下面这几件事情。 检查当前的http.Server类型的值（以下简称当前值）的Addr字段。该字段的值代表了当前的网络服务需要使用的网络地址，即：IP 地址和端口号。如果这个字段的值为空字符串，那么就用&quot;:http&quot;代替。也就是说，使用任何可以代表本机的域名和 IP 地址，并且端口号为80。 通过调用net.Listen函数在已确定的网络地址上启动基于 TCP 协议的监听。 检查net.Listen函数返回的错误值。如果该错误值不为nil，那么就直接返回该值。否则，通过调用当前值的Serve方法准备接受和处理将要到来的 HTTP 请求。 &nbsp;可以从当前问题直接衍生出的问题一般有两个，一个是“net.Listen函数都做了哪些事情”，另一个是“http.Server类型的Serve方法是怎样接受和处理 HTTP 请求的”。&nbsp;对于第一个直接的衍生问题，如果概括地说，回答可以是： 解析参数值中包含的网络地址隐含的 IP 地址和端口号； 根据给定的网络协议，确定监听的方法，并开始进行监听。 &nbsp;从这里的第二个步骤出发，我们还可以继续提出一些间接的衍生问题。这往往会涉及net.socket函数以及相关的 socket 知识。&nbsp;对于第二个直接的衍生问题，我们可以这样回答：&nbsp;在一个for循环中，网络监听器的Accept方法会被不断地调用，该方法会返回两个结果值；第一个结果值是net.Conn类型的，它会代表包含了新到来的 HTTP 请求的网络连接；第二个结果值是代表了可能发生的错误的error类型值。&nbsp;如果这个错误值不为nil，除非它代表了一个暂时性的错误，否则循环都会被终止。如果是暂时性的错误，那么循环的下一次迭代将会在一段时间之后开始执行。&nbsp;如果这里的Accept方法没有返回非nil的错误值，那么这里的程序将会先把它的第一个结果值包装成一个*http.conn类型的值（以下简称conn值），然后通过在新的 goroutine 中调用这个conn值的serve方法，来对当前的 HTTP 请求进行处理。&nbsp;这个处理的细节还是很多的，所以我们依然可以找出不少的间接的衍生问题。比如，这个conn值的状态有几种，分别代表着处理的哪个阶段？又比如，处理过程中会用到哪些读取器和写入器，它们的作用分别是什么？再比如，这里的程序是怎样调用我们自定义的处理函数的，等等。&nbsp;怎样优雅地停止基于 HTTP 协议的网络服务程序？&nbsp;net/http.Server类型有一个名为Shutdown的指针方法可以实现“优雅的停止”。也就是说，它可以在不中断任何正处在活动状态的连接的情况下平滑地关闭当前的服务器。&nbsp;它会先关闭所有的空闲连接，并一直等待。只有活动的连接变为空闲之后，它才会关闭它们。当所有的连接都被平滑地关闭之后，它会关闭当前的服务器并返回。当有错误发生时，它还会把相应的错误值返回。&nbsp;另外，你还可以通过调用Server值的RegisterOnShutdown方法来注册可以在服务器即将关闭时被自动调用的函数。&nbsp;更确切地说，当前服务器的Shutdown方法会以异步的方式调用如此注册的所有函数。我们可以利用这样的函数来通知长连接的客户端“连接即将关闭”。 程序性能分析基础Go 语言为程序开发者们提供了丰富的性能分析 API，和非常好用的标准工具。这些 API 主要存在于： runtime/pprof； net/http/pprof； runtime/trace； 这三个代码包中。 &nbsp; 另外，runtime代码包中还包含了一些更底层的 API。它们可以被用来收集或输出 Go 程序运行过程中的一些关键指标，并帮助我们生成相应的概要文件以供后续分析时使用。&nbsp;至于标准工具，主要有go tool pprof和go tool trace这两个。它们可以解析概要文件中的信息，并以人类易读的方式把这些信息展示出来。&nbsp;此外，go test命令也可以在程序测试完成后生成概要文件。如此一来，我们就可以很方便地使用前面那两个工具读取概要文件，并对被测程序的性能加以分析。这无疑会让程序性能测试的一手资料更加丰富，结果更加精确和可信。&nbsp;在 Go 语言中，用于分析程序性能的概要文件有三种，分别是：CPU 概要文件（CPU Profile）、内存概要文件（Mem Profile）和阻塞概要文件（Block Profile）。&nbsp;这些概要文件中包含的都是：在某一段时间内，对 Go 程序的相关指标进行多次采样后得到的概要信息。&nbsp;对于 CPU 概要文件来说，其中的每一段独立的概要信息都记录着，在进行某一次采样的那个时刻，CPU 上正在执行的 Go 代码。&nbsp;而对于内存概要文件，其中的每一段概要信息都记载着，在某个采样时刻，正在执行的 Go 代码以及堆内存的使用情况，这里包含已分配和已释放的字节数量和对象数量。至于阻塞概要文件，其中的每一段概要信息，都代表着 Go 程序中的一个 goroutine 阻塞事件。&nbsp;注意，在默认情况下，这些概要文件中的信息并不是普通的文本，它们都是以二进制的形式展现的。如果你使用一个常规的文本编辑器查看它们的话，那么肯定会看到一堆“乱码”。&nbsp;这时就可以显现出go tool pprof这个工具的作用了。我们可以通过它进入一个基于命令行的交互式界面，并对指定的概要文件进行查阅。就像下面这样： 123456$ go tool pprof cpuprofile.outType: cpuTime: Nov 9, 2018 at 4:31pm (CST)Duration: 7.96s, Total samples = 6.88s (86.38%)Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)(pprof) 关于这个工具的具体用法，我就不在这里赘述了。在进入这个工具的交互式界面之后，我们只要输入指令help并按下回车键，就可以看到很详细的帮助文档。&nbsp;我们现在来说说怎样生成概要文件。&nbsp;你可能会问，既然在概要文件中的信息不是普通的文本，那么它们到底是什么格式的呢？一个对广大的程序开发者而言，并不那么重要的事实是，它们是通过 protocol buffers 生成的二进制数据流，或者说字节流。&nbsp;概括来讲，protocol buffers 是一种数据序列化协议，同时也是一个序列化工具。它可以把一个值，比如一个结构体或者一个字典，转换成一段字节流。&nbsp;也可以反过来，把经过它生成的字节流反向转换为程序中的一个值。前者就被叫做序列化，而后者则被称为反序列化。&nbsp;换句话说，protocol buffers 定义和实现了一种“可以让数据在结构形态和扁平形态之间互相转换”的方式。&nbsp;Protocol buffers 的优势有不少。比如，它可以在序列化数据的同时对数据进行压缩，所以它生成的字节流，通常都要比相同数据的其他格式（例如 XML 和 JSON）占用的空间明显小很多。&nbsp;又比如，它既能让我们自己去定义数据序列化和结构化的格式，也允许我们在保证向后兼容的前提下去更新这种格式。&nbsp;正因为这些优势，Go 语言从 1.8 版本开始，把所有 profile 相关的信息生成工作都交给 protocol buffers 来做了。这也是我们在上述概要文件中，看不到普通文本的根本原因了。&nbsp;Protocol buffers 的用途非常广泛，并且在诸如数据存储、数据传输等任务中有着很高的使用率。&nbsp;怎样让程序对 CPU 概要信息进行采样？&nbsp;这需要用到runtime/pprof包中的 API。更具体地说，在我们想让程序开始对 CPU 概要信息进行采样的时候，需要调用这个代码包中的StartCPUProfile函数，而在停止采样的时候则需要调用该包中的StopCPUProfile函数。&nbsp;runtime/pprof.StartCPUProfile函数（以下简称StartCPUProfile函数）在被调用的时候，先会去设定 CPU 概要信息的采样频率，并会在单独的 goroutine 中进行 CPU 概要信息的收集和输出。&nbsp;注意，StartCPUProfile函数设定的采样频率总是固定的，即：100赫兹。也就是说，每秒采样100次，或者说每10毫秒采样一次。&nbsp;赫兹，也称 Hz，是从英文单词“Hertz”（一个英文姓氏）音译过来的一个中文词。它是 CPU 主频的基本单位。&nbsp;CPU 的主频指的是，CPU 内核工作的时钟频率，也常被称为 CPU clock speed。这个时钟频率的倒数即为时钟周期（clock cycle），也就是一个 CPU 内核执行一条运算指令所需的时间，单位是秒。&nbsp;例如，主频为1000Hz 的 CPU，它的单个内核执行一条运算指令所需的时间为0.001秒，即1毫秒。又例如，我们现在常用的3.2GHz 的多核 CPU，其单个内核在1个纳秒的时间里就可以至少执行三条运算指令。&nbsp;StartCPUProfile函数设定的 CPU 概要信息采样频率，相对于现代的 CPU 主频来说是非常低的。这主要有两个方面的原因。&nbsp;一方面，过高的采样频率会对 Go 程序的运行效率造成很明显的负面影响。因此，runtime包中SetCPUProfileRate函数在被调用的时候，会保证采样频率不超过1MHz（兆赫），也就是说，它只允许每1微秒最多采样一次。StartCPUProfile函数正是通过调用这个函数来设定 CPU 概要信息的采样频率的。&nbsp;另一方面，经过大量的实验，Go 语言团队发现100Hz 是一个比较合适的设定。因为这样做既可以得到足够多、足够有用的概要信息，又不至于让程序的运行出现停滞。另外，操作系统对高频采样的处理能力也是有限的，一般情况下，超过500Hz 就很可能得不到及时的响应了。&nbsp;在StartCPUProfile函数执行之后，一个新启用的 goroutine 将会负责执行 CPU 概要信息的收集和输出，直到runtime/pprof包中的StopCPUProfile函数被成功调用。&nbsp;StopCPUProfile函数也会调用runtime.SetCPUProfileRate函数，并把参数值（也就是采样频率）设为0。这会让针对 CPU 概要信息的采样工作停止。&nbsp;同时，它也会给负责收集 CPU 概要信息的代码一个“信号”，以告知收集工作也需要停止了。&nbsp;在接到这样的“信号”之后，那部分程序将会把这段时间内收集到的所有 CPU 概要信息，全部写入到我们在调用StartCPUProfile函数的时候指定的写入器中。只有在上述操作全部完成之后，StopCPUProfile函数才会返回。&nbsp;怎样设定内存概要信息的采样频率？&nbsp;针对内存概要信息的采样会按照一定比例收集 Go 程序在运行期间的堆内存使用情况。设定内存概要信息采样频率的方法很简单，只要为runtime.MemProfileRate变量赋值即可。&nbsp;这个变量的含义是，平均每分配多少个字节，就对堆内存的使用情况进行一次采样。如果把该变量的值设为0，那么，Go 语言运行时系统就会完全停止对内存概要信息的采样。该变量的缺省值是512 KB，也就是512千字节。&nbsp;注意，如果你要设定这个采样频率，那么越早设定越好，并且只应该设定一次，否则就可能会对 Go 语言运行时系统的采样工作，造成不良影响。比如，只在main函数的开始处设定一次。&nbsp;在这之后，当我们想获取内存概要信息的时候，还需要调用runtime/pprof包中的WriteHeapProfile函数。该函数会把收集好的内存概要信息，写到我们指定的写入器中。&nbsp;注意，我们通过WriteHeapProfile函数得到的内存概要信息并不是实时的，它是一个快照，是在最近一次的内存垃圾收集工作完成时产生的。如果你想要实时的信息，那么可以调用runtime.ReadMemStats函数。不过要特别注意，该函数会引起 Go 语言调度器的短暂停顿。&nbsp;怎样获取到阻塞概要信息？&nbsp;我们调用runtime包中的SetBlockProfileRate函数，即可对阻塞概要信息的采样频率进行设定。该函数有一个名叫rate的参数，它是int类型的。&nbsp;这个参数的含义是，只要发现一个阻塞事件的持续时间达到了多少个纳秒，就可以对其进行采样。如果这个参数的值小于或等于0，那么就意味着 Go 语言运行时系统将会完全停止对阻塞概要信息的采样。&nbsp;在runtime包中，还有一个名叫blockprofilerate的包级私有变量，它是uint64类型的。这个变量的含义是，只要发现一个阻塞事件的持续时间跨越了多少个 CPU 时钟周期，就可以对其进行采样。它的含义与我们刚刚提到的rate参数的含义非常相似，不是吗？&nbsp;实际上，这两者的区别仅仅在于单位不同。runtime.SetBlockProfileRate函数会先对参数rate的值进行单位换算和必要的类型转换，然后，它会把换算结果用原子操作赋给blockprofilerate变量。由于此变量的缺省值是0，所以 Go 语言运行时系统在默认情况下并不会记录任何在程序中发生的阻塞事件。&nbsp;另一方面，当我们需要获取阻塞概要信息的时候，需要先调用runtime/pprof包中的Lookup函数并传入参数值&quot;block&quot;，从而得到一个*runtime/pprof.Profile类型的值（以下简称Profile值）。在这之后，我们还需要调用这个Profile值的WriteTo方法，以驱使它把概要信息写进我们指定的写入器中。&nbsp;这个WriteTo方法有两个参数，一个参数就是我们刚刚提到的写入器，它是io.Writer类型的。而另一个参数则是代表了概要信息详细程度的int类型参数debug。&nbsp;debug参数主要的可选值有两个，即：0和1。当debug的值为0时，通过WriteTo方法写进写入器的概要信息仅会包含go tool pprof工具所需的内存地址，这些内存地址会以十六进制的形式展现出来。&nbsp;当该值为1时，相应的包名、函数名、源码文件路径、代码行号等信息就都会作为注释被加入进去。另外，debug为0时的概要信息，会经由 protocol buffers 转换为字节流。而在debug为1的时候，WriteTo方法输出的这些概要信息就是我们可以读懂的普通文本了。&nbsp;除此之外，debug的值也可以是2。这时，被输出的概要信息也会是普通的文本，并且通常会包含更多的细节。至于这些细节都包含了哪些内容，那就要看我们调用runtime/pprof.Lookup函数的时候传入的是什么样的参数值了。&nbsp;runtime/pprof.Lookup函数的正确调用方式是什么？&nbsp;runtime/pprof.Lookup函数（以下简称Lookup函数）的功能是，提供与给定的名称相对应的概要信息。这个概要信息会由一个Profile值代表。如果该函数返回了一个nil，那么就说明不存在与给定名称对应的概要信息。&nbsp;runtime/pprof包已经为我们预先定义了 6 个概要名称。它们对应的概要信息收集方法和输出方法也都已经准备好了。我们直接拿来使用就可以了。它们是：goroutine、heap、allocs、threadcreate、block和mutex。&nbsp;当我们把&quot;goroutine&quot;传入Lookup函数的时候，该函数会利用相应的方法，收集到当前正在使用的所有 goroutine 的堆栈跟踪信息。注意，这样的收集会引起 Go 语言调度器的短暂停顿。&nbsp;当调用该函数返回的Profile值的WriteTo方法时，如果参数debug的值大于或等于2，那么该方法就会输出所有 goroutine 的堆栈跟踪信息。这些信息可能会非常多。如果它们占用的空间超过了64 MB（也就是64兆字节），那么相应的方法就会将超出的部分截掉。&nbsp;如果Lookup函数接到的参数值是&quot;heap&quot;，那么它就会收集与堆内存的分配和释放有关的采样信息。这实际上就是我们在前面讨论过的内存概要信息。在我们传入&quot;allocs&quot;的时候，后续的操作会与之非常的相似。&nbsp;在这两种情况下，Lookup函数返回的Profile值也会极其相像。只不过，在这两种Profile值的WriteTo方法被调用时，它们输出的概要信息会有细微的差别，而且这仅仅体现在参数debug等于0的时候。&nbsp;&quot;heap&quot;会使得被输出的内存概要信息默认以“在用空间”（inuse_space）的视角呈现，而&quot;allocs&quot;对应的默认视角则是“已分配空间”（alloc_space）。&nbsp;“在用空间”是指，已经被分配但还未被释放的内存空间。在这个视角下，go tool pprof工具并不会去理会与已释放空间有关的那部分信息。而在“已分配空间”的视角下，所有的内存分配信息都会被展现出来，无论这些内存空间在采样时是否已被释放。&nbsp;此外，无论是&quot;heap&quot;还是&quot;allocs&quot;，在我们调用Profile值的WriteTo方法的时候，只要赋予debug参数的值大于0，那么该方法输出内容的规格就会是相同的。&nbsp;参数值&quot;threadcreate&quot;会使Lookup函数去收集一些堆栈跟踪信息。这些堆栈跟踪信息中的每一个都会描绘出一个代码调用链，这些调用链上的代码都导致新的操作系统线程产生。这样的Profile值的输出规格也只有两种，取决于我们传给其WriteTo方法的参数值是否大于0。&nbsp;再说&quot;block&quot;和&quot;mutex&quot;。&quot;block&quot;代表的是，因争用同步原语而被阻塞的那些代码的堆栈跟踪信息。还记得吗？这就是我们在前面讲过的阻塞概要信息。&nbsp;与之相对应，&quot;mutex&quot;代表的是，曾经作为同步原语持有者的那些代码，它们的堆栈跟踪信息。它们的输出规格也都只有两种，取决于debug是否大于0。&nbsp;这里所说的同步原语，指的是存在于 Go 语言运行时系统内部的一种底层的同步工具，或者说一种同步机制。&nbsp;它是直接面向内存地址的，并以异步信号量和原子操作作为实现手段。我们已经熟知的通道、互斥锁、条件变量、WaitGroup，以及 Go 语言运行时系统本身，都会利用它来实现自己的功能。 如何为基于 HTTP 协议的网络服务添加性能分析接口？&nbsp;这个问题说起来还是很简单的。这是因为我们在一般情况下只要在程序中导入net/http/pprof代码包就可以了，就像这样： 1import _ &quot;net/http/pprof&quot; 然后，启动网络服务并开始监听，比如： 1log.Println(http.ListenAndServe(&quot;localhost:8082&quot;, nil)) 在运行这个程序之后，我们就可以通过在网络浏览器中访问http://localhost:8082/debug/pprof这个地址看到一个简约的网页。如果你认真地看了上一个问题的话，那么肯定可以快速搞明白这个网页中各个部分的含义。&nbsp;在/debug/pprof/这个 URL 路径下还有很多可用的子路径，这一点你通过点选网页中的链接就可以了解到。像allocs、block、goroutine、heap、mutex、threadcreate这 6 个子路径，在底层其实都是通过Lookup函数来处理的。关于这个函数，你应该已经很熟悉了。&nbsp;这些子路径都可以接受查询参数debug。它用于控制概要信息的格式和详细程度。至于它的可选值，我就不再赘述了。它的缺省值是0。另外，还有一个名叫gc的查询参数。它用于控制是否在获取概要信息之前强制地执行一次垃圾回收。只要它的值大于0，程序就会这样做。不过，这个参数仅在/debug/pprof/heap路径下有效。&nbsp;一旦/debug/pprof/profile路径被访问，程序就会去执行对 CPU 概要信息的采样。它接受一个名为seconds的查询参数。该参数的含义是，采样工作需要持续多少秒。如果这个参数未被显式地指定，那么采样工作会持续30秒。注意，在这个路径下，程序只会响应经 protocol buffers 转换的字节流。我们可以通过go tool pprof工具直接读取这样的 HTTP 响应，例如： 1go tool pprof http://localhost:6060/debug/pprof/profile?seconds=60 除此之外，还有一个值得我们关注的路径，即：/debug/pprof/trace。在这个路径下，程序主要会利用runtime/trace代码包中的 API 来处理我们的请求。&nbsp;更具体地说，程序会先调用trace.Start函数，然后在查询参数seconds指定的持续时间之后再调用trace.Stop函数。这里的seconds的缺省值是1秒。&nbsp;前面说的这些 URL 路径都是固定不变的。这是默认情况下的访问规则。我们还可以对它们进行定制，就像这样： 1234567891011121314151617181920mux := http.NewServeMux()pathPrefix := &quot;/d/pprof/&quot;mux.HandleFunc(pathPrefix, func(w http.ResponseWriter, r *http.Request) &#123; name := strings.TrimPrefix(r.URL.Path, pathPrefix) if name != &quot;&quot; &#123; pprof.Handler(name).ServeHTTP(w, r) return &#125; pprof.Index(w, r) &#125;)mux.HandleFunc(pathPrefix+&quot;cmdline&quot;, pprof.Cmdline)mux.HandleFunc(pathPrefix+&quot;profile&quot;, pprof.Profile)mux.HandleFunc(pathPrefix+&quot;symbol&quot;, pprof.Symbol)mux.HandleFunc(pathPrefix+&quot;trace&quot;, pprof.Trace) server := http.Server&#123; Addr: &quot;localhost:8083&quot;, Handler: mux,&#125; 可以看到，我们几乎只使用了net/http/pprof代码包中的几个程序实体，就完成了这样的定制。这在我们使用第三方的网络服务开发框架时尤其有用。&nbsp;我们自定义的 HTTP 请求多路复用器mux所包含的访问规则与默认的规则很相似，只不过 URL 路径的前缀更短了一些而已。&nbsp;我们定制mux的过程与net/http/pprof包中的init函数所做的事情也是类似的。这个init函数的存在，其实就是我们在前面仅仅导入net/http/pprof代码包就能够访问相关路径的原因。&nbsp;在我们编写网络服务程序的时候，使用net/http/pprof包要比直接使用runtime/pprof包方便和实用很多。通过合理运用，这个代码包可以为网络服务的监测提供有力的支撑。&nbsp;runtime/trace代码包的功用是什么？&nbsp;简单来说，这个代码包是用来帮助 Go 程序实现内部跟踪操作的。其中的程序实体可以帮助我们记录程序中各个 goroutine 的状态、各种系统调用的状态，与 GC 有关的各种事件，以及内存相关和 CPU 相关的变化，等等。&nbsp;通过它们生成的跟踪记录可以通过go tool trace命令来查看。更具体的说明可以参看runtime/trace代码包的文档。&nbsp;有了runtime/trace代码包，我们就可以为 Go 程序加装上可以满足个性化需求的跟踪器了。Go 语言标准库中有的代码包正是通过使用该包实现了自身的功能，例如net/http/pprof包。&nbsp; &nbsp;原文链接：https://time.geekbang.org/column/intro/112","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Go语言","slug":"Go语言","permalink":"http://wht6.github.io/tags/Go%E8%AF%AD%E8%A8%80/"}]},{"title":"io包、bufio包和os包","slug":"io包中的接口和工具","date":"2021-08-01T12:00:00.000Z","updated":"2022-04-10T10:28:08.718Z","comments":true,"path":"posts/6c1c.html","link":"","permalink":"http://wht6.github.io/posts/6c1c.html","excerpt":"","text":"io包中的接口和工具先来看一下strings.Builder、strings.Reader和bytes.Buffer这三个数据类型实现了哪些接口。 &nbsp; strings.Builder类型主要用于构建字符串，它的指针类型实现的接口有io.Writer、io.ByteWriter和fmt.Stringer。另外，它其实还实现了一个io包的包级私有接口io.StringWriter&nbsp;strings.Reader类型主要用于读取字符串，它的指针类型实现的接口比较多，包括：&nbsp; io.Reader； io.ReaderAt； io.ByteReader； io.RuneReader； io.Seeker； io.ByteScanner； io.RuneScanner； io.WriterTo； &nbsp;共有 8 个，它们都是io包中的接口。其中，io.ByteScanner是io.ByteReader的扩展接口，而io.RuneScanner又是io.RuneReader的扩展接口。&nbsp;bytes.Buffer是集读、写功能于一身的数据类型，它非常适合作为字节序列的缓冲区。 它的指针类型实现的接口就更多了。更具体地说，该指针类型实现的读取相关的接口有下面几个。&nbsp; io.Reader； io.ByteReader； io.RuneReader； io.ByteScanner； io.RuneScanner； io.WriterTo；&nbsp; 共有 6 个。而其实现的写入相关的接口则有这些。&nbsp; io.Writer； io.ByteWriter； io.stringWriter； io.ReaderFrom； &nbsp;共 4 个。此外，它还实现了导出相关的接口fmt.Stringer。&nbsp; io 包中接口的好处与优势&nbsp;那么，这些类型实现了这么多的接口，其动机（或者说目的）究竟是什么呢？&nbsp;简单地说，这是为了提高不同程序实体之间的互操作性。远的不说，我们就以io包中的一些函数为例。&nbsp;在io包中，有这样几个用于拷贝数据的函数，它们是：&nbsp; io.Copy； io.CopyBuffer； io.CopyN。 &nbsp;虽然这几个函数在功能上都略有差别，但是它们都首先会接受两个参数，即：用于代表数据目的地、io.Writer类型的参数dst，以及用于代表数据来源的、io.Reader类型的参数src。这些函数的功能大致上都是把数据从src拷贝到dst。&nbsp;不论我们给予它们的第一个参数值是什么类型的，只要这个类型实现了io.Writer接口即可。&nbsp;同样的，无论我们传给它们的第二个参数值的实际类型是什么，只要该类型实现了io.Reader接口就行。&nbsp;一旦我们满足了这两个条件，这些函数几乎就可以正常地执行了。当然了，函数中还会对必要的参数值进行有效性的检查，如果检查不通过，它的执行也是不能够成功结束的。&nbsp;下面来看一段示例代码：&nbsp;1234567891011src := strings.NewReader( &quot;CopyN copies n bytes (or until an error) from src to dst. &quot; + &quot;It returns the number of bytes copied and &quot; + &quot;the earliest error encountered while copying.&quot;)dst := new(strings.Builder)written, err := io.CopyN(dst, src, 58)if err != nil &#123; fmt.Printf(&quot;error: %v\\n&quot;, err)&#125; else &#123; fmt.Printf(&quot;Written(%d): %q\\n&quot;, written, dst.String())&#125;&nbsp; 我先使用strings.NewReader创建了一个字符串读取器，并把它赋给了变量src，然后我又new了一个字符串构建器，并将其赋予了变量dst。 &nbsp;之后，我在调用io.CopyN函数的时候，把这两个变量的值都传了进去，同时把给这个函数的第三个参数值设定为了58。也就是说，我想从src中拷贝前58个字节到dst那里。&nbsp;虽然，变量src和dst的类型分别是strings.Reader和strings.Builder，但是当它们被传到io.CopyN函数的时候，就已经分别被包装成了io.Reader类型和io.Writer类型的值。io.CopyN函数也根本不会去在意，它们的实际类型到底是什么。&nbsp;为了优化的目的，io.CopyN函数中的代码会对参数值进行再包装，也会检测这些参数值是否还实现了别的接口，甚至还会去探求某个参数值被包装后的实际类型，是否为某个特殊的类型。&nbsp;但是，从总体上来看，这些代码都是面向参数声明中的接口来做的。io.CopyN函数的作者通过面向接口编程，极大地拓展了它的适用范围和应用场景。&nbsp;换个角度看，正因为strings.Reader类型和strings.Builder类型都实现了不少接口，所以它们的值才能够被使用在更广阔的场景中。&nbsp;换句话说，如此一来，Go 语言的各种库中，能够操作它们的函数和数据类型明显多了很多。&nbsp;这就是我想要告诉你的，strings包和bytes包中的数据类型在实现了若干接口之后得到的最大好处。&nbsp;也可以说，这就是面向接口编程带来的最大优势。这些数据类型和函数的做法，也是非常值得我们在编程的过程中去效仿的。&nbsp;可以看到，前文所述的几个类型实现的大都是io代码包中的接口。实际上，io包中的接口，对于 Go 语言的标准库和很多第三方库而言，都起着举足轻重的作用。它们非常基础也非常重要。&nbsp;就拿io.Reader和io.Writer这两个最核心的接口来说，它们是很多接口的扩展对象和设计源泉。同时，单从 Go 语言的标准库中统计，实现了它们的数据类型都（各自）有上百个，而引用它们的代码更是都（各自）有 400 多处。&nbsp;很多数据类型实现了io.Reader接口，是因为它们提供了从某处读取数据的功能。类似的，许多能够把数据写入某处的数据类型，也都会去实现io.Writer接口。&nbsp;其实，有不少类型的设计初衷都是：实现这两个核心接口的某个，或某些扩展接口，以提供比单纯的字节序列读取或写入，更加丰富的功能，就像前面讲到的那几个strings包和bytes包中的数据类型那样。&nbsp;在 Go 语言中，对接口的扩展是通过接口类型之间的嵌入来实现的，这也常被叫做接口的组合。&nbsp;Go 语言提倡使用小接口加接口组合的方式，来扩展程序的行为以及增加程序的灵活性。io代码包恰恰就可以作为这样的一个标杆，它可以成为我们运用这种技巧时的一个参考标准。&nbsp;在io包中，io.Reader的扩展接口和实现类型都有哪些？它们分别都有什么功用？&nbsp;在io包中，io.Reader的扩展接口有下面几种。&nbsp; io.ReadWriter：此接口既是io.Reader的扩展接口，也是io.Writer的扩展接口。换句话说，该接口定义了一组行为，包含且仅包含了基本的字节序列读取方法Read，和字节序列写入方法Write。 io.ReadCloser：此接口除了包含基本的字节序列读取方法之外，还拥有一个基本的关闭方法Close。后者一般用于关闭数据读写的通路。这个接口其实是io.Reader接口和io.Closer接口的组合。 io.ReadWriteCloser：很明显，此接口是io.Reader、io.Writer和io.Closer这三个接口的组合。 io.ReadSeeker：此接口的特点是拥有一个用于寻找读写位置的基本方法Seek。更具体地说，该方法可以根据给定的偏移量基于数据的起始位置、末尾位置，或者当前读写位置去寻找新的读写位置。这个新的读写位置用于表明下一次读或写时的起始索引。Seek是io.Seeker接口唯一拥有的方法。 io.ReadWriteSeeker：显然，此接口是另一个三合一的扩展接口，它是io.Reader、io.Writer和io.Seeker的组合。 &nbsp;再来说说io包中的io.Reader接口的实现类型，它们包括下面几项内容。&nbsp; *io.LimitedReader：此类型的基本类型会包装io.Reader类型的值，并提供一个额外的受限读取的功能。所谓的受限读取指的是，此类型的读取方法Read返回的总数据量会受到限制，无论该方法被调用多少次。这个限制由该类型的字段N指明，单位是字节。 *io.SectionReader：此类型的基本类型可以包装io.ReaderAt类型的值，并且会限制它的Read方法，只能够读取原始数据中的某一个部分（或者说某一段）。 这个数据段的起始位置和末尾位置，需要在它被初始化的时候就指明，并且之后无法变更。该类型值的行为与切片有些类似，它只会对外暴露在其窗口之中的那些数据。 *io.teeReader：此类型是一个包级私有的数据类型，也是io.TeeReader函数结果值的实际类型。这个函数接受两个参数r和w，类型分别是io.Reader和io.Writer。 其结果值的Read方法会把r中的数据经过作为方法参数的字节切片p写入到w。可以说，这个值就是r和w之间的数据桥梁，而那个参数p就是这座桥上的数据搬运者。 io.multiReader：此类型也是一个包级私有的数据类型。类似的，io包中有一个名为MultiReader的函数，它可以接受若干个io.Reader类型的参数值，并返回一个实际类型为io.multiReader的结果值。 当这个结果值的Read方法被调用时，它会顺序地从前面那些io.Reader类型的参数值中读取数据。因此，我们也可以称之为多对象读取器。 io.pipe：此类型为一个包级私有的数据类型，它比上述类型都要复杂得多。它不但实现了io.Reader接口，而且还实现了io.Writer接口。 实际上，io.PipeReader类型和io.PipeWriter类型拥有的所有指针方法都是以它为基础的。这些方法都只是代理了io.pipe类型值所拥有的某一个方法而已。 又因为io.Pipe函数会返回这两个类型的指针值并分别把它们作为其生成的同步内存管道的两端，所以可以说，*io.pipe类型就是io包提供的同步内存管道的核心实现。 io.PipeReader：此类型可以被视为io.pipe类型的代理类型。它代理了后者的一部分功能，并基于后者实现了io.ReadCloser接口。同时，它还定义了同步内存管道的读取端。 &nbsp;注意，我在这里忽略掉了测试源码文件中的实现类型，以及不会以任何形式直接对外暴露的那些实现类型。&nbsp;这个代码包是 Go 语言标准库中所有 I/O 相关 API 的根基，所以，我们必须对其中的每一个程序实体都有所了解。&nbsp;然而，由于该包包含的内容众多，因此这里的问题是以io.Reader接口作为切入点的。通过io.Reader接口，我们应该能够梳理出基于它的类型树，并知晓其中每一个类型的功用。&nbsp;io.Reader可谓是io包乃至是整个 Go 语言标准库中的核心接口，所以我们可以从它那里牵扯出很多扩展接口和实现类型。&nbsp;这些类型中的每一个都值得你认真去理解，尤其是那几个实现了io.Reader接口的类型。它们实现的功能在细节上都各有不同。&nbsp;在很多时候，我们可以根据实际需求将它们搭配起来使用。&nbsp;例如，对施加在原始数据之上的（由Read方法提供的）读取功能进行多层次的包装（比如受限读取和多对象读取等），以满足较为复杂的读取需求。&nbsp;在实际的面试中，只要应聘者能够从某一个方面出发，说出io.Reader的扩展接口及其存在意义，或者说清楚该接口的三五个实现类型，那么就可以算是基本回答正确了。&nbsp;比如，从读取、写入、关闭这一些列的基本功能出发，描述清楚：&nbsp; io.ReadWriter； io.ReadCloser； io.ReadWriteCloser； &nbsp;这几个接口。&nbsp;又比如，说明白io.LimitedReader和io.SectionReader这两个类型之间的异同点。&nbsp;再比如，阐述*io.SectionReader类型实现io.ReadSeeker接口的具体方式，等等。不过，这只是合格的门槛，应聘者回答得越全面越好。&nbsp;io包中的接口都有哪些？它们之间都有着怎样的关系？&nbsp;我们可以把没有嵌入其他接口并且只定义了一个方法的接口叫做简单接口。在io包中，这样的接口一共有 11 个。&nbsp;在它们之中，有的接口有着众多的扩展接口和实现类型，我们可以称之为核心接口。io包中的核心接口只有 3 个，它们是：io.Reader、io.Writer和io.Closer。&nbsp;我们还可以把io包中的简单接口分为四大类。这四大类接口分别针对于四种操作，即：读取、写入、关闭和读写位置设定。前三种操作属于基本的 I/O 操作。&nbsp;关于读取操作，我们在前面已经重点讨论过核心接口io.Reader。它在io包中有 5 个扩展接口，并有 6 个实现类型。除了它，这个包中针对读取操作的接口还有不少。我们下面就来梳理一下。&nbsp;首先来看io.ByteReader和io.RuneReader这两个简单接口。它们分别定义了一个读取方法，即：ReadByte和ReadRune。&nbsp;但与io.Reader接口中Read方法不同的是，这两个读取方法分别只能够读取下一个单一的字节和 Unicode 字符。&nbsp;我们之前讲过的数据类型strings.Reader和bytes.Buffer都是io.ByteReader和io.RuneReader的实现类型。&nbsp;不仅如此，这两个类型还都实现了io.ByteScanner接口和io.RuneScanner接口。&nbsp;io.ByteScanner接口内嵌了简单接口io.ByteReader，并定义了额外的UnreadByte方法。如此一来，它就抽象出了一个能够读取和读回退单个字节的功能集。&nbsp;与之类似，io.RuneScanner内嵌了简单接口io.RuneReader，并定义了额外的UnreadRune方法。它抽象的是可以读取和读回退单个 Unicode 字符的功能集。&nbsp;再来看io.ReaderAt接口。它也是一个简单接口，其中只定义了一个方法ReadAt。与我们在前面说过的读取方法都不同，ReadAt是一个纯粹的只读方法。&nbsp;它只去读取其所属值中包含的字节，而不对这个值进行任何的改动，比如，它绝对不能去修改已读计数的值。这也是io.ReaderAt接口与其实现类型之间最重要的一个约定。&nbsp;因此，如果仅仅并发地调用某一个值的ReadAt方法，那么安全性应该是可以得到保障的。&nbsp;另外，还有一个读取操作相关的接口我们没有介绍过，它就是io.WriterTo。这个接口定义了一个名为WriteTo的方法。&nbsp;千万不要被它的名字迷惑，这个WriteTo方法其实是一个读取方法。它会接受一个io.Writer类型的参数值，并会把其所属值中的数据读出并写入到这个参数值中。&nbsp;与之相对应的是io.ReaderFrom接口。它定义了一个名叫ReadFrom的写入方法。该方法会接受一个io.Reader类型的参数值，并会从该参数值中读出数据, 并写入到其所属值中。&nbsp;值得一提的是，我们在前面用到过的io.CopyN函数，在复制数据的时候会先检测其参数src的值，是否实现了io.WriterTo接口。如果是，那么它就直接利用该值的WriteTo方法，把其中的数据拷贝给参数dst代表的值。&nbsp;类似的，这个函数还会检测dst的值是否实现了io.ReaderFrom接口。如果是，那么它就会利用这个值的ReadFrom方法，直接从src那里把数据拷贝进该值。&nbsp;实际上，对于io.Copy函数和io.CopyBuffer函数来说也是如此，因为它们在内部做数据复制的时候用的都是同一套代码。&nbsp;你也看到了，io.ReaderFrom接口与io.WriterTo接口对应得很规整。实际上，在io包中，与写入操作有关的接口都与读取操作的相关接口有着一定的对应关系。下面，我们就来说说写入操作相关的接口。&nbsp;首先当然是核心接口io.Writer。基于它的扩展接口除了有我们已知的io.ReadWriter、io.ReadWriteCloser和io.ReadWriteSeeker之外，还有io.WriteCloser和io.WriteSeeker。&nbsp;我们之前提及的*io.pipe就是io.ReadWriter接口的实现类型。然而，在io包中并没有io.ReadWriteCloser接口的实现，它的实现类型主要集中在net包中。&nbsp;除此之外，写入操作相关的简单接口还有io.ByteWriter和io.WriterAt。可惜，io包中也没有它们的实现类型。不过，有一个数据类型值得在这里提一句，那就是*os.File。&nbsp;这个类型不但是io.WriterAt接口的实现类型，还同时实现了io.ReadWriteCloser接口和io.ReadWriteSeeker接口。也就是说，该类型支持的 I/O 操作非常的丰富。&nbsp;io.Seeker接口作为一个读写位置设定相关的简单接口，也仅仅定义了一个方法，名叫Seek。&nbsp;我在讲strings.Reader类型的时候还专门说过这个Seek方法，当时还给出了一个与已读计数估算有关的例子。该方法主要用于寻找并设定下一次读取或写入时的起始索引位置。&nbsp;io包中有几个基于io.Seeker的扩展接口，包括前面讲过的io.ReadSeeker和io.ReadWriteSeeker，以及还未曾提过的io.WriteSeeker。io.WriteSeeker是基于io.Writer和io.Seeker的扩展接口。&nbsp;我们之前多次提到的两个指针类型strings.Reader和io.SectionReader都实现了io.Seeker接口。顺便说一句，这两个类型也都是io.ReaderAt接口的实现类型。&nbsp;最后，关闭操作相关的接口io.Closer非常通用，它的扩展接口和实现类型都不少。我们单从名称上就能够一眼看出io包中的哪些接口是它的扩展接口。至于它的实现类型，io包中只有io.PipeReader和io.PipeWriter。 bufio包中的数据类型bufio是“buffered I/O”的缩写。顾名思义，这个代码包中的程序实体实现的 I/O 操作都内置了缓冲区。 &nbsp; bufio包中的数据类型主要有：&nbsp; Reader； Scanner； Writer和ReadWriter。 &nbsp;与io包中的数据类型类似，这些类型的值也都需要在初始化的时候，包装一个或多个简单 I/O 接口类型的值。&nbsp;（这里的简单 I/O 接口类型指的就是io包中的那些简单接口。）&nbsp;bufio.Reader类型值中的缓冲区起着怎样的作用？&nbsp;bufio.Reader类型的值（以下简称Reader值）内的缓冲区，其实就是一个数据存储中介，它介于底层读取器与读取方法及其调用方之间。所谓的底层读取器，就是在初始化此类值的时候传入的io.Reader类型的参数值。&nbsp;Reader值的读取方法一般都会先从其所属值的缓冲区中读取数据。同时，在必要的时候，它们还会预先从底层读取器那里读出一部分数据，并暂存于缓冲区之中以备后用。&nbsp;有这样一个缓冲区的好处是，可以在大多数的时候降低读取方法的执行时间。虽然，读取方法有时还要负责填充缓冲区，但从总体来看，读取方法的平均执行时间一般都会因此有大幅度的缩短。&nbsp;bufio.Reader类型并不是开箱即用的，因为它包含了一些需要显式初始化的字段。为了让你能在后面更好地理解它的读取方法的内部流程，我先在这里简要地解释一下这些字段，如下所示。&nbsp; buf：[]byte类型的字段，即字节切片，代表缓冲区。虽然它是切片类型的，但是其长度却会在初始化的时候指定，并在之后保持不变。 rd：io.Reader类型的字段，代表底层读取器。缓冲区中的数据就是从这里拷贝来的。 r：int类型的字段，代表对缓冲区进行下一次读取时的开始索引。我们可以称它为已读计数。 w：int类型的字段，代表对缓冲区进行下一次写入时的开始索引。我们可以称之为已写计数。 err：error类型的字段。它的值用于表示在从底层读取器获得数据时发生的错误。这里的值在被读取或忽略之后，该字段会被置为nil。 lastByte：int类型的字段，用于记录缓冲区中最后一个被读取的字节。读回退时会用到它的值。 lastRuneSize：int类型的字段，用于记录缓冲区中最后一个被读取的 Unicode 字符所占用的字节数。读回退的时候会用到它的值。这个字段只会在其所属值的ReadRune方法中才会被赋予有意义的值。在其他情况下，它都会被置为-1。 &nbsp;bufio包为我们提供了两个用于初始化Reader值的函数，分别叫：&nbsp; NewReader； NewReaderSize； &nbsp;它们都会返回一个*bufio.Reader类型的值。&nbsp;NewReader函数初始化的Reader值会拥有一个默认尺寸的缓冲区。这个默认尺寸是 4096 个字节，即：4 KB。而NewReaderSize函数则将缓冲区尺寸的决定权抛给了使用方。&nbsp;由于这里的缓冲区在一个Reader值的生命周期内其尺寸不可变，所以在有些时候是需要做一些权衡的。NewReaderSize函数就提供了这样一个途径。&nbsp;在bufio.Reader类型拥有的读取方法中，Peek方法和ReadSlice方法都会调用该类型一个名为fill的包级私有方法。fill方法的作用是填充内部缓冲区。我们在这里就先重点说说它。&nbsp;fill方法会先检查其所属值的已读计数。如果这个计数不大于0，那么有两种可能。&nbsp;一种可能是其缓冲区中的字节都是全新的，也就是说它们都没有被读取过，另一种可能是缓冲区刚被压缩过。&nbsp;对缓冲区的压缩包括两个步骤。第一步，把缓冲区中在[已读计数, 已写计数)范围之内的所有元素值（或者说字节）都依次拷贝到缓冲区的头部。&nbsp;比如，把缓冲区中与已读计数代表的索引对应字节拷贝到索引0的位置，并把紧挨在它后边的字节拷贝到索引1的位置，以此类推。&nbsp;这一步之所以不会有任何副作用，是因为它基于两个事实。&nbsp;第一事实，已读计数之前的字节都已经被读取过，并且肯定不会再被读取了，因此把它们覆盖掉是安全的。&nbsp;第二个事实，在压缩缓冲区之后，已写计数之后的字节只可能是已被读取过的字节，或者是已被拷贝到缓冲区头部的未读字节，又或者是代表未曾被填入数据的零值0x00。所以，后续的新字节是可以被写到这些位置上的。&nbsp;在压缩缓冲区的第二步中，fill方法会把已写计数的新值设定为原已写计数与原已读计数的差。这个差所代表的索引，就是压缩后第一次写入字节时的开始索引。&nbsp;另外，该方法还会把已读计数的值置为0。显而易见，在压缩之后，再读取字节就肯定要从缓冲区的头部开始读了。 实际上，fill方法只要在开始时发现其所属值的已读计数大于0，就会对缓冲区进行一次压缩。之后，如果缓冲区中还有可写的位置，那么该方法就会对其进行填充。&nbsp;在填充缓冲区的时候，fill方法会试图从底层读取器那里，读取足够多的字节，并尽量把从已写计数代表的索引位置到缓冲区末尾之间的空间都填满。&nbsp;在这个过程中，fill方法会及时地更新已写计数，以保证填充的正确性和顺序性。另外，它还会判断从底层读取器读取数据的时候，是否有错误发生。如果有，那么它就会把错误值赋给其所属值的err字段，并终止填充流程。&nbsp;bufio.Writer类型值中缓冲的数据什么时候会被写到它的底层写入器？&nbsp;我们先来看一下bufio.Writer类型都有哪些字段： err：error类型的字段。它的值用于表示在向底层写入器写数据时发生的错误。 buf：[]byte类型的字段，代表缓冲区。在初始化之后，它的长度会保持不变。 n：int类型的字段，代表对缓冲区进行下一次写入时的开始索引。我们可以称之为已写计数。 wr：io.Writer类型的字段，代表底层写入器。 bufio.Writer类型有一个名为Flush的方法，它的主要功能是把相应缓冲区中暂存的所有数据，都写到底层写入器中。数据一旦被写进底层写入器，该方法就会把它们从缓冲区中删除掉。&nbsp;不过，这里的删除有时候只是逻辑上的删除而已。不论是否成功地写入了所有的暂存数据，Flush方法都会妥当处置，并保证不会出现重写和漏写的情况。该类型的字段n在此会起到很重要的作用。&nbsp;bufio.Writer类型值（以下简称Writer值）拥有的所有数据写入方法都会在必要的时候调用它的Flush方法。&nbsp;比如，Write方法有时候会在把数据写进缓冲区之后，调用Flush方法，以便为后续的新数据腾出空间。WriteString方法的行为与之类似。&nbsp;又比如，WriteByte方法和WriteRune方法，都会在发现缓冲区中的可写空间不足以容纳新的字节，或 Unicode 字符的时候，调用Flush方法。&nbsp;此外，如果Write方法发现需要写入的字节太多，同时缓冲区已空，那么它就会跨过缓冲区，并直接把这些数据写到底层写入器中。&nbsp;而ReadFrom方法，则会在发现底层写入器的类型是io.ReaderFrom接口的实现之后，直接调用其ReadFrom方法把参数值持有的数据写进去。&nbsp;总之，在通常情况下，只要缓冲区中的可写空间无法容纳需要写入的新数据，Flush方法就一定会被调用。并且，bufio.Writer类型的一些方法有时候还会试图走捷径，跨过缓冲区而直接对接数据供需的双方。&nbsp;你可以在理解了这些内部机制之后，有的放矢地编写你的代码。不过，在你把所有的数据都写入Writer值之后，再调用一下它的Flush方法，显然是最稳妥的。&nbsp;bufio.Reader类型读取方法有哪些不同？&nbsp;bufio.Reader类型拥有很多用于读取数据的指针方法，这里面有 4 个方法可以作为不同读取流程的代表，它们是：Peek、Read、ReadSlice和ReadBytes。&nbsp;Reader值的Peek方法的功能是：读取并返回其缓冲区中的n个未读字节，并且它会从已读计数代表的索引位置开始读。&nbsp;在缓冲区未被填满，并且其中的未读字节的数量小于n的时候，该方法就会调用fill方法，以启动缓冲区填充流程。但是，如果它发现上次填充缓冲区的时候有错误，那就不会再次填充。&nbsp;如果调用方给定的n比缓冲区的长度还要大，或者缓冲区中未读字节的数量小于n，那么Peek方法就会把“所有未读字节组成的序列”作为第一个结果值返回。&nbsp;同时，它通常还把“bufio.ErrBufferFull变量的值（以下简称缓冲区已满的错误）”作为第二个结果值返回，用来表示：虽然缓冲区被压缩和填满了，但是仍然满足不了要求。&nbsp;只有在上述的情况都没有出现时，Peek方法才能返回：“以已读计数为起始的n个字节”和“表示未发生任何错误的nil”。&nbsp;bufio.Reader类型的 Peek 方法有一个鲜明的特点，那就是：即使它读取了缓冲区中的数据，也不会更改已读计数的值。&nbsp;这个类型的其他读取方法并不是这样。就拿该类型的Read方法来说，它有时会把缓冲区中的未读字节，依次拷贝到其参数p代表的字节切片中，并立即根据实际拷贝的字节数增加已读计数的值。 在缓冲区中还有未读字节的情况下，该方法的做法就是如此。不过，在另一些时候，其所属值的已读计数会等于已写计数，这表明：此时的缓冲区中已经没有任何未读的字节了。 当缓冲区中已无未读字节时，Read方法会先检查参数p的长度是否大于或等于缓冲区的长度。如果是，那么Read方法会索性放弃向缓冲区中填充数据，转而直接从其底层读取器中读出数据并拷贝到p中。这意味着它完全跨过了缓冲区，并直连了数据供需的双方。 &nbsp;需要注意的是，Peek方法在遇到类似情况时的做法与这里的区别（这两种做法孰优孰劣还要看具体的使用场景）。&nbsp;Peek方法会在条件满足时填充缓冲区，并在发现参数n的值比缓冲区的长度更大时，直接返回缓冲区中的所有未读字节。&nbsp;如果我们当初设定的缓冲区长度很大，那么在这种情况下的方法执行耗时，就有可能会比较长。最主要的原因是填充缓冲区需要花费较长的时间。&nbsp;由fill方法执行的流程可知，它会尽量填满缓冲区中的可写空间。然而，Read方法在大多数的情况下，是不会向缓冲区中写入数据的，尤其是在前面描述的那种情况下，即：缓冲区中已无未读字节，且参数p的长度大于或等于缓冲区的长度。&nbsp;此时，该方法会直接从底层读取器那里读出数据，所以数据的读出速度就成为了这种情况下方法执行耗时的决定性因素。&nbsp;当然了，我在这里说的只是耗时操作在某些情况下更可能出现在哪里，一切的结论还是要以性能测试的客观结果为准。&nbsp;说回Read方法的内部流程。如果缓冲区中已无未读字节，但其长度比参数p的长度更大，那么该方法会先把已读计数和已写计数的值都重置为0，然后再尝试着使用从底层读取器那里获取的数据，对缓冲区进行一次从头至尾的填充。&nbsp;不过要注意，这里的尝试只会进行一次。无论在这一时刻是否能够获取到数据，也无论获取时是否有错误发生，都会是如此。而fill方法的做法与此不同，只要没有发生错误，它就会进行多次尝试，因此它真正获取到一些数据的可能性更大。&nbsp;不过，这两个方法有一点是相同，那就是：只要它们把获取到的数据写入缓冲区，就会及时地更新已写计数的值。&nbsp;再来说ReadSlice方法和ReadBytes方法。 这两个方法的功能总体上来说，都是持续地读取数据，直至遇到调用方给定的分隔符为止。&nbsp;ReadSlice方法会先在其缓冲区的未读部分中寻找分隔符。如果未能找到，并且缓冲区未满，那么该方法会先通过调用fill方法对缓冲区进行填充，然后再次寻找，如此往复。&nbsp;如果在填充的过程中发生了错误，那么它会把缓冲区中的未读部分作为结果返回，同时返回相应的错误值。&nbsp;注意，在这个过程中有可能会出现虽然缓冲区已被填满，但仍然没能找到分隔符的情况。&nbsp;这时，ReadSlice方法会把整个缓冲区（也就是buf字段代表的字节切片）作为第一个结果值，并把缓冲区已满的错误（即bufio.ErrBufferFull变量的值）作为第二个结果值。&nbsp;经过fill方法填满的缓冲区肯定从头至尾都只包含了未读的字节，所以这样做是合理的。&nbsp;当然了，一旦ReadSlice方法找到了分隔符，它就会在缓冲区上切出相应的、包含分隔符的字节切片，并把该切片作为结果值返回。无论分隔符找到与否，该方法都会正确地设置已读计数的值。&nbsp;比如，在返回缓冲区中的所有未读字节，或者代表全部缓冲区的字节切片之前，它会把已写计数的值赋给已读计数，以表明缓冲区中已无未读字节。&nbsp;如果说ReadSlice是一个容易半途而废的方法的话，那么可以说ReadBytes方法算得上是相当的执着。&nbsp;ReadBytes方法会通过调用ReadSlice方法一次又一次地从缓冲区中读取数据，直至找到分隔符为止。&nbsp;在这个过程中，ReadSlice方法可能会因缓冲区已满而返回所有已读到的字节和相应的错误值，但ReadBytes方法总是会忽略掉这样的错误，并再次调用ReadSlice方法，这使得后者会继续填充缓冲区并在其中寻找分隔符。&nbsp;除非ReadSlice方法返回的错误值并不代表缓冲区已满的错误，或者它找到了分隔符，否则这一过程永远不会结束。&nbsp;如果寻找的过程结束了，不管是不是因为找到了分隔符，ReadBytes方法都会把在这个过程中读到的所有字节，按照读取的先后顺序组装成一个字节切片，并把它作为第一个结果值。如果过程结束是因为出现错误，那么它还会把拿到的错误值作为第二个结果值。&nbsp;在bufio.Reader类型的众多读取方法中，依赖ReadSlice方法的除了ReadBytes方法，还有ReadLine方法。不过后者在读取流程上并没有什么特别之处，我就不在这里赘述了。&nbsp;另外，该类型的ReadString方法完全依赖于ReadBytes方法，前者只是在后者返回的结果值之上做了一个简单的类型转换而已。&nbsp;最后，我还要提醒你一下，有个安全性方面的问题需要你注意。bufio.Reader类型的Peek方法、ReadSlice方法和ReadLine方法都有可能会造成内容泄露。&nbsp;这主要是因为它们在正常的情况下都会返回直接基于缓冲区的字节切片。&nbsp;调用方可以通过这些方法返回的结果值访问到缓冲区的其他部分，甚至修改缓冲区中的内容。这通常都是很危险的。&nbsp;bufio.Scanner类型的主要功用是什么？它有哪些特点？&nbsp;bufio.Scanner类型俗称带缓存的扫描器。它的功能还是比较强大的。&nbsp;比如，我们可以自定义每次扫描的边界，或者说内容的分段方法。我们在调用它的Scan方法对目标进行扫描之前，可以先调用其Split方法并传入一个函数来自定义分段方法。&nbsp;在默认情况下，扫描器会以行为单位对目标内容进行扫描。bufio代码包提供了一些现成的分段方法。实际上，扫描器在默认情况下会使用bufio.ScanLines函数作为分段方法。&nbsp;又比如，我们还可以在扫描之前自定义缓存的载体和缓存的最大容量，这需要调用它的Buffer方法。在默认情况下，扫描器内部设定的最大缓存容量是64K个字节。&nbsp;换句话说，目标内容中的每一段都不能超过64K个字节。否则，扫描器就会使它的Scan方法返回false，并通过其Err方法给予我们一个表示“token too long”的错误值。这里的“token”代表的就是一段内容。 使用os包中的APIos 包中的 API&nbsp;这个代码包提供的都是平台不相关的 API。那么说，什么叫平台不相关的 API 呢？&nbsp;它的意思是：这些 API 基于（或者说抽象自）操作系统，为我们使用操作系统的功能提供高层次的支持，但是，它们并不依赖于具体的操作系统。&nbsp;不论是 Linux、macOS、Windows，还是 FreeBSD、OpenBSD、Plan9，os代码包都可以为之提供统一的使用接口。这使得我们可以用同样的方式，来操纵不同的操作系统，并得到相似的结果。&nbsp;os包中的 API 主要可以帮助我们使用操作系统中的文件系统、权限系统、环境变量、系统进程以及系统信号。&nbsp;其中，操纵文件系统的 API 最为丰富。我们不但可以利用这些 API 创建和删除文件以及目录，还可以获取到它们的各种信息、修改它们的内容、改变它们的访问权限，等等。&nbsp;说到这里，就不得不提及一个非常常用的数据类型：os.File。&nbsp;从字面上来看，os.File类型代表了操作系统中的文件。但实际上，它可以代表的远不止于此。或许你已经知道，对于类 Unix 的操作系统（包括 Linux、macOS、FreeBSD 等），其中的一切都可以被看做是文件。&nbsp;除了文本文件、二进制文件、压缩文件、目录这些常见的形式之外，还有符号链接、各种物理设备（包括内置或外接的面向块或者字符的设备）、命名管道，以及套接字（也就是 socket），等等。&nbsp;因此，可以说，我们能够利用os.File类型操纵的东西太多了。不过，为了聚焦于os.File本身，同时也为了让本文讲述的内容更加通用，我们在这里主要把os.File类型应用于常规的文件。&nbsp;os.File类型都实现了哪些io包中的接口？&nbsp;os.File类型拥有的都是指针方法，所以除了空接口之外，它本身没有实现任何接口。而它的指针类型则实现了很多io代码包中的接口。&nbsp;首先，对于io包中最核心的 3 个简单接口io.Reader、io.Writer和io.Closer，*os.File类型都实现了它们。&nbsp;其次，该类型还实现了另外的 3 个简单接口，即：io.ReaderAt、io.Seeker和io.WriterAt。&nbsp;正是因为*os.File类型实现了这些简单接口，所以它也顺便实现了io包的 9 个扩展接口中的 7 个。&nbsp;然而，由于它并没有实现简单接口io.ByteReader和io.RuneReader，所以它没有实现分别作为这两者的扩展接口的io.ByteScanner和io.RuneScanner。&nbsp;总之，os.File类型及其指针类型的值，不但可以通过各种方式读取和写入某个文件中的内容，还可以寻找并设定下一次读取或写入时的起始索引位置，另外还可以随时对文件进行关闭。&nbsp;但是，它们并不能专门地读取文件中的下一个字节，或者下一个 Unicode 字符，也不能进行任何的读回退操作。&nbsp;不过，单独读取下一个字节或字符的功能也可以通过其他方式来实现，比如，调用它的Read方法并传入适当的参数值就可以做到这一点。&nbsp;怎样才能获得一个os.File类型的指针值（以下简称File值）。&nbsp;在os包中，有这样几个函数，即：Create、NewFile、Open和OpenFile。&nbsp;os.Create函数用于根据给定的路径创建一个新的文件。 它会返回一个File值和一个错误值。我们可以在该函数返回的File值之上，对相应的文件进行读操作和写操作。&nbsp;不但如此，我们使用这个函数创建的文件，对于操作系统中的所有用户来说，都是可以读和写的。&nbsp;换句话说，一旦这样的文件被创建出来，任何能够登录其所属的操作系统的用户，都可以在任意时刻读取该文件中的内容，或者向该文件写入内容。&nbsp;注意，如果在我们给予os.Create函数的路径之上，已经存在了一个文件，那么该函数会先清空现有文件中的全部内容，然后再把它作为第一个结果值返回。&nbsp;另外，os.Create函数是有可能返回非nil的错误值的。&nbsp;比如，如果我们给定的路径上的某一级父目录并不存在，那么该函数就会返回一个*os.PathError类型的错误值，以表示“不存在的文件或目录”。&nbsp;再来看os.NewFile函数。 该函数在被调用的时候，需要接受一个代表文件描述符的、uintptr类型的值，以及一个用于表示文件名的字符串值。&nbsp;如果我们给定的文件描述符并不是有效的，那么这个函数将会返回nil，否则，它将会返回一个代表了相应文件的File值。&nbsp;注意，不要被这个函数的名称误导了，它的功能并不是创建一个新的文件，而是依据一个已经存在的文件的描述符，来新建一个包装了该文件的File值。&nbsp;例如，我们可以像这样拿到一个包装了标准错误输出的File值：&nbsp; 1file3 := os.NewFile(uintptr(syscall.Stderr), &quot;/dev/stderr&quot;) &nbsp; 然后，通过这个File值向标准错误输出上写入一些内容： &nbsp; 12345if file3 != nil &#123; defer file3.Close() file3.WriteString( &quot;The Go language program writes the contents into stderr.\\n&quot;)&#125; &nbsp; os.Open函数会打开一个文件并返回包装了该文件的File值。 然而，该函数只能以只读模式打开文件。换句话说，我们只能从该函数返回的File值中读取内容，而不能向它写入任何内容。 &nbsp;如果我们调用了这个File值的任何一个写入方法，那么都将会得到一个表示了“坏的文件描述符”的错误值。实际上，我们刚刚说的只读模式，正是应用在File值所持有的文件描述符之上的。&nbsp;所谓的文件描述符，是由通常很小的非负整数代表的。它一般会由 I/O 相关的系统调用返回，并作为某个文件的一个标识存在。&nbsp;从操作系统的层面看，针对任何文件的 I/O 操作都需要用到这个文件描述符。只不过，Go 语言中的一些数据类型，为我们隐匿掉了这个描述符，如此一来我们就无需时刻关注和辨别它了（就像os.File类型这样）。&nbsp;实际上，我们在调用前文所述的os.Create函数、os.Open函数以及将会提到的os.OpenFile函数的时候，它们都会执行同一个系统调用，并且在成功之后得到这样一个文件描述符。这个文件描述符将会被储存在它们返回的File值中。&nbsp;os.File类型有一个指针方法，名叫Fd。它在被调用之后将会返回一个uintptr类型的值。这个值就代表了当前的File值所持有的那个文件描述符。&nbsp;不过，在os包中，除了NewFile函数需要用到它，它也没有什么别的用武之地了。所以，如果你操作的只是常规的文件或者目录，那么就无需特别地在意它了。&nbsp;最后，再说一下os.OpenFile函数。 这个函数其实是os.Create函数和os.Open函数的底层支持，它最为灵活。&nbsp;这个函数有 3 个参数，分别名为name、flag和perm。其中的name指代的就是文件的路径。而flag参数指的则是需要施加在文件描述符之上的模式，我在前面提到的只读模式就是这里的一个可选项。&nbsp;在 Go 语言中，这个只读模式由常量os.O_RDONLY代表，它是int类型的。当然了，这里除了只读模式之外，还有几个别的模式可选，我们稍后再细说。&nbsp;os.OpenFile函数的参数perm代表的也是模式，它的类型是os.FileMode，此类型是一个基于uint32类型的再定义类型。&nbsp;为了加以区别，我们把参数flag指代的模式叫做操作模式，而把参数perm指代的模式叫做权限模式。可以这么说，操作模式限定了操作文件的方式，而权限模式则可以控制文件的访问权限。 到这里，你需要记住的是，通过os.File类型的值，我们不但可以对文件进行读取、写入、关闭等操作，还可以设定下一次读取或写入时的起始索引位置。&nbsp;此外，os包中还有用于创建全新文件的Create函数，用于包装现存文件的NewFile函数，以及可被用来打开已存在的文件的Open函数和OpenFile函数。&nbsp; 操作模式和访问权限&nbsp;可应用于File值的操作模式都有哪些？&nbsp;针对File值的操作模式主要有只读模式、只写模式和读写模式。&nbsp;这些模式分别由常量os.O_RDONLY、os.O_WRONLY和os.O_RDWR代表。在我们新建或打开一个文件的时候，必须把这三个模式中的一个设定为此文件的操作模式。&nbsp;除此之外，我们还可以为这里的文件设置额外的操作模式，可选项如下所示。 os.O_APPEND：当向文件中写入内容时，把新内容追加到现有内容的后边。 os.O_CREATE：当给定路径上的文件不存在时，创建一个新文件。 os.O_EXCL：需要与os.O_CREATE一同使用，表示在给定的路径上不能有已存在的文件。 os.O_SYNC：在打开的文件之上实施同步 I/O。它会保证读写的内容总会与硬盘上的数据保持同步。 os.O_TRUNC：如果文件已存在，并且是常规的文件，那么就先清空其中已经存在的任何内容。 对于以上操作模式的使用，os.Create函数和os.Open函数都是现成的例子。&nbsp; 123func Create(name string) (*File, error) &#123; return OpenFile(name, O_RDWR|O_CREATE|O_TRUNC, 0666)&#125; &nbsp; os.Create函数在调用os.OpenFile函数的时候，给予的操作模式是os.O_RDWR、os.O_CREATE和os.O_TRUNC的组合。 &nbsp;这就基本上决定了前者的行为，即：如果参数name代表路径之上的文件不存在，那么就新建一个，否则，先清空现存文件中的全部内容。&nbsp;并且，它返回的File值的读取方法和写入方法都是可用的。这里需要注意，多个操作模式是通过按位或操作符|组合起来的。&nbsp; 123func Open(name string) (*File, error) &#123; return OpenFile(name, O_RDONLY, 0)&#125; &nbsp; 我在前面说过，os.Open函数的功能是：以只读模式打开已经存在的文件。其根源就是它在调用os.OpenFile函数的时候，只提供了一个单一的操作模式os.O_RDONLY。 &nbsp;以上，就是我对可应用于File值的操作模式的简单解释。&nbsp;怎样设定常规文件的访问权限？&nbsp;我们已经知道，os.OpenFile函数的第三个参数perm代表的是权限模式，其类型是os.FileMode。但实际上，os.FileMode类型能够代表的，可远不只权限模式，它还可以代表文件模式（也可以称之为文件种类）。&nbsp;由于os.FileMode是基于uint32类型的再定义类型，所以它的每个值都包含了 32 个比特位。在这 32 个比特位当中，每个比特位都有其特定的含义。&nbsp;比如，如果在其最高比特位上的二进制数是1，那么该值表示的文件模式就等同于os.ModeDir，也就是说，相应的文件代表的是一个目录。&nbsp;又比如，如果其中的第 26 个比特位上的是1，那么相应的值表示的文件模式就等同于os.ModeNamedPipe，也就是说，那个文件代表的是一个命名管道。&nbsp;实际上，在一个os.FileMode类型的值（以下简称FileMode值）中，只有最低的 9 个比特位才用于表示文件的权限。当我们拿到一个此类型的值时，可以把它和os.ModePerm常量的值做按位与操作。&nbsp;这个常量的值是0777，是一个八进制的无符号整数，其最低的 9 个比特位上都是1，而更高的 23 个比特位上都是0。&nbsp;所以，经过这样的按位与操作之后，我们即可得到这个FileMode值中所有用于表示文件权限的比特位，也就是该值所表示的权限模式。这将会与我们调用FileMode值的Perm方法所得到的结果值是一致。&nbsp;在这 9 个用于表示文件权限的比特位中，每 3 个比特位为一组，共可分为 3 组。&nbsp;从高到低，这 3 组分别表示的是文件所有者（也就是创建这个文件的那个用户）、文件所有者所属的用户组，以及其他用户对该文件的访问权限。而对于每个组，其中的 3 个比特位从高到低分别表示读权限、写权限和执行权限。&nbsp;如果在其中的某个比特位上的是1，那么就意味着相应的权限开启，否则，就表示相应的权限关闭。&nbsp;因此，八进制整数0777就表示：操作系统中的所有用户都对当前的文件有读、写和执行的权限，而八进制整数0666则表示：所有用户都对当前文件有读和写的权限，但都没有执行的权限。&nbsp;我们在调用os.OpenFile函数的时候，可以根据以上说明设置它的第三个参数。但要注意，只有在新建文件的时候，这里的第三个参数值才是有效的。在其他情况下，即使我们设置了此参数，也不会对目标文件产生任何的影响。&nbsp;扩展：怎样通过os包中的 API 创建和操纵一个系统进程？&nbsp;可以从os包的FindProcess函数和StartProcess函数开始。前者用于通过进程 ID（pid）查找进程，后者用来基于某个程序启动一个进程。&nbsp;这两者都会返回一个*os.Process类型的值。该类型提供了一些方法，比如，用于杀掉当前进程的Kill方法，又比如，可以给当前进程发送系统信号的Signal方法，以及会等待当前进程结束的Wait方法。&nbsp;与此相关的还有os.ProcAttr类型、os.ProcessState类型、os.Signal类型，等等。&nbsp; &nbsp;原文链接：https://time.geekbang.org/column/intro/112","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Go语言","slug":"Go语言","permalink":"http://wht6.github.io/tags/Go%E8%AF%AD%E8%A8%80/"}]},{"title":"字符编码、字符串和字节串","slug":"unicode与字符编码","date":"2021-07-15T09:00:00.000Z","updated":"2022-04-10T10:22:14.451Z","comments":true,"path":"posts/2b2.html","link":"","permalink":"http://wht6.github.io/posts/2b2.html","excerpt":"","text":"字符编码Go语言字符编码&nbsp; 首先，让我们来关注字符编码方面的问题。这应该是在计算机软件领域中非常基础的一个问题了。&nbsp;我在前面说过，Go 语言中的标识符可以包含“任何 Unicode 编码可以表示的字母字符”。我还说过，虽然我们可以直接把一个整数值转换为一个string类型的值。&nbsp;但是，被转换的整数值应该可以代表一个有效的 Unicode 代码点，否则转换的结果就将会是&quot;�&quot;，即：一个仅由高亮的问号组成的字符串值。&nbsp;另外，当一个string类型的值被转换为[]rune类型值的时候，其中的字符串会被拆分成一个一个的 Unicode 字符。&nbsp;显然，Go 语言采用的字符编码方案从属于 Unicode 编码规范。更确切地说，Go 语言的代码正是由 Unicode 字符组成的。Go 语言的所有源代码，都必须按照 Unicode 编码规范中的 UTF-8 编码格式进行编码。&nbsp;换句话说，Go 语言的源码文件必须使用 UTF-8 编码格式进行存储。如果源码文件中出现了非 UTF-8 编码的字符，那么在构建、安装以及运行的时候，go 命令就会报告错误“illegal UTF-8 encoding”。&nbsp;在这里，我们首先要对 Unicode 编码规范有所了解。不过，在讲述它之前，我先来简要地介绍一下 ASCII 编码。&nbsp; ASCII编码&nbsp;ASCII 是英文“American Standard Code for Information Interchange”的缩写，中文译为美国信息交换标准代码。它是由美国国家标准学会（ANSI）制定的单字节字符编码方案，可用于基于文本的数据交换。&nbsp;它最初是美国的国家标准，后又被国际标准化组织（ISO）定为国际标准，称为 ISO 646 标准，并适用于所有的拉丁文字字母。&nbsp;ASCII 编码方案使用单个字节（byte）的二进制数来编码一个字符。标准的 ASCII 编码用一个字节的最高比特（bit）位作为奇偶校验位，而扩展的 ASCII 编码则将此位也用于表示字符。ASCII 编码支持的可打印字符和控制字符的集合也被叫做 ASCII 编码集。&nbsp;我们所说的 Unicode 编码规范，实际上是另一个更加通用的、针对书面字符和文本的字符编码标准。它为世界上现存的所有自然语言中的每一个字符，都设定了一个唯一的二进制编码。&nbsp;它定义了不同自然语言的文本数据在国际间交换的统一方式，并为全球化软件创建了一个重要的基础。&nbsp;Unicode 编码规范以 ASCII 编码集为出发点，并突破了 ASCII 只能对拉丁字母进行编码的限制。它不但提供了可以对世界上超过百万的字符进行编码的能力，还支持所有已知的转义序列和控制代码。&nbsp;我们都知道，在计算机系统的内部，抽象的字符会被编码为整数。这些整数的范围被称为代码空间。在代码空间之内，每一个特定的整数都被称为一个代码点。&nbsp;一个受支持的抽象字符会被映射并分配给某个特定的代码点，反过来讲，一个代码点总是可以被看成一个被编码的字符。&nbsp;Unicode 编码规范通常使用十六进制表示法来表示 Unicode 代码点的整数值，并使用“U+”作为前缀。比如，英文字母字符“a”的 Unicode 代码点是 U+0061。在 Unicode 编码规范中，一个字符能且只能由与它对应的那个代码点表示。&nbsp;Unicode 编码规范提供了三种不同的编码格式，即：UTF-8、UTF-16 和 UTF-32。其中的 UTF 是 UCS Transformation Format 的缩写。而 UCS 又是 Universal Character Set 的缩写，但也可以代表 Unicode Character Set。所以，UTF 也可以被翻译为 Unicode 转换格式。它代表的是字符与字节序列之间的转换方式。&nbsp;在这几种编码格式的名称中，“-”右边的整数的含义是，以多少个比特位作为一个编码单元。以 UTF-8 为例，它会以 8 个比特，也就是一个字节，作为一个编码单元。并且，它与标准的 ASCII 编码是完全兼容的。也就是说，在 [0x00, 0x7F] 的范围内，这两种编码表示的字符都是相同的。这也是 UTF-8 编码格式的一个巨大优势。&nbsp;UTF-8 是一种可变宽的编码方案。换句话说，它会用一个或多个字节的二进制数来表示某个字符，最多使用四个字节。比如，对于一个英文字符，它仅用一个字节的二进制数就可以表示，而对于一个中文字符，它需要使用三个字节才能够表示。不论怎样，一个受支持的字符总是可以由 UTF-8 编码为一个字节序列。以下会简称后者为 UTF-8 编码值。&nbsp;一个string类型的值在底层是怎样被表达的？&nbsp;在底层，一个string类型的值是由一系列相对应的 Unicode 代码点的 UTF-8 编码值来表达的。&nbsp;在 Go 语言中，一个string类型的值既可以被拆分为一个包含多个字符的序列，也可以被拆分为一个包含多个字节的序列。&nbsp;前者可以由一个以rune为元素类型的切片来表示，而后者则可以由一个以byte为元素类型的切片代表。&nbsp;rune是 Go 语言特有的一个基本数据类型，它的一个值就代表一个字符，即：一个 Unicode 字符。&nbsp;比如，&#39;G&#39;、&#39;o&#39;、&#39;爱&#39;、&#39;好&#39;、&#39;者&#39;代表的就都是一个 Unicode 字符。&nbsp;我们已经知道，UTF-8 编码方案会把一个 Unicode 字符编码为一个长度在 [1, 4] 范围内的字节序列。所以，一个rune类型的值也可以由一个或多个字节来代表。&nbsp;1type rune = int32&nbsp; 根据rune类型的声明可知，它实际上就是int32类型的一个别名类型。也就是说，一个rune类型的值会由四个字节宽度的空间来存储。它的存储空间总是能够存下一个 UTF-8 编码值。 &nbsp;一个rune类型的值在底层其实就是一个 UTF-8 编码值。前者是（便于我们人类理解的）外部展现，后者是（便于计算机系统理解的）内在表达。&nbsp;12345str := &quot;Go 爱好者 &quot;fmt.Printf(&quot;The string: %q\\n&quot;, str)fmt.Printf(&quot; =&gt; runes(char): %q\\n&quot;, []rune(str))fmt.Printf(&quot; =&gt; runes(hex): %x\\n&quot;, []rune(str))fmt.Printf(&quot; =&gt; bytes(hex): [% x]\\n&quot;, []byte(str))&nbsp; 字符串值&quot;Go爱好者&quot;如果被转换为[]rune类型的值的话，其中的每一个字符（不论是英文字符还是中文字符）就都会独立成为一个rune类型的元素值。因此，这段代码打印出的第二行内容就会如下所示： &nbsp;1&#x3D;&gt; runes(char): [&#39;G&#39; &#39;o&#39; &#39;爱&#39; &#39;好&#39; &#39;者&#39;]&nbsp; 又由于，每个rune类型的值在底层都是由一个 UTF-8 编码值来表达的，所以我们可以换一种方式来展现这个字符序列： &nbsp;1&#x3D;&gt; runes(hex): [47 6f 7231 597d 8005]&nbsp; 可以看到，五个十六进制数与五个字符相对应。很明显，前两个十六进制数47和6f代表的整数都比较小，它们分别表示字符&#39;G&#39;和&#39;o&#39;。 &nbsp;因为它们都是英文字符，所以对应的 UTF-8 编码值用一个字节表达就足够了。一个字节的编码值被转换为整数之后，不会大到哪里去。&nbsp;而后三个十六进制数7231、597d和8005都相对较大，它们分别表示中文字符&#39;爱&#39;、&#39;好&#39;和&#39;者&#39;。&nbsp;这些中文字符对应的 UTF-8 编码值，都需要使用三个字节来表达。所以，这三个数就是把对应的三个字节的编码值，转换为整数后得到的结果。&nbsp;我们还可以进一步地拆分，把每个字符的 UTF-8 编码值都拆成相应的字节序列。上述代码中的第五行就是这么做的。它会得到如下的输出：&nbsp;1&#x3D;&gt; bytes(hex): [47 6f e7 88 b1 e5 a5 bd e8 80 85]&nbsp; 这里得到的字节切片比前面的字符切片明显长了很多。这正是因为一个中文字符的 UTF-8 编码值需要用三个字节来表达。 &nbsp;这个字节切片的前两个元素值与字符切片的前两个元素值是一致的，而在这之后，前者的每三个元素值才对应字符切片中的一个元素值。&nbsp;注意，对于一个多字节的 UTF-8 编码值来说，我们可以把它当做一个整体转换为单一的整数，也可以先把它拆成字节序列，再把每个字节分别转换为一个整数，从而得到多个整数。&nbsp;这两种表示法展现出来的内容往往会很不一样。比如，对于中文字符&#39;爱&#39;来说，它的 UTF-8 编码值可以展现为单一的整数7231，也可以展现为三个整数，即：e7、88和b1。&nbsp;&nbsp;总之，一个string类型的值会由若干个 Unicode 字符组成，每个 Unicode 字符都可以由一个rune类型的值来承载。&nbsp;这些字符在底层都会被转换为 UTF-8 编码值，而这些 UTF-8 编码值又会以字节序列的形式表达和存储。因此，一个string类型的值在底层就是一个能够表达若干个 UTF-8 编码值的字节序列。&nbsp;使用带有range子句的for语句遍历字符串值的时候应该注意什么？&nbsp;带有range子句的for语句会先把被遍历的字符串值拆成一个字节序列，然后再试图找出这个字节序列中包含的每一个 UTF-8 编码值，或者说每一个 Unicode 字符。&nbsp;这样的for语句可以为两个迭代变量赋值。如果存在两个迭代变量，那么赋给第一个变量的值，就将会是当前字节序列中的某个 UTF-8 编码值的第一个字节所对应的那个索引值。&nbsp;而赋给第二个变量的值，则是这个 UTF-8 编码值代表的那个 Unicode 字符，其类型会是rune。&nbsp;1234str := &quot;Go 爱好者 &quot;for i, c := range str &#123; fmt.Printf(&quot;%d: %q [% x]\\n&quot;, i, c, []byte(string(c)))&#125;&nbsp; 这里被遍历的字符串值是&quot;Go爱好者&quot;。在每次迭代的时候，这段代码都会打印出两个迭代变量的值，以及第二个值的字节序列形式。完整的打印内容如下： &nbsp;123450: &#39;G&#39; [47]1: &#39;o&#39; [6f]2: &#39;爱&#39; [e7 88 b1]5: &#39;好&#39; [e5 a5 bd]8: &#39;者&#39; [e8 80 85]&nbsp; 第一行内容中的关键信息有0、&#39;G&#39;和[47]。这是由于这个字符串值中的第一个 Unicode 字符是&#39;G&#39;。该字符是一个单字节字符，并且由相应的字节序列中的第一个字节表达。这个字节的十六进制表示为47。 &nbsp;第二行展示的内容与之类似，即：第二个 Unicode 字符是&#39;o&#39;，由字节序列中的第二个字节表达，其十六进制表示为6f。&nbsp;再往下看，第三行展示的是&#39;爱&#39;，也是第三个 Unicode 字符。因为它是一个中文字符，所以由字节序列中的第三、四、五个字节共同表达，其十六进制表示也不再是单一的整数，而是e7、88和b1组成的序列。&nbsp;下面要注意了，正是因为&#39;爱&#39;是由三个字节共同表达的，所以第四个 Unicode 字符&#39;好&#39;对应的索引值并不是3，而是2加3后得到的5。&nbsp;这里的2代表的是&#39;爱&#39;对应的索引值，而3代表的则是&#39;爱&#39;对应的 UTF-8 编码值的宽度。对于这个字符串值中的最后一个字符&#39;者&#39;来说也是类似的，因此，它对应的索引值是8。&nbsp;由此可以看出，这样的for语句可以逐一地迭代出字符串值里的每个 Unicode 字符。但是，相邻的 Unicode 字符的索引值并不一定是连续的。这取决于前一个 Unicode 字符是否为单字节字符。&nbsp;正因为如此，如果我们想得到其中某个 Unicode 字符对应的 UTF-8 编码值的宽度，就可以用下一个字符的索引值减去当前字符的索引值。 字符串标准库中的strings代码包用到了不少unicode包和unicode/utf8包中的程序实体。比如，strings.Builder类型的WriteRune方法、strings.Reader类型的ReadRune方法，等等。 &nbsp; 与string值相比，strings.Builder类型的值有哪些优势？&nbsp;strings.Builder类型的值（以下简称Builder值）的优势有下面的三种：&nbsp; 已存在的内容不可变，但可以拼接更多的内容； 减少了内存分配和内容拷贝的次数； 可将内容重置，可重用值。 &nbsp;先来说说string类型。 我们都知道，在 Go 语言中，string类型的值是不可变的。 如果我们想获得一个不一样的字符串，那么就只能基于原字符串进行裁剪、拼接等操作，从而生成一个新的字符串。&nbsp; 裁剪操作可以使用切片表达式； 拼接操作可以用操作符+实现。 &nbsp;在底层，一个string值的内容会被存储到一块连续的内存空间中。同时，这块内存容纳的字节数量也会被记录下来，并用于表示该string值的长度。&nbsp;你可以把这块内存的内容看成一个字节数组，而相应的string值则包含了指向字节数组头部的指针值。如此一来，我们在一个string值上应用切片表达式，就相当于在对其底层的字节数组做切片。&nbsp;另外，我们在进行字符串拼接的时候，Go 语言会把所有被拼接的字符串依次拷贝到一个崭新且足够大的连续内存空间中，并把持有相应指针值的string值作为结果返回。&nbsp;显然，当程序中存在过多的字符串拼接操作的时候，会对内存的分配产生非常大的压力。&nbsp;注意，虽然string值在内部持有一个指针值，但其类型仍然属于值类型。不过，由于string值的不可变，其中的指针值也为内存空间的节省做出了贡献。&nbsp;更具体地说，一个string值会在底层与它的所有副本共用同一个字节数组。由于这里的字节数组永远不会被改变，所以这样做是绝对安全的。&nbsp;与string值相比，Builder值的优势其实主要体现在字符串拼接方面。&nbsp;Builder值中有一个用于承载内容的容器（以下简称内容容器）。它是一个以byte为元素类型的切片（以下简称字节切片）。&nbsp;由于这样的字节切片的底层数组就是一个字节数组，所以我们可以说它与string值存储内容的方式是一样的。&nbsp;实际上，它们都是通过一个unsafe.Pointer类型的字段来持有那个指向了底层字节数组的指针值的。&nbsp;正是因为这样的内部构造，Builder值同样拥有高效利用内存的前提条件。虽然，对于字节切片本身来说，它包含的任何元素值都可以被修改，但是Builder值并不允许这样做，其中的内容只能够被拼接或者完全重置。&nbsp;这就意味着，已存在于Builder值中的内容是不可变的。因此，我们可以利用Builder值提供的方法拼接更多的内容，而丝毫不用担心这些方法会影响到已存在的内容。&nbsp; 这里所说的方法指的是，Builder值拥有的一系列指针方法，包括：Write、WriteByte、WriteRune和WriteString。我们可以把它们统称为拼接方法。 &nbsp;我们可以通过调用上述方法把新的内容拼接到已存在的内容的尾部（也就是右边）。这时，如有必要，Builder值会自动地对自身的内容容器进行扩容。这里的自动扩容策略与切片的扩容策略一致。&nbsp;换句话说，我们在向Builder值拼接内容的时候并不一定会引起扩容。只要内容容器的容量够用，扩容就不会进行，针对于此的内存分配也不会发生。同时，只要没有扩容，Builder值中已存在的内容就不会再被拷贝。&nbsp;除了Builder值的自动扩容，我们还可以选择手动扩容，这通过调用Builder值的Grow方法就可以做到。Grow方法也可以被称为扩容方法，它接受一个int类型的参数n，该参数用于代表将要扩充的字节数量。&nbsp;如有必要，Grow方法会把其所属值中内容容器的容量增加n个字节。更具体地讲，它会生成一个字节切片作为新的内容容器，该切片的容量会是原容器容量的二倍再加上n。之后，它会把原容器中的所有字节全部拷贝到新容器中。&nbsp; 12345var builder1 strings.Builder// 省略若干代码。fmt.Println(&quot;Grow the builder ...&quot;)builder1.Grow(10)fmt.Printf(&quot;The length of contents in the builder is %d.\\n&quot;, builder1.Len()) &nbsp; 当然，Grow方法还可能什么都不做。这种情况的前提条件是：当前的内容容器中的未用容量已经够用了，即：未用容量大于或等于n。这里的前提条件与前面提到的自动扩容策略中的前提条件是类似的。 &nbsp; 123fmt.Println(&quot;Reset the builder ...&quot;)builder1.Reset()fmt.Printf(&quot;The third output(%d):\\n%q\\n&quot;, builder1.Len(), builder1.String()) &nbsp; 最后，Builder值是可以被重用的。通过调用它的Reset方法，我们可以让Builder值重新回到零值状态，就像它从未被使用过那样。 &nbsp;一旦被重用，Builder值中原有的内容容器会被直接丢弃。之后，它和其中的所有内容，将会被 Go 语言的垃圾回收器标记并回收掉。&nbsp;strings.Builder类型在使用上有约束吗？有约束，概括如下：&nbsp; 在已被真正使用后就不可再被复制； 由于其内容不是完全不可变的，所以需要使用方自行解决操作冲突和并发安全问题。 &nbsp;我们只要调用了Builder值的拼接方法或扩容方法，就意味着开始真正使用它了。显而易见，这些方法都会改变其所属值中的内容容器的状态。&nbsp;一旦调用了它们，我们就不能再以任何的方式对其所属值进行复制了。否则，只要在任何副本上调用上述方法就都会引发 panic。&nbsp;这种 panic 会告诉我们，这样的使用方式是并不合法的，因为这里的Builder值是副本而不是原值。顺便说一句，这里所说的复制方式，包括但不限于在函数间传递值、通过通道传递值、把值赋予变量等等。&nbsp; 12345var builder1 strings.Builderbuilder1.Grow(1)builder3 := builder1//builder3.Grow(1) // 这里会引发 panic。_ = builder3 &nbsp; 虽然这个约束非常严格，但是如果我们仔细思考一下的话，就会发现它还是有好处的。 &nbsp;正是由于已使用的Builder值不能再被复制，所以肯定不会出现多个Builder值中的内容容器（也就是那个字节切片）共用一个底层字节数组的情况。这样也就避免了多个同源的Builder值在拼接内容时可能产生的冲突问题。&nbsp;不过，虽然已使用的Builder值不能再被复制，但是它的指针值却可以。无论什么时候，我们都可以通过任何方式复制这样的指针值。注意，这样的指针值指向的都会是同一个Builder值。&nbsp; 1234567f2 := func(bp *strings.Builder) &#123; (*bp).Grow(1) // 这里虽然不会引发 panic，但不是并发安全的。 builder4 := *bp //builder4.Grow(1) // 这里会引发 panic。 _ = builder4&#125;f2(&amp;builder1) &nbsp; 正因为如此，这里就产生了一个问题，即：如果Builder值被多方同时操作，那么其中的内容就很可能会产生混乱。这就是我们所说的操作冲突和并发安全问题。 &nbsp;Builder值自己是无法解决这些问题的。所以，我们在通过传递其指针值共享Builder值的时候，一定要确保各方对它的使用是正确、有序的，并且是并发安全的；而最彻底的解决方案是，绝不共享Builder值以及它的指针值。&nbsp;我们可以在各处分别声明一个Builder值来使用，也可以先声明一个Builder值，然后在真正使用它之前，便将它的副本传到各处。另外，我们还可以先使用再传递，只要在传递之前调用它的Reset方法即可。&nbsp; 123builder1.Reset()builder5 := builder1builder5.Grow(1) // 这里不会引发 panic。 &nbsp; 总之，关于复制Builder值的约束是有意义的，也是很有必要的。虽然我们仍然可以通过某些方式共享Builder值，但最好还是不要以身犯险，“各自为政”是最好的解决方案。不过，对于处在零值状态的Builder值，复制不会有任何问题。 &nbsp;为什么说strings.Reader类型的值可以高效地读取字符串？&nbsp;与strings.Builder类型恰恰相反，strings.Reader类型是为了高效读取字符串而存在的。后者的高效主要体现在它对字符串的读取机制上，它封装了很多用于在string值上读取内容的最佳实践。&nbsp;strings.Reader类型的值（以下简称Reader值）可以让我们很方便地读取一个字符串中的内容。在读取的过程中，Reader值会保存已读取的字节的计数（以下简称已读计数）。&nbsp;已读计数也代表着下一次读取的起始索引位置。Reader值正是依靠这样一个计数，以及针对字符串值的切片表达式，从而实现快速读取。&nbsp;此外，这个已读计数也是读取回退和位置设定时的重要依据。虽然它属于Reader值的内部结构，但我们还是可以通过该值的Len方法和Size把它计算出来的。代码如下：&nbsp; 123var reader1 strings.Reader// 省略若干代码。readingIndex := reader1.Size() - int64(reader1.Len()) // 计算出的已读计数。 &nbsp; Reader值拥有的大部分用于读取的方法都会及时地更新已读计数。比如，ReadByte方法会在读取成功后将这个计数的值加1。 &nbsp;又比如，ReadRune方法在读取成功之后，会把被读取的字符所占用的字节数作为计数的增量。&nbsp;不过，ReadAt方法算是一个例外。它既不会依据已读计数进行读取，也不会在读取后更新它。正因为如此，这个方法可以自由地读取其所属的Reader值中的任何内容。&nbsp;除此之外，Reader值的Seek方法也会更新该值的已读计数。实际上，这个Seek方法的主要作用正是设定下一次读取的起始索引位置。&nbsp;另外，如果我们把常量io.SeekCurrent的值作为第二个参数值传给该方法，那么它还会依据当前的已读计数，以及第一个参数offset的值来计算新的计数值。&nbsp;由于Seek方法会返回新的计数值，所以我们可以很容易地验证这一点。比如像下面这样：&nbsp; 123456offset2 := int64(17)expectedIndex := reader1.Size() - int64(reader1.Len()) + offset2fmt.Printf(&quot;Seek with offset %d and whence %d ...\\n&quot;, offset2, io.SeekCurrent)readingIndex, _ := reader1.Seek(offset2, io.SeekCurrent)fmt.Printf(&quot;The reading index in reader: %d (returned by Seek)\\n&quot;, readingIndex)fmt.Printf(&quot;The reading index in reader: %d (computed by me)\\n&quot;, expectedIndex) &nbsp; 综上所述，Reader值实现高效读取的关键就在于它内部的已读计数。计数的值就代表着下一次读取的起始索引位置。它可以很容易地被计算出来。Reader值的Seek方法可以直接设定该值中的已读计数值。 字节串bytes.Buffer基础知识&nbsp; strings包和bytes包可以说是一对孪生兄弟，它们在 API 方面非常的相似。单从它们提供的函数的数量和功能上讲，差别可以说是微乎其微。&nbsp;只不过，strings包主要面向的是 Unicode 字符和经过 UTF-8 编码的字符串，而bytes包面对的则主要是字节和字节切片。&nbsp;bytes.Buffer类型的用途主要是作为字节序列的缓冲区。与strings.Builder类型一样，bytes.Buffer也是开箱即用的。&nbsp;但不同的是，strings.Builder只能拼接和导出字符串，而bytes.Buffer不但可以拼接、截断其中的字节序列，以各种形式导出其中的内容，还可以顺序地读取其中的子序列。&nbsp;可以说，bytes.Buffer是集读、写功能于一身的数据类型。当然了，这些也基本上都是作为一个缓冲区应该拥有的功能。&nbsp;在内部，bytes.Buffer类型同样是使用字节切片作为内容容器的。并且，与strings.Reader类型类似，bytes.Buffer有一个int类型的字段，用于代表已读字节的计数，可以简称为已读计数。&nbsp;不过，这里的已读计数就无法通过bytes.Buffer提供的方法计算出来了。&nbsp; 123456var buffer1 bytes.Buffercontents := &quot;Simple byte buffer for marshaling data.&quot;fmt.Printf(&quot;Writing contents %q ...\\n&quot;, contents)buffer1.WriteString(contents)fmt.Printf(&quot;The length of buffer: %d\\n&quot;, buffer1.Len())fmt.Printf(&quot;The capacity of buffer: %d\\n&quot;, buffer1.Cap()) &nbsp; 我先声明了一个bytes.Buffer类型的变量buffer1，并写入了一个字符串。然后，我想打印出这个bytes.Buffer类型的值（以下简称Buffer值）的长度和容量。在运行这段代码之后，我们将会看到如下的输出： &nbsp; 123Writing contents &quot;Simple byte buffer for marshaling data.&quot; ...The length of buffer: 39The capacity of buffer: 64 &nbsp; 乍一看这没什么问题。长度39和容量64的含义看起来与我们已知的概念是一致的。我向缓冲区中写入了一个长度为39的字符串，所以buffer1的长度就是39。 &nbsp;根据切片的自动扩容策略，64这个数字也是合理的。另外，可以想象，这时的已读计数的值应该是0，这是因为我还没有调用任何用于读取其中内容的方法。&nbsp;可实际上，与strings.Reader类型的Len方法一样，buffer1的Len方法返回的也是内容容器中未被读取部分的长度，而不是其中已存内容的总长度（以下简称内容长度）。示例如下：&nbsp; 12345p1 := make([]byte, 7)n, _ := buffer1.Read(p1)fmt.Printf(&quot;%d bytes were read. (call Read)\\n&quot;, n)fmt.Printf(&quot;The length of buffer: %d\\n&quot;, buffer1.Len())fmt.Printf(&quot;The capacity of buffer: %d\\n&quot;, buffer1.Cap()) &nbsp; 当我从buffer1中读取一部分内容，并用它们填满长度为7的字节切片p1之后，buffer1的Len方法返回的结果值也会随即发生变化。如果运行这段代码，我们会发现，这个缓冲区的长度已经变为了32。 &nbsp;另外，因为我们并没有再向该缓冲区中写入任何内容，所以它的容量会保持不变，仍是64。&nbsp;总之，在这里，你需要记住的是，Buffer值的长度是未读内容的长度，而不是已存内容的总长度。 它与在当前值之上的读操作和写操作都有关系，并会随着这两种操作的进行而改变，它可能会变得更小，也可能会变得更大。&nbsp;而Buffer值的容量指的是它的内容容器（也就是那个字节切片）的容量，它只与在当前值之上的写操作有关，并会随着内容的写入而不断增长。&nbsp;再说已读计数。由于strings.Reader还有一个Size方法可以给出内容长度的值，所以我们用内容长度减去未读部分的长度，就可以很方便地得到它的已读计数。&nbsp;然而，bytes.Buffer类型却没有这样一个方法，它只有Cap方法。可是Cap方法提供的是内容容器的容量，也不是内容长度。&nbsp;并且，这里的内容容器容量在很多时候都与内容长度不相同。因此，没有了现成的计算公式，只要遇到稍微复杂些的情况，我们就很难估算出Buffer值的已读计数。&nbsp;一旦理解了已读计数这个概念，并且能够在读写的过程中，实时地获得已读计数和内容长度的值，我们就可以很直观地了解到当前Buffer值各种方法的行为了。不过，很可惜，这两个数字我们都无法直接拿到。&nbsp;虽然，我们无法直接得到一个Buffer值的已读计数，并且有时候也很难估算它，但是我们绝对不能就此作罢，而应该通过研读bytes.Buffer和文档和源码，去探究已读计数在其中起到的关键作用。&nbsp;否则，我们想用好bytes.Buffer的意愿，恐怕就不会那么容易实现了。&nbsp;下面的这个问题，如果你认真地阅读了bytes.Buffer的源码之后，就可以很好地回答出来。&nbsp;bytes.Buffer类型的值记录的已读计数，在其中起到了怎样的作用？&nbsp;bytes.Buffer中的已读计数的大致功用如下所示。&nbsp; 读取内容时，相应方法会依据已读计数找到未读部分，并在读取后更新计数。 写入内容时，如需扩容，相应方法会根据已读计数实现扩容策略。 截断内容时，相应方法截掉的是已读计数代表索引之后的未读部分。 读回退时，相应方法需要用已读计数记录回退点。 重置内容时，相应方法会把已读计数置为0。 导出内容时，相应方法只会导出已读计数代表的索引之后的未读部分。 获取长度时，相应方法会依据已读计数和内容容器的长度，计算未读部分的长度并返回。 &nbsp;通过上面的典型回答，我们已经能够体会到已读计数在bytes.Buffer类型，及其方法中的重要性了。没错，bytes.Buffer的绝大多数方法都用到了已读计数，而且都是非用不可。&nbsp;在读取内容的时候，相应方法会先根据已读计数，判断一下内容容器中是否还有未读的内容。如果有，那么它就会从已读计数代表的索引处开始读取。&nbsp;在读取完成后，它还会及时地更新已读计数。也就是说，它会记录一下又有多少个字节被读取了。这里所说的相应方法包括了所有名称以Read开头的方法，以及Next方法和WriteTo方法。&nbsp;在写入内容的时候，绝大多数的相应方法都会先检查当前的内容容器，是否有足够的容量容纳新的内容。如果没有，那么它们就会对内容容器进行扩容。&nbsp;在扩容的时候，方法会在必要时，依据已读计数找到未读部分，并把其中的内容拷贝到扩容后内容容器的头部位置。&nbsp;然后，方法将会把已读计数的值置为0，以表示下一次读取需要从内容容器的第一个字节开始。用于写入内容的相应方法，包括了所有名称以Write开头的方法，以及ReadFrom方法。&nbsp;用于截断内容的方法Truncate，会让很多对bytes.Buffer不太了解的程序开发者迷惑。 它会接受一个int类型的参数，这个参数的值代表了：在截断时需要保留头部的多少个字节。&nbsp;不过，需要注意的是，这里说的头部指的并不是内容容器的头部，而是其中的未读部分的头部。头部的起始索引正是由已读计数的值表示的。因此，在这种情况下，已读计数的值再加上参数值后得到的和，就是内容容器新的总长度。&nbsp;在bytes.Buffer中，用于读回退的方法有UnreadByte和UnreadRune。 这两个方法分别用于回退一个字节和回退一个 Unicode 字符。调用它们一般都是为了退回在上一次被读取内容末尾的那个分隔符，或者为重新读取前一个字节或字符做准备。&nbsp;不过，退回的前提是，在调用它们之前的那一个操作必须是“读取”，并且是成功的读取，否则这些方法就只能忽略后续操作并返回一个非nil的错误值。&nbsp;UnreadByte方法的做法比较简单，把已读计数的值减1就好了。而UnreadRune方法需要从已读计数中减去的，是上一次被读取的 Unicode 字符所占用的字节数。&nbsp;这个字节数由bytes.Buffer的另一个字段负责存储，它在这里的有效取值范围是 [1, 4]。只有ReadRune方法才会把这个字段的值设定在此范围之内。&nbsp;由此可见，只有紧接在调用ReadRune方法之后，对UnreadRune方法的调用才能够成功完成。该方法明显比UnreadByte方法的适用面更窄。&nbsp;我在前面说过，bytes.Buffer的Len方法返回的是内容容器中未读部分的长度，而不是其中已存内容的总长度（即：内容长度）。&nbsp;而该类型的Bytes方法和String方法的行为，与Len方法是保持一致的。前两个方法只会去访问未读部分中的内容，并返回相应的结果值。&nbsp;在我们剖析了所有的相关方法之后，可以这样来总结：在已读计数代表的索引之前的那些内容，永远都是已经被读过的，它们几乎没有机会再次被读取。&nbsp;不过，这些已读内容所在的内存空间可能会被存入新的内容。这一般都是由于重置或者扩充内容容器导致的。这时，已读计数一定会被置为0，从而再次指向内容容器中的第一个字节。这有时候也是为了避免内存分配和重用内存空间。&nbsp;bytes.Buffer的扩容策略是怎样的？&nbsp;Buffer值既可以被手动扩容，也可以进行自动扩容。并且，这两种扩容方式的策略是基本一致的。所以，除非我们完全确定后续内容所需的字节数，否则让Buffer值自动去扩容就好了。&nbsp;在扩容的时候，Buffer值中相应的代码（以下简称扩容代码）会先判断内容容器的剩余容量，是否可以满足调用方的要求，或者是否足够容纳新的内容。&nbsp;如果可以，那么扩容代码会在当前的内容容器之上，进行长度扩充。&nbsp;更具体地说，如果内容容器的容量与其长度的差，大于或等于另需的字节数，那么扩容代码就会通过切片操作对原有的内容容器的长度进行扩充，就像下面这样：&nbsp; 1b.buf = b.buf[:length+need] &nbsp; 反之，如果内容容器的剩余容量不够了，那么扩容代码可能就会用新的内容容器去替代原有的内容容器，从而实现扩容。 &nbsp;不过，这里还有一步优化。&nbsp;如果当前内容容器的容量的一半，仍然大于或等于其现有长度再加上另需的字节数的和，即：&nbsp; 1cap(b.buf)/2 &gt;= len(b.buf)+need &nbsp; 那么，扩容代码就会复用现有的内容容器，并把容器中的未读内容拷贝到它的头部位置。 &nbsp;这也意味着其中的已读内容，将会全部被未读内容和之后的新内容覆盖掉。&nbsp;这样的复用预计可以至少节省掉一次后续的扩容所带来的内存分配，以及若干字节的拷贝。&nbsp;若这一步优化未能达成，也就是说，当前内容容器的容量小于新长度的二倍。&nbsp;那么，扩容代码就只能再创建一个新的内容容器，并把原有容器中的未读内容拷贝进去，最后再用新的容器替换掉原有的容器。这个新容器的容量将会等于原有容量的二倍再加上另需字节数的和。&nbsp; 新容器的容量 =2* 原有容量 + 所需字节数 &nbsp;通过上面这些步骤，对内容容器的扩充基本上就完成了。不过，为了内部数据的一致性，以及避免原有的已读内容可能造成的数据混乱，扩容代码还会把已读计数置为0，并再对内容容器做一下切片操作，以掩盖掉原有的已读内容。&nbsp;顺便说一下，对于处在零值状态的Buffer值来说，如果第一次扩容时的另需字节数不大于64，那么该值就会基于一个预先定义好的、长度为64的字节数组来创建内容容器。&nbsp;在这种情况下，这个内容容器的容量就是64。这样做的目的是为了让Buffer值在刚被真正使用的时候就可以快速地做好准备。&nbsp;bytes.Buffer中的哪些方法可能会造成内容的泄露？&nbsp;首先明确一点，什么叫内容泄露？这里所说的内容泄露是指，使用Buffer值的一方通过某种非标准的（或者说不正式的）方式，得到了本不该得到的内容。&nbsp;比如说，我通过调用Buffer值的某个用于读取内容的方法，得到了一部分未读内容。我应该，也只应该通过这个方法的结果值，拿到在那一时刻Buffer值中的未读内容。&nbsp;但是，在这个Buffer值又有了一些新内容之后，我却可以通过当时得到的结果值，直接获得新的内容，而不需要再次调用相应的方法。&nbsp;这就是典型的非标准读取方式。这种读取方式是不应该存在的，即使存在，我们也不应该使用。因为它是在无意中（或者说一不小心）暴露出来的，其行为很可能是不稳定的。&nbsp;在bytes.Buffer中，Bytes方法和Next方法都可能会造成内容的泄露。原因在于，它们都把基于内容容器的切片直接返回给了方法的调用方。&nbsp;我们都知道，通过切片，我们可以直接访问和操纵它的底层数组。不论这个切片是基于某个数组得来的，还是通过对另一个切片做切片操作获得的，都是如此。&nbsp;在这里，Bytes方法和Next方法返回的字节切片，都是通过对内容容器做切片操作得到的。也就是说，它们与内容容器共用了同一个底层数组，起码在一段时期之内是这样的。&nbsp;以Bytes方法为例。它会返回在调用那一刻其所属值中的所有未读内容。示例代码如下：&nbsp; 123456contents := &quot;ab&quot;buffer1 := bytes.NewBufferString(contents)fmt.Printf(&quot;The capacity of new buffer with contents %q: %d\\n&quot;, contents, buffer1.Cap()) // 内容容器的容量为：8。unreadBytes := buffer1.Bytes()fmt.Printf(&quot;The unread bytes of the buffer: %v\\n&quot;, unreadBytes) // 未读内容为：[97 98]。 &nbsp; 我用字符串值&quot;ab&quot;初始化了一个Buffer值，由变量buffer1代表，并打印了当时该值的一些状态。 &nbsp;你可能会有疑惑，我只在这个Buffer值中放入了一个长度为2的字符串值，但为什么该值的容量却变为了8。&nbsp;虽然这与我们当前的主题无关，但是我可以提示你一下：你可以去阅读runtime包中一个名叫stringtoslicebyte的函数，答案就在其中。&nbsp;接着说buffer1。我又向该值写入了字符串值&quot;cdefg&quot;，此时，其容量仍然是8。我在前面通过调用buffer1的Bytes方法得到的结果值unreadBytes，包含了在那时其中的所有未读内容。&nbsp;但是，由于这个结果值与buffer1的内容容器在此时还共用着同一个底层数组，所以，我只需通过简单的再切片操作，就可以利用这个结果值拿到buffer1在此时的所有未读内容。如此一来，buffer1的新内容就被泄露出来了。&nbsp; 1234buffer1.WriteString(&quot;cdefg&quot;)fmt.Printf(&quot;The capacity of buffer: %d\\n&quot;, buffer1.Cap()) // 内容容器的容量仍为：8。unreadBytes = unreadBytes[:cap(unreadBytes)]fmt.Printf(&quot;The unread bytes of the buffer: %v\\n&quot;, unreadBytes) // 基于前面获取到的结果值可得，未读内容为：[97 98 99 100 101 102 103 0]。 &nbsp; 如果我当时把unreadBytes的值传到了外界，那么外界就可以通过该值操纵buffer1的内容了，就像下面这样： &nbsp; 12unreadBytes[len(unreadBytes)-2] = byte(&#x27;X&#x27;) // &#x27;X&#x27;的 ASCII 编码为 88。fmt.Printf(&quot;The unread bytes of the buffer: %v\\n&quot;, buffer1.Bytes()) // 未读内容变为了：[97 98 99 100 101 102 88]。 &nbsp; 现在，你应该能够体会到，这里的内容泄露可能造成的严重后果了吧？对于Buffer值的Next方法，也存在相同的问题。 &nbsp;不过，如果经过扩容，Buffer值的内容容器或者它的底层数组被重新设定了，那么之前的内容泄露问题就无法再进一步发展了。&nbsp;&nbsp; 原文链接：https://time.geekbang.org/column/intro/112","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Go语言","slug":"Go语言","permalink":"http://wht6.github.io/tags/Go%E8%AF%AD%E8%A8%80/"}]},{"title":"Kali的学习和使用","slug":"Kali的学习和使用","date":"2021-07-11T08:00:00.000Z","updated":"2022-03-29T02:00:35.435Z","comments":true,"path":"posts/480f.html","link":"","permalink":"http://wht6.github.io/posts/480f.html","excerpt":"","text":"基础DNS的域名记录的几种形式： A记录（address）正向解析。主机名于ip关联起来，通过域名找ip。 PTR记录（Pointer）反向解析，主机名于ip关联起来，通过ip找域名。 CNAME记录（canonical name）别名。允许多个域名映射到同一台服务器。 MX记录（mail exchange）指向邮件服务器。根据邮箱地址定位mail服务器。 NS记录（name sever）指定该域名是由哪个域名服务器解析的。 系统的host文件包括常用的域名和对应的ip（其中包括你之前访问过的域名），相当于本地DNS，解析域名的时候会先去查找本地host文件中是否有域名对应的ip。修改host文件可以屏蔽一些网站，比如修改域名的ip为127.0.0.1。 DNS查询步骤：1 浏览器缓存——2 系统缓存（host文件）——3 路由器缓存——4 ISP服务器（本地DNS服务器）的DNS缓存——5 根域名服务器——6 顶级域名服务器——7 主域名服务器——返回IP并保存结果到缓存中。 获取域名的IP地址 直接发送一个ping包，ping xxx.xxx -c 1。 tracert路由跟踪：tracert xxx.xxx。 nslookup xxx.xxx。 dig工具：dig @114.114.114.114 xxx.xxx any，指定DNS服务，查看所有IP（不一定能查询到）。 dig -x 114.114.114.114，反向解析，通过IP查域名 dig txt chaos VERSION.BIND @ns3.dns4.com，查询dns服务器的版本号，不一定能查到，可能会被屏蔽 whois xxx.xxx，查询域名的注册信息（不一定能查到）。","categories":[{"name":"网络","slug":"网络","permalink":"http://wht6.github.io/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"网络安全","slug":"网络安全","permalink":"http://wht6.github.io/tags/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"}]},{"title":"frp远程访问","slug":"frp远程访问","date":"2021-06-22T01:00:00.000Z","updated":"2022-03-20T01:46:22.511Z","comments":true,"path":"posts/a3d2.html","link":"","permalink":"http://wht6.github.io/posts/a3d2.html","excerpt":"","text":"frp远程访问frp原理frp主要进行反向代理，将数据包定向转发，多用于内网穿透。 下图以SSH为例，说明frp是如何实现内网穿透的。 frp通过建立上图的三个通道，并把三个通道进行连通，即实现了ssh的内网穿透。 具体步骤如下： 1）frpc注册frps，两者建立TCP连接，frpc将端口映射信息发送给frps，frps根据端口映射信息建立连接池，监听固定端口的连接。 2）位于因特网的ssh客户端向frps发送连接请求，frps向frpc发送请求信息，frpc收到后请求ssh服务端建立连接，若连接失败，则frpc通过frps向ssh客户端发送连接失败的信息，否则ssh服务端通过frpc和frps发送验证请求到ssh客户端。 3）ssh客户端通过frps和frpc发送用户名和密码到ssh服务端，ssh服务段验证成功后返回连接成功的消息，并等待数据的发送，否则验证失败断开连接。 frp配置frp下载地址 下载之后解压： 1tar -zxvf frp_0.34.3_linux_amd64.tar.gz 配置frps修改配置文件frps.ini 1234567891011121314# 通用配置[common]# bind_addr = 0.0.0.0 # 指定监听的地址，默认监听所有网段bind_port = 7000 # frps的监听端口，用于与frpc连接dashboard_port = 7500 # frps的web控制面板的端口号，web界面用于查看和管理连接信息dashboard_user = user # web控制面板的用户名dashboard_pwd = pass # web控制面板的登陆密码authentication_method = token # 口令验证token = xxxxx # frpc连接时的验证密码# max_pool_count = 100 # 连接池最大连接数量 1./frps -c ./frps.ini # shell启动frps服务 通过浏览器输入（服务器IP:端口）可访问web控制面板。 配置frpc修改配置文件frpc.ini 1234567891011121314151617181920[common]server_addr = xx.xx.xx.xx # frps服务器的公网ipauthentication_method = token # 口令验证token = xxxxx # frps连接密码 server_port = 7000 # frps用于连接的服务端口# ssh映射[Fusion-ssh]type = tcplocal_ip = 127.0.0.1 local_port = 22remote_port = 20022 # rdp映射[Fusion-rdp]type = tcplocal_ip = 127.0.0.1local_port = 3389remote_port = 23389 1./frps -c ./frps.ini # shell启动frpc 内网服务器开启ssh： 1vim /etc/ssh/sshd_config # ssh的配置文件 修改30几行，permitrootlogin改成yes，允许root登录 pubkeyauthentication改成yes,允许公钥认证 12systemctl restart ssh # 或/etc/init.d/ssh restart 重启ssh服务update-rc.d ssh enable # 允许开机自启动ssh 此时，ssh客户端连接frps的20022端口，frps通过代理就可以访问ssh服务器的22端口，然后验证并建立连接。 rdp是远程桌面。 配置frp自启动1vim /lib/systemd/system/frps.service 12345678910111213[Unit]Description=fraps serviceAfter=network.target network-online.target syslog.targetWants=network.target network-online.target[Service]Type=simple# 启动服务的命令（此处写你的frps的实际安装目录）ExecStart=/your/path/frps -c /your/path/frps.ini[Install]WantedBy=multi-user.target 123systemctl start frps # 开启systemctl enable frps # 自启动systemctl status frps # 状态 1vim /etc/systemd/system/frps.service # etc或lib都行 12345678910111213141516[Fusion]Description=Frp Server DaemonAfter=syslog.target network.targetWants=network.target[Service]Type=simpleExecStart=/usr/local/bin/frp/frpc -c /usr/local/bin/frp/frpc.ini # 修改为你的frpc实际安装目录ExecStop=/usr/bin/killall frpcRestartSec=1min #启动失败1分钟后再次启动KillMode=control-groupRestart=always #重启控制：总是重启[Install]WantedBy=multi-user.target 123systemctl start frpc.service # 开启systemctl enable frpc.service # 自启动systemctl status frpc.service # 状态","categories":[{"name":"网络","slug":"网络","permalink":"http://wht6.github.io/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"工具","slug":"工具","permalink":"http://wht6.github.io/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"搭建ss服务器","slug":"搭建ss服务器","date":"2021-06-20T06:20:00.000Z","updated":"2022-03-20T01:46:36.914Z","comments":true,"path":"posts/486.html","link":"","permalink":"http://wht6.github.io/posts/486.html","excerpt":"","text":"搭建ss服务器准备工作首先需要有一台外网服务器和xshell等ssh远程连接工具。通过xshell连接服务器给定的ip和端口，登录验证，然后通过远程shell连接服务器，下面的配置过程一centos 7 x64为例。 配置ss123456789# 安装wgetyum -y install wget# 从github上下载sswget --no-check-certificate -O shadowsocks-all.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-all.sh# 给予文件可执行权限chmod +x shadowsocks-all.sh# 开始安装./shadowsocks-all.sh 2&gt;&amp;1 | tee shadowsocks-all.log 安装的过程中会提示你选择哪个版本，选择 Shadowsocks-libev版本。 然后是让你设定一个连接密码。 接着设定一个监听的端口号。 然后选择一种加密方式，这里我选择的是aes-256-gcm。 接下来问你是否安装simeple-obfs，是y，然后选择tls。 安装成功后会显示你的ss的所有配置信息。 如果要修改配置，运行 1bash shadowsocks-libev.sh 然后选择第8项：修改 Shadowsocks 配置即可重新设置 Shadowsocks 的密码、端口以及加密方式。 选择第2项则是卸载ss。也可以运行 1./shadowsocks-all.sh uninstall 来卸载。 1/etc/init.d/shadowsocks-libev start | stop | restart | status # 启动、停止、重启、查看状态 /etc/shadowsocks-libev/config.json是ss的配置文件。 ss加速下载并执行91yun的一键安装锐速脚本： 1wget -N --no-check-certificate https://github.com/91yun/serverspeeder/raw/master/serverspeeder.sh &amp;&amp; bash serverspeeder.sh 但是这样直接安装的时候很可能出现内核版本不匹配的问题。因此需要修改系统内核为锐速支持的内核版本。 centos 6支持的内核版本：2.6.32-504.3.3.el6.x86_64 123rpm -ivh http://soft.91yun.org/ISO/Linux/CentOS/kernel/kernel-firmware-2.6.32-504.3.3.el6.noarch.rpmrpm -ivh http://soft.91yun.org/ISO/Linux/CentOS/kernel/kernel-2.6.32-504.3.3.el6.x86_64.rpm --force centos 7支持的内核版本：3.10.0-229.1.2.el7.x86_64 1rpm -ivh http://soft.91yun.org/ISO/Linux/CentOS/kernel/kernel-3.10.0-229.1.2.el7.x86_64.rpm --force 升级/降级系统内核后执行如下命令查看是否修改成功： 123rpm -qa | grep kernel # 查看内核rebootuname -r # 再次查看内核 锐速加速常用的命令： 123456789service serverSpeeder start #启动service serverSpeeder stop #停止service serverSpeeder reload #重新加载配置service serverSpeeder restart #重启service serverSpeeder status #状态service serverSpeeder stats #统计service serverSpeeder renewLic #更新许可文件service serverSpeeder update #更新chattr -i /serverspeeder/etc/apx* &amp;&amp; /serverspeeder/bin/serverSpeeder.sh uninstall -f #卸载 ss多端口先停止 ss-server 服务： 12345$ sudo service shadowsocks-libev status * shadowsocks-libev is running$ sudo service shadowsocks-libev stop$ sudo service shadowsocks-libev status * shadowsocks-libev is not running 然后，拷贝一份原来的配置文件，自定义新的文件名，只要保证扩展名为 .json 即可，我这里命名为 configuser1.json ： 123$ cd /etc/shadowsocks-libev$ sudo cp config.json configuser1.json$ sudo vi configuser1.json 修改配置参数中的端口号，密码等。 然后启动 ss-server 服务： 123$ sudo service shadowsocks-libev start$ sudo service shadowsocks-libev status * shadowsocks-libev is running 执行如下命令添加新的配置文件设置 ： 1$ setsid ss-server -c /etc/shadowsocks-libev/***.json -u 如果你嫌上面的“停止-拷贝已有配置文件-重启”操作太麻烦，也可以直接新建一个json配置文件，填入config.json文件的配置信息，修改端口号或密码。 然后直接执行如下命令即可： 1$ setsid ss-server -c /etc/shadowsocks-libev/***.json -u 查看启动信息： 1$ ps ax |grep ss-server 可以看到比之前多了一条后台服务。 通过 netstat -lnp 来查看 ss-server 是否监听了多个端口： 1$ netstat -lnp 这样，就实现了监听多个端口，实现多用户连接了。如果想要停止新增的监听端口，只需要重启shadowsocks服务就又恢复默认，只会监听的 config.json 中配置的端口了。","categories":[{"name":"网络","slug":"网络","permalink":"http://wht6.github.io/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"代理","slug":"代理","permalink":"http://wht6.github.io/tags/%E4%BB%A3%E7%90%86/"}]},{"title":"临时对象池与并发安全字典","slug":"临时对象池","date":"2021-06-13T02:00:00.000Z","updated":"2022-04-10T10:17:03.600Z","comments":true,"path":"posts/b82f.html","link":"","permalink":"http://wht6.github.io/posts/b82f.html","excerpt":"","text":"临时对象池sync.Pool类型可以被称为临时对象池，它的值可以被用来存储临时的对象。与 Go 语言的很多同步工具一样，sync.Pool类型也属于结构体类型，它的值在被真正使用之后，就不应该再被复制了。 &nbsp; 这里的“临时对象”的意思是：不需要持久使用的某一类值。这类值对于程序来说可有可无，但如果有的话会明显更好。它们的创建和销毁可以在任何时候发生，并且完全不会影响到程序的功能。&nbsp;同时，它们也应该是无需被区分的，其中的任何一个值都可以代替另一个。如果你的某类值完全满足上述条件，那么你就可以把它们存储到临时对象池中。&nbsp;你可能已经想到了，我们可以把临时对象池当作针对某种数据的缓存来用。实际上，在我看来，临时对象池最主要的用途就在于此。&nbsp;sync.Pool类型只有两个方法——Put和Get。Put 用于在当前的池中存放临时对象，它接受一个interface&#123;&#125;类型的参数；而 Get 则被用于从当前的池中获取临时对象，它会返回一个interface&#123;&#125;类型的值。&nbsp;更具体地说，这个类型的Get方法可能会从当前的池中删除掉任何一个值，然后把这个值作为结果返回。如果此时当前的池中没有任何值，那么这个方法就会使用当前池的New字段创建一个新值，并直接将其返回。&nbsp;sync.Pool类型的New字段代表着创建临时对象的函数。它的类型是没有参数但有唯一结果的函数类型，即：func() interface&#123;&#125;。&nbsp;这个函数是Get方法最后的临时对象获取手段。Get方法如果到了最后，仍然无法获取到一个值，那么就会调用该函数。该函数的结果值并不会被存入当前的临时对象池中，而是直接返回给Get方法的调用方。&nbsp;这里的New字段的实际值需要我们在初始化临时对象池的时候就给定。否则，在我们调用它的Get方法的时候就有可能会得到nil。所以，sync.Pool类型并不是开箱即用的。不过，这个类型也就只有这么一个公开的字段，因此初始化起来也并不麻烦。&nbsp;举个例子。标准库代码包fmt就使用到了sync.Pool类型。这个包会创建一个用于缓存某类临时对象的sync.Pool类型值，并将这个值赋给一个名为ppFree的变量。这类临时对象可以识别、格式化和暂存需要打印的内容。&nbsp;123var ppFree = sync.Pool&#123; New: func() interface&#123;&#125; &#123; return new(pp) &#125;,&#125;&nbsp; 临时对象池ppFree的New字段在被调用的时候，总是会返回一个全新的pp类型值的指针（即临时对象）。这就保证了ppFree的Get方法总能返回一个可以包含需要打印内容的值。 &nbsp;pp类型是fmt包中的私有类型，它有很多实现了不同功能的方法。不过，这里的重点是，它的每一个值都是独立的、平等的和可重用的。&nbsp; 更具体地说，这些对象既互不干扰，又不会受到外部状态的影响。它们几乎只针对某个需要打印内容的缓冲区而已。由于fmt包中的代码在真正使用这些临时对象之前，总是会先对其进行重置，所以它们并不在意取到的是哪一个临时对象。这就是临时对象的平等性的具体体现。 &nbsp;另外，这些代码在使用完临时对象之后，都会先抹掉其中已缓冲的内容，然后再把它存放到ppFree中。这样就为重用这类临时对象做好了准备。&nbsp;众所周知的fmt.Println、fmt.Printf等打印函数都是如此使用ppFree，以及其中的临时对象的。因此，在程序同时执行很多的打印函数调用的时候，ppFree可以及时地把它缓存的临时对象提供给它们，以加快执行的速度。&nbsp;而当程序在一段时间内不再执行打印函数调用时，ppFree中的临时对象又能够被及时地清理掉，以节省内存空间。&nbsp;显然，在这个维度上，临时对象池可以帮助程序实现可伸缩性。这就是它的最大价值。&nbsp; 内部机制&nbsp;为什么说临时对象池中的值会被及时地清理掉？&nbsp;回答是：因为，Go 语言运行时系统中的垃圾回收器，所以在每次开始执行之前，都会对所有已创建的临时对象池中的值进行全面地清除。&nbsp;在前面已经向你讲述了临时对象会在什么时候被创建，下面我再来详细说说它会在什么时候被销毁。&nbsp;sync包在被初始化的时候，会向 Go 语言运行时系统注册一个函数，这个函数的功能就是清除所有已创建的临时对象池中的值。我们可以把它称为池清理函数。&nbsp;一旦池清理函数被注册到了 Go 语言运行时系统，后者在每次即将执行垃圾回收时就都会执行前者。&nbsp;另外，在sync包中还有一个包级私有的全局变量。这个变量代表了当前的程序中使用的所有临时对象池的汇总，它是元素类型为*sync.Pool的切片。我们可以称之为池汇总列表。&nbsp;通常，在一个临时对象池的Put方法或Get方法第一次被调用的时候，这个池就会被添加到池汇总列表中。正因为如此，池清理函数总是能访问到所有正在被真正使用的临时对象池。&nbsp;更具体地说，池清理函数会遍历池汇总列表。对于其中的每一个临时对象池，它都会先将池中所有的私有临时对象和共享临时对象列表都置为nil，然后再把这个池中的所有本地池列表都销毁掉。&nbsp;最后，池清理函数会把池汇总列表重置为空的切片。如此一来，这些池中存储的临时对象就全部被清除干净了。&nbsp;如果临时对象池以外的代码再无对它们的引用，那么在稍后的垃圾回收过程中，这些临时对象就会被当作垃圾销毁掉，它们占用的内存空间也会被回收以备他用。&nbsp;临时对象池存储值所用的数据结构是怎样的？&nbsp;在临时对象池中，有一个多层的数据结构。正因为有了它的存在，临时对象池才能够非常高效地存储大量的值。&nbsp;这个数据结构的顶层，我们可以称之为本地池列表，不过更确切地说，它是一个数组。这个列表的长度，总是与 Go 语言调度器中的 P 的数量相同。&nbsp;还记得吗？Go 语言调度器中的 P 是 processor 的缩写，它指的是一种可以承载若干个 G、且能够使这些 G 适时地与 M 进行对接，并得到真正运行的中介。&nbsp;这里的 G 正是 goroutine 的缩写，而 M 则是 machine 的缩写，后者指代的是系统级的线程。正因为有了 P 的存在，G 和 M 才能够进行灵活、高效的配对，从而实现强大的并发编程模型。&nbsp;P 存在的一个很重要的原因是为了分散并发程序的执行压力，而让临时对象池中的本地池列表的长度与 P 的数量相同的主要原因也是分散压力。这里所说的压力包括了存储和性能两个方面。在说明它们之前，我们先来探索一下临时对象池中的那个数据结构。&nbsp;在本地池列表中的每个本地池都包含了三个字段（或者说组件），它们是：存储私有临时对象的字段private、代表了共享临时对象列表的字段shared，以及一个sync.Mutex类型的嵌入字段。&nbsp;sync.Pool 中的本地池与各个 G 的对应关系如图所示。&nbsp;&nbsp;实际上，每个本地池都对应着一个 P。我们都知道，一个 goroutine 要想真正运行就必须先与某个 P 产生关联。也就是说，一个正在运行的 goroutine 必然会关联着某个 P。&nbsp;在程序调用临时对象池的Put方法或Get方法的时候，总会先试图从该临时对象池的本地池列表中，获取与之对应的本地池，依据的就是与当前的 goroutine 关联的那个 P 的 ID。&nbsp;换句话说，一个临时对象池的Put方法或Get方法会获取到哪一个本地池，完全取决于调用它的代码所在的 goroutine 关联的那个 P。&nbsp;临时对象池是怎样利用内部数据结构来存取值的？&nbsp;临时对象池的Put方法总会先试图把新的临时对象，存储到对应的本地池的private字段中，以便在后面获取临时对象的时候，可以快速地拿到一个可用的值。&nbsp;只有当这个private字段已经存有某个值时，该方法才会去访问本地池的shared字段。&nbsp;相应的，临时对象池的Get方法，总会先试图从对应的本地池的private字段处获取一个临时对象。只有当这个private字段的值为nil时，它才会去访问本地池的shared字段。&nbsp;一个本地池的shared字段原则上可以被任何 goroutine 中的代码访问到，不论这个 goroutine 关联的是哪一个 P。这也是我把它叫做共享临时对象列表的原因。&nbsp;相比之下，一个本地池的private字段，只可能被与之对应的那个 P 所关联的 goroutine 中的代码访问到，所以可以说，它是 P 级私有的。&nbsp;以临时对象池的Put方法为例，它一旦发现对应的本地池的private字段已存有值，就会去访问这个本地池的shared字段。当然，由于shared字段是共享的，所以此时必须受到互斥锁的保护。&nbsp;还记得本地池嵌入的那个sync.Mutex类型的字段吗？它就是这里用到的互斥锁，也就是说，本地池本身就拥有互斥锁的功能。Put方法会在互斥锁的保护下，把新的临时对象追加到共享临时对象列表的末尾。&nbsp;相应的，临时对象池的Get方法在发现对应本地池的private字段未存有值时，也会去访问后者的shared字段。它会在互斥锁的保护下，试图把该共享临时对象列表中的最后一个元素值取出并作为结果。&nbsp;不过，这里的共享临时对象列表也可能是空的，这可能是由于这个本地池中的所有临时对象都已经被取走了，也可能是当前的临时对象池刚被清理过。&nbsp;无论原因是什么，Get方法都会去访问当前的临时对象池中的所有本地池，它会去逐个搜索它们的共享临时对象列表。&nbsp;只要发现某个共享临时对象列表中包含元素值，它就会把该列表的最后一个元素值取出并作为结果返回。&nbsp;从 sync.Pool 中获取临时对象的步骤如图所示。&nbsp;&nbsp;当然了，即使这样也可能无法拿到一个可用的临时对象，比如，在所有的临时对象池都刚被大清洗的情况下就会是如此。&nbsp;这时，Get方法就会使出最后的手段——调用可创建临时对象的那个函数。还记得吗？这个函数是由临时对象池的New字段代表的，并且需要我们在初始化临时对象池的时候给定。如果这个字段的值是nil，那么Get方法此时也只能返回nil了。&nbsp;在临时对象池的内部，有一个多层的数据结构支撑着对临时对象的存储。它的顶层是本地池列表，其中包含了与某个 P 对应的那些本地池，并且其长度与 P 的数量总是相同的。&nbsp;在每个本地池中，都包含一个私有的临时对象和一个共享的临时对象列表。前者只能被其对应的 P 所关联的那个 goroutine 中的代码访问到，而后者却没有这个约束。从另一个角度讲，前者用于临时对象的快速存取，而后者则用于临时对象的池内共享。&nbsp;正因为有了这样的数据结构，临时对象池才能够有效地分散存储压力和性能压力。同时，又因为临时对象池的Get方法对这个数据结构的妙用，才使得其中的临时对象能够被高效地利用。比如，该方法有时候会从其他的本地池的共享临时对象列表中，“偷取”一个临时对象。&nbsp;这样的内部结构和存取方式，让临时对象池成为了一个特点鲜明的同步工具。它存储的临时对象都应该是拥有较长生命周期的值，并且，这些值不应该被某个 goroutine 中的代码长期的持有和使用。&nbsp;因此，临时对象池非常适合用作针对某种数据的缓存。从某种角度讲，临时对象池可以帮助程序实现可伸缩性，这也正是它的最大价值。&nbsp;怎样保证一个临时对象池中总有比较充足的临时对象？&nbsp;答：首先，我们应该事先向临时对象池中放入足够多的临时对象。其次，在用完临时对象之后，我们需要及时地把它归还给临时对象池。最后，我们应该保证它的New字段所代表的值是可用的。虽然New函数返回的临时对象并不会被放入池中，但是起码能够保证池的Get方法总能返回一个临时对象。 并发安全字典在sync包中提供了一个并发安全的高级数据结构sync.Map，而Go 语言自带的字典类型map并不是并发安全的。 &nbsp; 换句话说，在同一时间段内，让不同 goroutine 中的代码，对同一个字典进行读写操作是不安全的。字典值本身可能会因这些操作而产生混乱，相关的程序也可能会因此发生不可预知的问题。&nbsp;在sync.Map出现之前，我们如果要实现并发安全的字典，就只能自行构建。不过，这其实也不是什么麻烦事，使用 sync.Mutex或sync.RWMutex，再加上原生的map就可以轻松地做到。&nbsp;sync.Map这个字典类型提供了一些常用的键值存取操作方法，并保证了这些操作的并发安全。同时，它的存、取、删等操作都可以基本保证在常数时间内执行完毕。换句话说，它们的算法复杂度与map类型一样都是O(1)的。&nbsp;在有些时候，与单纯使用原生map和互斥锁的方案相比，使用sync.Map可以显著地减少锁的争用。sync.Map本身虽然也用到了锁，但是，它其实在尽可能地避免使用锁。&nbsp;我们都知道，使用锁就意味着要把一些并发的操作强制串行化。这往往会降低程序的性能，尤其是在计算机拥有多个 CPU 核心的情况下。&nbsp;因此，我们常说，能用原子操作就不要用锁，不过这很有局限性，毕竟原子只能对一些基本的数据类型提供支持。&nbsp;无论在何种场景下使用sync.Map，我们都需要注意，与原生map明显不同，它只是 Go 语言标准库中的一员，而不是语言层面的东西。也正因为这一点，Go 语言的编译器并不会对它的键和值，进行特殊的类型检查。&nbsp;如果你看过sync.Map的文档或者实际使用过它，那么就一定会知道，它所有的方法涉及的键和值的类型都是interface&#123;&#125;，也就是空接口，这意味着可以包罗万象。所以，我们必须在程序中自行保证它的键类型和值类型的正确性。&nbsp;键的实际类型不能是函数类型、字典类型和切片类型。&nbsp;Go 语言的原生字典的键类型不能是函数类型、字典类型和切片类型。由于并发安全字典内部使用的存储介质正是原生字典，又因为它使用的原生字典键类型也是可以包罗万象的interface&#123;&#125;；所以，我们绝对不能带着任何实际类型为函数类型、字典类型或切片类型的键值去操作并发安全字典。&nbsp;由于这些键值的实际类型只有在程序运行期间才能够确定，所以 Go 语言编译器是无法在编译期对它们进行检查的，不正确的键值实际类型肯定会引发 panic。&nbsp;因此，我们在这里首先要做的一件事就是：一定不要违反上述规则。我们应该在每次操作并发安全字典的时候，都去显式地检查键值的实际类型。无论是存、取还是删，都应该如此。&nbsp;当然，更好的做法是，把针对同一个并发安全字典的这几种操作都集中起来，然后统一地编写检查代码。除此之外，把并发安全字典封装在一个结构体类型中，往往是一个很好的选择。&nbsp;总之，我们必须保证键的类型是可比较的（或者说可判等的）。如果你实在拿不准，那么可以先通过调用reflect.TypeOf函数得到一个键值对应的反射类型值（即：reflect.Type类型的值），然后再调用这个值的Comparable方法，得到确切的判断结果。&nbsp;怎样保证并发安全字典中的键和值的类型正确性？&nbsp;第一种方案是，让并发安全字典只能存储某个特定类型的键。&nbsp;比如，指定这里的键只能是int类型的，或者只能是字符串，又或是某类结构体。一旦完全确定了键的类型，你就可以在进行存、取、删操作的时候，使用类型断言表达式去对键的类型做检查了。&nbsp;一般情况下，这种检查并不繁琐。而且，你要是把并发安全字典封装在一个结构体类型里面，那就更加方便了。你这时完全可以让 Go 语言编译器帮助你做类型检查。请看下面的代码：&nbsp; 1234567891011121314151617181920212223242526272829303132type IntStrMap struct &#123; m sync.Map&#125; func (iMap *IntStrMap) Delete(key int) &#123; iMap.m.Delete(key)&#125; func (iMap *IntStrMap) Load(key int) (value string, ok bool) &#123; v, ok := iMap.m.Load(key) if v != nil &#123; value = v.(string) &#125; return&#125; func (iMap *IntStrMap) LoadOrStore(key int, value string) (actual string, loaded bool) &#123; a, loaded := iMap.m.LoadOrStore(key, value) actual = a.(string) return&#125; func (iMap *IntStrMap) Range(f func(key int, value string) bool) &#123; f1 := func(key, value interface&#123;&#125;) bool &#123; return f(key.(int), value.(string)) &#125; iMap.m.Range(f1)&#125; func (iMap *IntStrMap) Store(key int, value string) &#123; iMap.m.Store(key, value)&#125; &nbsp; 如上所示，我编写了一个名为IntStrMap的结构体类型，它代表了键类型为int、值类型为string的并发安全字典。在这个结构体类型中，只有一个sync.Map类型的字段m。并且，这个类型拥有的所有方法，都与sync.Map类型的方法非常类似。 &nbsp;两者对应的方法名称完全一致，方法签名也非常相似，只不过，与键和值相关的那些参数和结果的类型不同而已。在IntStrMap类型的方法签名中，明确了键的类型为int，且值的类型为string。&nbsp;显然，这些方法在接受键和值的时候，就不用再做类型检查了。另外，这些方法在从m中取出键和值的时候，完全不用担心它们的类型会不正确，因为它的正确性在当初存入的时候，就已经由 Go 语言编译器保证了。&nbsp;第一种方案适用于我们可以完全确定键和值的具体类型的情况。在这种情况下，我们可以利用 Go 语言编译器去做类型检查，并用类型断言表达式作为辅助，就像IntStrMap那样。&nbsp;这样做很方便，不是吗？不过，虽然方便，但是却让这样的字典类型缺少了一些灵活性。&nbsp;如果我们还需要一个键类型为uint32并发安全字典的话，那就不得不再如法炮制地写一遍代码了。因此，在需求多样化之后，工作量反而更大，甚至会产生很多雷同的代码。&nbsp;在第二种方案中，我们封装的结构体类型的所有方法，都可以与sync.Map类型的方法完全一致（包括方法名称和方法签名）。&nbsp;不过，在这些方法中，我们就需要添加一些做类型检查的代码了。另外，这样并发安全字典的键类型和值类型，必须在初始化的时候就完全确定。并且，这种情况下，我们必须先要保证键的类型是可比较的。&nbsp;所以在设计这样的结构体类型的时候，只包含sync.Map类型的字段就不够了。&nbsp; 12345type ConcurrentMap struct &#123; m sync.Map keyType reflect.Type valueType reflect.Type&#125; &nbsp; 这里ConcurrentMap类型代表的是：可自定义键类型和值类型的并发安全字典。这个类型同样有一个sync.Map类型的字段m，代表着其内部使用的并发安全字典。 &nbsp;另外，它的字段keyType和valueType，分别用于保存键类型和值类型。这两个字段的类型都是reflect.Type，我们可称之为反射类型。&nbsp;这个类型可以代表 Go 语言的任何数据类型。并且，这个类型的值也非常容易获得：通过调用reflect.TypeOf函数并把某个样本值传入即可。&nbsp;调用表达式reflect.TypeOf(int(123))的结果值，就代表了int类型的反射类型值。&nbsp;我们现在来看一看ConcurrentMap类型方法应该怎么写。&nbsp;先说Load方法，这个方法接受一个interface&#123;&#125;类型的参数key，参数key代表了某个键的值。&nbsp;因此，当我们根据 ConcurrentMap 在m字段的值中查找键值对的时候，就必须保证 ConcurrentMap 的类型是正确的。由于反射类型值之间可以直接使用操作符==或!=进行判等，所以这里的类型检查代码非常简单。&nbsp; 123456func (cMap *ConcurrentMap) Load(key interface&#123;&#125;) (value interface&#123;&#125;, ok bool) &#123; if reflect.TypeOf(key) != cMap.keyType &#123; return &#125; return cMap.m.Load(key)&#125; &nbsp; 我们把一个接口类型值传入reflect.TypeOf函数，就可以得到与这个值的实际类型对应的反射类型值。 &nbsp;因此，如果参数值的反射类型与keyType字段代表的反射类型不相等，那么我们就忽略后续操作，并直接返回。&nbsp;这时，Load方法的第一个结果value的值为nil，而第二个结果ok的值为false。这完全符合Load方法原本的含义。&nbsp;再来说Store方法。Store方法接受两个参数key和value，它们的类型也都是interface&#123;&#125;。因此，我们的类型检查应该针对它们来做。&nbsp; 123456789func (cMap *ConcurrentMap) Store(key, value interface&#123;&#125;) &#123; if reflect.TypeOf(key) != cMap.keyType &#123; panic(fmt.Errorf(&quot;wrong key type: %v&quot;, reflect.TypeOf(key))) &#125; if reflect.TypeOf(value) != cMap.valueType &#123; panic(fmt.Errorf(&quot;wrong value type: %v&quot;, reflect.TypeOf(value))) &#125; cMap.m.Store(key, value)&#125; &nbsp; 这里的类型检查代码与Load方法中的代码很类似，不同的是对检查结果的处理措施。当参数key或value的实际类型不符合要求时，Store方法会立即引发 panic。 &nbsp;这主要是由于Store方法没有结果声明，所以在参数值有问题的时候，它无法通过比较平和的方式告知调用方。不过，这也是符合Store方法的原本含义的。&nbsp;如果你不想这么做，也是可以的，那么就需要为Store方法添加一个error类型的结果。&nbsp;并且，在发现参数值类型不正确的时候，让它直接返回相应的error类型值，而不是引发 panic。要知道，这里展示的只一个参考实现，你可以根据实际的应用场景去做优化和改进。&nbsp;至于与ConcurrentMap类型相关的其他方法和函数，我在这里就不展示了。&nbsp;并发安全字典如何做到尽量避免使用锁？&nbsp;与单纯使用原生字典和互斥锁的方案相比，使用sync.Map可以显著地减少锁的争用。sync.Map本身确实也用到了锁，但是，它会尽可能地避免使用锁。&nbsp;sync.Map类型在内部使用了大量的原子操作来存取键和值，并使用了两个原生的map作为存储介质。&nbsp;其中一个原生map被存在了sync.Map的read字段中，该字段是sync/atomic.Value类型的。 这个原生字典可以被看作一个快照，它总会在条件满足时，去重新保存所属的sync.Map值中包含的所有键值对。&nbsp;为了描述方便，我们在后面简称它为只读字典。不过，只读字典虽然不会增减其中的键，但却允许变更其中的键所对应的值。所以，它并不是传统意义上的快照，它的只读特性只是对于其中键的集合而言的。&nbsp;由read字段的类型可知，sync.Map在替换只读字典的时候根本用不着锁。另外，这个只读字典在存储键值对的时候，还在值之上封装了一层。&nbsp;它先把值转换为了unsafe.Pointer类型的值，然后再把后者封装，并储存在其中的原生字典中。如此一来，在变更某个键所对应的值的时候，就也可以使用原子操作了。&nbsp;sync.Map中的另一个原生字典由它的dirty字段代表。 它存储键值对的方式与read字段中的原生字典一致，它的键类型也是interface&#123;&#125;，并且同样是把值先做转换和封装后再进行储存的。我们暂且把它称为脏字典。&nbsp;注意，脏字典和只读字典如果都存有同一个键值对，那么这里的两个键指的肯定是同一个基本值，对于两个值来说也是如此。&nbsp;正如前文所述，这两个字典在存储键和值的时候都只会存入它们的某个指针，而不是基本值。&nbsp;sync.Map在查找指定的键所对应的值的时候，总会先去只读字典中寻找，并不需要锁定互斥锁。只有当确定“只读字典中没有，但脏字典中可能会有这个键”的时候，它才会在锁的保护下去访问脏字典。&nbsp;相对应的，sync.Map在存储键值对的时候，只要只读字典中已存有这个键，并且该键值对未被标记为“已删除”，就会把新值存到里面并直接返回，这种情况下也不需要用到锁。&nbsp;否则，它才会在锁的保护下把键值对存储到脏字典中。这个时候，该键值对的“已删除”标记会被抹去。&nbsp; &nbsp; 顺便说一句，只有当一个键值对应该被删除，但却仍然存在于只读字典中的时候，才会被用标记为“已删除”的方式进行逻辑删除，而不会直接被物理删除。&nbsp;这种情况会在重建脏字典以后的一段时间内出现。不过，过不了多久，它们就会被真正删除掉。在查找和遍历键值对的时候，已被逻辑删除的键值对永远会被无视。&nbsp;对于删除键值对，sync.Map会先去检查只读字典中是否有对应的键。如果没有，脏字典中可能有，那么它就会在锁的保护下，试图从脏字典中删掉该键值对。&nbsp;最后，sync.Map会把该键值对中指向值的那个指针置为nil，这是另一种逻辑删除的方式。&nbsp;除此之外，还有一个细节需要注意，只读字典和脏字典之间是会互相转换的。在脏字典中查找键值对次数足够多的时候，sync.Map会把脏字典直接作为只读字典，保存在它的read字段中，然后把代表脏字典的dirty字段的值置为nil。&nbsp;在这之后，一旦再有新的键值对存入，它就会依据只读字典去重建脏字典。这个时候，它会把只读字典中已被逻辑删除的键值对过滤掉。理所当然，这些转换操作肯定都需要在锁的保护下进行。&nbsp;&nbsp;综上所述，sync.Map的只读字典和脏字典中的键值对集合，并不是实时同步的，它们在某些时间段内可能会有不同。&nbsp;由于只读字典中键的集合不能被改变，所以其中的键值对有时候可能是不全的。相反，脏字典中的键值对集合总是完全的，并且其中不会包含已被逻辑删除的键值对。&nbsp;因此，可以看出，在读操作有很多但写操作却很少的情况下，并发安全字典的性能往往会更好。在几个写操作当中，新增键值对的操作对并发安全字典的性能影响是最大的，其次是删除操作，最后才是修改操作。&nbsp;如果被操作的键值对已经存在于sync.Map的只读字典中，并且没有被逻辑删除，那么修改它并不会使用到锁，对其性能的影响就会很小。&nbsp;&nbsp; 原文链接：https://time.geekbang.org/column/intro/112","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Go语言","slug":"Go语言","permalink":"http://wht6.github.io/tags/Go%E8%AF%AD%E8%A8%80/"}]},{"title":"原子操作和线程协作","slug":"原子操作","date":"2021-06-10T11:00:00.000Z","updated":"2022-04-10T10:13:10.607Z","comments":true,"path":"posts/9000.html","link":"","permalink":"http://wht6.github.io/posts/9000.html","excerpt":"","text":"原子操作原子性执行和原子操作&nbsp; 我们已经知道，对于一个 Go 程序来说，Go 语言运行时系统中的调度器会恰当地安排其中所有的 goroutine 的运行。不过，在同一时刻，只可能有少数的 goroutine 真正地处于运行状态，并且这个数量只会与 M 的数量一致，而不会随着 G 的增多而增长。&nbsp;所以，为了公平起见，调度器总是会频繁地换上或换下这些 goroutine。换上的意思是，让一个 goroutine 由非运行状态转为运行状态，并促使其中的代码在某个 CPU 核心上执行。&nbsp;换下的意思正好相反，即：使一个 goroutine 中的代码中断执行，并让它由运行状态转为非运行状态。&nbsp;这个中断的时机有很多，任何两条语句执行的间隙，甚至在某条语句执行的过程中都是可以的。&nbsp;即使这些语句在临界区之内也是如此。所以，我们说，互斥锁虽然可以保证临界区中代码的串行执行，但却不能保证这些代码执行的原子性（atomicity）。&nbsp;在众多的同步工具中，真正能够保证原子性执行的只有原子操作（atomic operation）。原子操作在进行的过程中是不允许中断的。在底层，这会由 CPU 提供芯片级别的支持，所以绝对有效。即使在拥有多 CPU 核心，或者多 CPU 的计算机系统中，原子操作的保证也是不可撼动的。&nbsp;这使得原子操作可以完全地消除竞态条件，并能够绝对地保证并发安全性。并且，它的执行速度要比其他的同步工具快得多，通常会高出好几个数量级。不过，它的缺点也很明显。&nbsp;更具体地说，正是因为原子操作不能被中断，所以它需要足够简单，并且要求快速。&nbsp;你可以想象一下，如果原子操作迟迟不能完成，而它又不会被中断，那么将会给计算机执行指令的效率带来多么大的影响。因此，操作系统层面只对针对二进制位或整数的原子操作提供了支持。&nbsp;Go 语言的原子操作当然是基于 CPU 和操作系统的，所以它也只针对少数数据类型的值提供了原子操作函数。这些函数都存在于标准库代码包sync/atomic中。&nbsp; sync/atomic包的使用&nbsp;sync/atomic包中提供了几种原子操作？可操作的数据类型又有哪些？&nbsp;sync/atomic包中的函数可以做的原子操作有：加法（add）、比较并交换（compare and swap，简称 CAS）、加载（load）、存储（store）和交换（swap）。&nbsp;这些函数针对的数据类型并不多。但是，对这些类型中的每一个，sync/atomic包都会有一套函数给予支持。这些数据类型有：int32、int64、uint32、uint64、uintptr，以及unsafe包中的Pointer。不过，针对unsafe.Pointer类型，该包并未提供进行原子加法操作的函数。&nbsp;此外，sync/atomic包还提供了一个名为Value的类型，它可以被用来存储任意类型的值。&nbsp;我们都知道，传入这些原子操作函数的第一个参数值对应的都应该是那个被操作的值。比如，atomic.AddInt32函数的第一个参数，对应的一定是那个要被增大的整数。可是，这个参数的类型为什么不是int32而是*int32呢？&nbsp;回答是：因为原子操作函数需要的是被操作值的指针，而不是这个值本身；被传入函数的参数值都会被复制，像这种基本类型的值一旦被传入函数，就已经与函数外的那个值毫无关系了。&nbsp;所以，传入值本身没有任何意义。unsafe.Pointer类型虽然是指针类型，但是那些原子操作函数要操作的是这个指针值，而不是它指向的那个值，所以需要的仍然是指向这个指针值的指针。&nbsp;只要原子操作函数拿到了被操作值的指针，就可以定位到存储该值的内存地址。只有这样，它们才能够通过底层的指令，准确地操作这个内存地址上的数据。&nbsp;用于原子加法操作的函数可以做原子减法吗？比如，atomic.AddInt32函数可以用于减小那个被操作的整数值吗？&nbsp;回答是：当然是可以的。atomic.AddInt32函数的第二个参数代表差量，它的类型是int32，是有符号的。如果我们想做原子减法，那么把这个差量设置为负整数就可以了。&nbsp;对于atomic.AddInt64函数来说也是类似的。不过，要想用atomic.AddUint32和atomic.AddUint64函数做原子减法，就不能这么直接了，因为它们的第二个参数的类型分别是uint32和uint64，都是无符号的，不过，这也是可以做到的，就是稍微麻烦一些。&nbsp;例如，如果想对uint32类型的被操作值18做原子减法，比如说差量是-3，那么我们可以先把这个差量转换为有符号的int32类型的值，然后再把该值的类型转换为uint32，用表达式来描述就是uint32(int32(-3))。&nbsp;不过要注意，直接这样写会使 Go 语言的编译器报错，它会告诉你：“常量-3不在uint32类型可表示的范围内”，换句话说，这样做会让表达式的结果值溢出。&nbsp;不过，如果我们先把int32(-3)的结果值赋给变量delta，再把delta的值转换为uint32类型的值，就可以绕过编译器的检查并得到正确的结果了。&nbsp;最后，我们把这个结果作为atomic.AddUint32函数的第二个参数值，就可以达到对uint32类型的值做原子减法的目的了。&nbsp;还有一种更加直接的方式。我们可以依据下面这个表达式来给定atomic.AddUint32函数的第二个参数值：&nbsp; 1^uint32(-N-1)) &nbsp; 其中的N代表由负整数表示的差量。也就是说，我们先要把差量的绝对值减去1，然后再把得到的这个无类型的整数常量，转换为uint32类型的值，最后，在这个值之上做按位异或操作，就可以获得最终的参数值了。 &nbsp;这么做的原理也并不复杂。简单来说，此表达式的结果值的补码，与使用前一种方法得到的值的补码相同，所以这两种方式是等价的。我们都知道，整数在计算机中是以补码的形式存在的，所以在这里，结果值的补码相同就意味着表达式的等价。&nbsp;比较并交换操作与交换操作相比有什么不同？优势在哪里？&nbsp;回答是：比较并交换操作即 CAS 操作，是有条件的交换操作，只有在条件满足的情况下才会进行值的交换。&nbsp;所谓的交换指的是，把新值赋给变量，并返回变量的旧值。&nbsp;在进行 CAS 操作的时候，函数会先判断被操作变量的当前值，是否与我们预期的旧值相等。如果相等，它就把新值赋给该变量，并返回true以表明交换操作已进行；否则就忽略交换操作，并返回false。&nbsp;可以看到，CAS 操作并不是单一的操作，而是一种操作组合。这与其他的原子操作都不同。正因为如此，它的用途要更广泛一些。例如，我们将它与for语句联用就可以实现一种简易的自旋锁（spinlock）。&nbsp;1234567for &#123; if atomic.CompareAndSwapInt32(&amp;num2, 10, 0) &#123; fmt.Println(&quot;The second number has gone to zero.&quot;) break &#125; time.Sleep(time.Millisecond * 500)&#125;&nbsp; 在for语句中的 CAS 操作可以不停地检查某个需要满足的条件，一旦条件满足就退出for循环。这就相当于，只要条件未被满足，当前的流程就会被一直“阻塞”在这里。 &nbsp;这在效果上与互斥锁有些类似。不过，它们的适用场景是不同的。我们在使用互斥锁的时候，总是假设共享资源的状态会被其他的 goroutine 频繁地改变。&nbsp;而for语句加 CAS 操作的假设往往是：共享资源状态的改变并不频繁，或者，它的状态总会变成期望的那样。这是一种更加乐观，或者说更加宽松的做法。&nbsp;假设我已经保证了对一个变量的写操作都是原子操作，比如：加或减、存储、交换等等，那我对它进行读操作的时候，还有必要使用原子操作吗？&nbsp;回答是：很有必要。其中的道理你可以对照一下读写锁。为什么在读写锁保护下的写操作和读操作之间是互斥的？这是为了防止读操作读到没有被修改完的值，对吗？&nbsp;如果写操作还没有进行完，读操作就来读了，那么就只能读到仅修改了一部分的值。这显然破坏了值的完整性，读出来的值也是完全错误的。&nbsp;所以，一旦你决定了要对一个共享资源进行保护，那就要做到完全的保护。不完全的保护基本上与不保护没有什么区别。&nbsp;怎样用好sync/atomic.Value？&nbsp;此类型的值相当于一个容器，可以被用来“原子地”存储和加载任意的值。&nbsp;atomic.Value类型是开箱即用的，我们声明一个该类型的变量（以下简称原子变量）之后就可以直接使用了。这个类型使用起来很简单，它只有两个指针方法：Store和Load。不过，虽然简单，但还是有一些值得注意的地方的。&nbsp;首先一点，一旦atomic.Value类型的值（以下简称原子值）被真正使用，它就不应该再被复制了。什么叫做“真正使用”呢？&nbsp;我们只要用它来存储值了，就相当于开始真正使用了。atomic.Value类型属于结构体类型，而结构体类型属于值类型。&nbsp;所以，复制该类型的值会产生一个完全分离的新值。这个新值相当于被复制的那个值的一个快照。之后，不论后者存储的值怎样改变，都不会影响到前者，反之亦然。&nbsp;另外，关于用原子值来存储值，有两条强制性的使用规则。第一条规则，不能用原子值存储nil。&nbsp;也就是说，我们不能把nil作为参数值传入原子值的Store方法，否则就会引发一个 panic。&nbsp;这里要注意，如果有一个接口类型的变量，它的动态值是nil，但动态类型却不是nil，那么它的值就不等于nil。我在前面讲接口的时候和你说明过这个问题。正因为如此，这样一个变量的值是可以被存入原子值的。&nbsp;第二条规则，我们向原子值存储的第一个值，决定了它今后能且只能存储哪一个类型的值。&nbsp;例如，我第一次向一个原子值存储了一个string类型的值，那我在后面就只能用该原子值来存储字符串了。如果我又想用它存储结构体，那么在调用它的Store方法的时候就会引发一个 panic。这个 panic 会告诉我，这次存储的值的类型与之前的不一致。&nbsp;你可能会想：我先存储一个接口类型的值，然后再存储这个接口的某个实现类型的值，这样是不是可以呢？&nbsp;很可惜，这样是不可以的，同样会引发一个 panic。因为原子值内部是依据被存储值的实际类型来做判断的。所以，即使是实现了同一个接口的不同类型，它们的值也不能被先后存储到同一个原子值中。&nbsp;遗憾的是，我们无法通过某个方法获知一个原子值是否已经被真正使用，并且，也没有办法通过常规的途径得到一个原子值可以存储值的实际类型。这使得我们误用原子值的可能性大大增加，尤其是在多个地方使用同一个原子值的时候。&nbsp;下面，给你几条具体的使用建议。&nbsp; 不要把内部使用的原子值暴露给外界。比如，声明一个全局的原子变量并不是一个正确的做法。这个变量的访问权限最起码也应该是包级私有的。 如果不得不让包外，或模块外的代码使用你的原子值，那么可以声明一个包级私有的原子变量，然后再通过一个或多个公开的函数，让外界间接地使用到它。注意，这种情况下不要把原子值传递到外界，不论是传递原子值本身还是它的指针值。 如果通过某个函数可以向内部的原子值存储值的话，那么就应该在这个函数中先判断被存储值类型的合法性。若不合法，则应该直接返回对应的错误值，从而避免 panic 的发生。 如果可能的话，我们可以把原子值封装到一个数据类型中，比如一个结构体类型。这样，我们既可以通过该类型的方法更加安全地存储值，又可以在该类型中包含可存储值的合法类型信息。 &nbsp;除了上述使用建议之外，我还要再特别强调一点：尽量不要向原子值中存储引用类型的值。因为这很容易造成安全漏洞。请看下面的代码：&nbsp;1234var box6 atomic.Valuev6 := []int&#123;1, 2, 3&#125;box6.Store(v6)v6[1] = 4 // 注意，此处的操作不是并发安全的！&nbsp; 我把一个[]int类型的切片值v6, 存入了原子值box6。注意，切片类型属于引用类型。所以，我在外面改动这个切片值，就等于修改了box6中存储的那个值。这相当于绕过了原子值而进行了非并发安全的操作。那么，应该怎样修补这个漏洞呢？可以这样做： &nbsp;1234567store := func(v []int) &#123; replica := make([]int, len(v)) copy(replica, v) box6.Store(replica)&#125;store(v6)v6[2] = 5 // 此处的操作是安全的。&nbsp; 我先为切片值v6创建了一个完全的副本。这个副本涉及的数据已经与原值毫不相干了。然后，我再把这个副本存入box6。如此一来，无论我再对v6的值做怎样的修改，都不会破坏box6提供的安全保护。 线程协作声明一个通道，使它的容量与我们手动启用的 goroutine 的数量相同，之后再利用这个通道，让主 goroutine 等待其他 goroutine 的运行结束。 &nbsp; 这一步更具体地说就是：让其他的 goroutine 在运行结束之前，都向这个通道发送一个元素值，并且，让主 goroutine 在最后从这个通道中接收元素值，接收的次数需要与其他的 goroutine 的数量相同。&nbsp;这就是下面的coordinateWithChan函数展示的多 goroutine 协作流程。&nbsp; 1234567891011121314func coordinateWithChan() &#123; sign := make(chan struct&#123;&#125;, 2) num := int32(0) fmt.Printf(&quot;The number: %d [with chan struct&#123;&#125;]\\n&quot;, num) max := int32(10) go addNum(&amp;num, 1, max, func() &#123; sign &lt;- struct&#123;&#125;&#123;&#125; &#125;) go addNum(&amp;num, 2, max, func() &#123; sign &lt;- struct&#123;&#125;&#123;&#125; &#125;) &lt;-sign &lt;-sign&#125; &nbsp; addNum函数会把它接受的最后一个参数值作为其中的defer函数。 &nbsp;手动启用的两个 goroutine 都会调用addNum函数，而它们传给该函数的最后一个参数值（也就是那个既无参数声明，也无结果声明的函数）都只会做一件事情，那就是向通道sign发送一个元素值。&nbsp; sync包的WaitGroup类型&nbsp;其实，在这种应用场景下，我们可以选用另外一个同步工具，即：sync包的WaitGroup类型。它比通道更加适合实现这种一对多的 goroutine 协作流程。&nbsp;sync.WaitGroup类型（以下简称WaitGroup类型）是开箱即用的，也是并发安全的。同时，与我们前面讨论的几个同步工具一样，它一旦被真正使用就不能被复制了。&nbsp;WaitGroup类型拥有三个指针方法：Add、Done和Wait。你可以想象该类型中有一个计数器，它的默认值是0。我们可以通过调用该类型值的Add方法来增加，或者减少这个计数器的值。&nbsp;一般情况下，我会用这个方法来记录需要等待的 goroutine 的数量。相对应的，这个类型的Done方法，用于对其所属值中计数器的值进行减一操作。我们可以在需要等待的 goroutine 中，通过defer语句调用它。&nbsp;而此类型的Wait方法的功能是，阻塞当前的 goroutine，直到其所属值中的计数器归零。如果在该方法被调用的时候，那个计数器的值就是0，那么它将不会做任何事情。&nbsp;你可能已经看出来了，WaitGroup类型的值（以下简称WaitGroup值）完全可以被用来替换coordinateWithChan函数中的通道sign。下面的coordinateWithWaitGroup函数就是它的改造版本。&nbsp; 12345678910func coordinateWithWaitGroup() &#123; var wg sync.WaitGroup wg.Add(2) num := int32(0) fmt.Printf(&quot;The number: %d [with sync.WaitGroup]\\n&quot;, num) max := int32(10) go addNum(&amp;num, 3, max, wg.Done) go addNum(&amp;num, 4, max, wg.Done) wg.Wait()&#125; &nbsp; 很明显，整体代码少了好几行，而且看起来也更加简洁了。这里我先声明了一个WaitGroup类型的变量wg。然后，我调用了它的Add方法并传入了2，因为我会在后面启用两个需要等待的 goroutine。 &nbsp;由于wg变量的Done方法本身就是一个既无参数声明，也无结果声明的函数，所以我在go语句中调用addNum函数的时候，可以直接把该方法作为最后一个参数值传进去。&nbsp;在coordinateWithWaitGroup函数的最后，我调用了wg的Wait方法。如此一来，该函数就可以等到那两个 goroutine 都运行结束之后，再结束执行了。&nbsp;以上就是WaitGroup类型最典型的应用场景了。不过不能止步于此，对于这个类型，我们还是有必要再深入了解一下的。我们一起看下面的问题。&nbsp;sync.WaitGroup类型值中计数器的值可以小于0吗？&nbsp;不可以。之所以说WaitGroup值中计数器的值不能小于0，是因为这样会引发一个 panic。 不适当地调用这类值的Done方法和Add方法都会如此。别忘了，我们在调用Add方法的时候是可以传入一个负数的。&nbsp;实际上，导致WaitGroup值的方法抛出 panic 的原因不只这一种。&nbsp;你需要知道，在我们声明了这样一个变量之后，应该首先根据需要等待的 goroutine，或者其他事件的数量，调用它的Add方法，以使计数器的值大于0。这是确保我们能在后面正常地使用这类值的前提。&nbsp;如果我们对它的Add方法的首次调用，与对它的Wait方法的调用是同时发起的，比如，在同时启用的两个 goroutine 中，分别调用这两个方法，那么就有可能会让这里的Add方法抛出一个 panic。&nbsp;这种情况不太容易复现，也正因为如此，我们更应该予以重视。所以，虽然WaitGroup值本身并不需要初始化，但是尽早地增加其计数器的值，还是非常有必要的。&nbsp;另外，你可能已经知道，WaitGroup值是可以被复用的，但需要保证其计数周期的完整性。这里的计数周期指的是这样一个过程：该值中的计数器值由0变为了某个正整数，而后又经过一系列的变化，最终由某个正整数又变回了0。&nbsp;也就是说，只要计数器的值始于0又归为0，就可以被视为一个计数周期。在一个此类值的生命周期中，它可以经历任意多个计数周期。但是，只有在它走完当前的计数周期之后，才能够开始下一个计数周期。&nbsp;&nbsp;因此，也可以说，如果一个此类值的Wait方法在它的某个计数周期中被调用，那么就会立即阻塞当前的 goroutine，直至这个计数周期完成。在这种情况下，该值的下一个计数周期，必须要等到这个Wait方法执行结束之后，才能够开始。&nbsp;如果在一个此类值的Wait方法被执行期间，跨越了两个计数周期，那么就会引发一个 panic。&nbsp;例如，在当前的 goroutine 因调用此类值的Wait方法，而被阻塞的时候，另一个 goroutine 调用了该值的Done方法，并使其计数器的值变为了0。&nbsp;这会唤醒当前的 goroutine，并使它试图继续执行Wait方法中其余的代码。但在这时，又有一个 goroutine 调用了它的Add方法，并让其计数器的值又从0变为了某个正整数。此时，这里的Wait方法就会立即抛出一个 panic。&nbsp;纵观上述会引发 panic 的后两种情况，我们可以总结出这样一条关于WaitGroup值的使用禁忌，即：不要把增加其计数器值的操作和调用其Wait方法的代码，放在不同的 goroutine 中执行。换句话说，要杜绝对同一个WaitGroup值的两种操作的并发执行。&nbsp;除了第一种情况外，我们通常需要反复地实验，才能够让WaitGroup值的方法抛出 panic。再次强调，虽然这不是每次都发生，但是在长期运行的程序中，这种情况发生的概率还是不小的，我们必须要重视它们。&nbsp;我们最好用“先统一Add，再并发Done，最后Wait”这种标准方式，来使用WaitGroup值。 尤其不要在调用Wait方法的同时，并发地通过调用Add方法去增加其计数器的值，因为这也有可能引发 panic。&nbsp; sync.Once类型&nbsp;sync.Once类型值的Do方法是怎么保证只执行参数函数一次的？&nbsp;与sync.WaitGroup类型一样，sync.Once类型（以下简称Once类型）也属于结构体类型，同样也是开箱即用和并发安全的。由于这个类型中包含了一个sync.Mutex类型的字段，所以，复制该类型的值也会导致功能的失效。&nbsp;Once类型的Do方法只接受一个参数，这个参数的类型必须是func()，即：无参数声明和结果声明的函数。&nbsp;该方法的功能并不是对每一种参数函数都只执行一次，而是只执行“首次被调用时传入的”那个函数，并且之后不会再执行任何参数函数。&nbsp;所以，如果你有多个只需要执行一次的函数，那么就应该为它们中的每一个都分配一个sync.Once类型的值（以下简称Once值）。&nbsp;Once类型中还有一个名叫done的uint32类型的字段。它的作用是记录其所属值的Do方法被调用的次数。不过，该字段的值只可能是0或者1。一旦Do方法的首次调用完成，它的值就会从0变为1。&nbsp;你可能会问，既然done字段的值不是0就是1，那为什么还要使用需要四个字节的uint32类型呢？&nbsp;原因很简单，因为对它的操作必须是“原子”的。Do方法在一开始就会通过调用atomic.LoadUint32函数来获取该字段的值，并且一旦发现该值为1，就会直接返回。这也初步保证了“Do方法，只会执行首次被调用时传入的函数”。&nbsp;不过，单凭这样一个判断的保证是不够的。因为，如果有两个 goroutine 都调用了同一个新的Once值的Do方法，并且几乎同时执行到了其中的这个条件判断代码，那么它们就都会因判断结果为false，而继续执行Do方法中剩余的代码。&nbsp;在这个条件判断之后，Do方法会立即锁定其所属值中的那个sync.Mutex类型的字段m。然后，它会在临界区中再次检查done字段的值，并且仅在条件满足时，才会去调用参数函数，以及用原子操作把done的值变为1。&nbsp;如果你熟悉 GoF 设计模式中的单例模式的话，那么肯定能看出来，这个Do方法的实现方式，与那个单例模式有很多相似之处。它们都会先在临界区之外，判断一次关键条件，若条件不满足则立即返回。这通常被称为 “快路径”，或者叫做“快速失败路径”。&nbsp;如果条件满足，那么到了临界区中还要再对关键条件进行一次判断，这主要是为了更加严谨。这两次条件判断常被统称为（跨临界区的）“双重检查”。&nbsp;由于进入临界区之前，肯定要锁定保护它的互斥锁m，显然会降低代码的执行速度，所以其中的第二次条件判断，以及后续的操作就被称为“慢路径”或者“常规路径”。&nbsp;别看Do方法中的代码不多，但它却应用了一个很经典的编程范式。我们在 Go 语言及其标准库中，还能看到不少这个经典范式及它衍生版本的应用案例。&nbsp;下面我再来说说这个Do方法在功能方面的两个特点。&nbsp;第一个特点，由于Do方法只会在参数函数执行结束之后把done字段的值变为1，因此，如果参数函数的执行需要很长时间或者根本就不会结束（比如执行一些守护任务），那么就有可能会导致相关 goroutine 的同时阻塞。&nbsp;例如，有多个 goroutine 并发地调用了同一个Once值的Do方法，并且传入的函数都会一直执行而不结束。那么，这些 goroutine 就都会因调用了这个Do方法而阻塞。因为，除了那个抢先执行了参数函数的 goroutine 之外，其他的 goroutine 都会被阻塞在锁定该Once值的互斥锁m的那行代码上。&nbsp;第二个特点，Do方法在参数函数执行结束后，对done字段的赋值用的是原子操作，并且，这一操作是被挂在defer语句中的。因此，不论参数函数的执行会以怎样的方式结束，done字段的值都会变为1。&nbsp;也就是说，即使这个参数函数没有执行成功（比如引发了一个 panic），我们也无法使用同一个Once值重新执行它了。所以，如果你需要为参数函数的执行设定重试机制，那么就要考虑Once值的适时替换问题。&nbsp;在很多时候，我们需要依据Do方法的这两个特点来设计与之相关的流程，以避免不必要的程序阻塞和功能缺失。&nbsp;如果我们不能在一开始就确定执行子任务的 goroutine 的数量，那么使用WaitGroup值来协调它们和分发子任务的 goroutine，就是有一定风险的。一个解决方案是：分批地启用执行子任务的 goroutine。&nbsp;我们都知道，WaitGroup值是可以被复用的，但需要保证其计数周期的完整性。尤其是涉及对其Wait方法调用的时候，它的下一个计数周期必须要等到，与当前计数周期对应的那个Wait方法调用完成之后，才能够开始。&nbsp;只要我们在严格遵循上述规则的前提下，分批地启用执行子任务的 goroutine，就肯定不会有问题。具体的实现方式有不少，其中最简单的方式就是使用for循环来作为辅助。这里的代码如下：&nbsp; 123456789101112131415func coordinateWithWaitGroup() &#123; total := 12 stride := 3 var num int32 fmt.Printf(&quot;The number: %d [with sync.WaitGroup]\\n&quot;, num) var wg sync.WaitGroup for i := 1; i &lt;= total; i = i + stride &#123; wg.Add(stride) for j := 0; j &lt; stride; j++ &#123; go addNum(&amp;num, i+j, wg.Done) &#125; wg.Wait() &#125; fmt.Println(&quot;End.&quot;)&#125; &nbsp; 经过改造后的coordinateWithWaitGroup函数，循环地使用了由变量wg代表的WaitGroup值。它运用的依然是“先统一Add，再并发Done，最后Wait”的这种模式，只不过它利用for语句，对此进行了复用。 &nbsp; context包&nbsp;怎样使用context包中的程序实体，实现一对多的 goroutine 协作流程？&nbsp; 123456789101112131415func coordinateWithContext() &#123; total := 12 var num int32 fmt.Printf(&quot;The number: %d [with context.Context]\\n&quot;, num) cxt, cancelFunc := context.WithCancel(context.Background()) for i := 1; i &lt;= total; i++ &#123; go addNum(&amp;num, i, func() &#123; if atomic.LoadInt32(&amp;num) == int32(total) &#123; cancelFunc() &#125; &#125;) &#125; &lt;-cxt.Done() fmt.Println(&quot;End.&quot;)&#125; &nbsp; 在这个函数体中，我先后调用了context.Background函数和context.WithCancel函数，并得到了一个可撤销的context.Context类型的值（由变量cxt代表），以及一个context.CancelFunc类型的撤销函数（由变量cancelFunc代表）。 &nbsp;在后面那条唯一的for语句中，我在每次迭代中都通过一条go语句，异步地调用addNum函数，调用的总次数只依据了total变量的值。&nbsp;请注意我给予addNum函数的最后一个参数值。它是一个匿名函数，其中只包含了一条if语句。这条if语句会“原子地”加载num变量的值，并判断它是否等于total变量的值。&nbsp;如果两个值相等，那么就调用cancelFunc函数。其含义是，如果所有的addNum函数都执行完毕，那么就立即通知分发子任务的 goroutine。&nbsp;这里分发子任务的 goroutine，即为执行coordinateWithContext函数的 goroutine。它在执行完for语句后，会立即调用cxt变量的Done函数，并试图针对该函数返回的通道，进行接收操作。&nbsp;由于一旦cancelFunc函数被调用，针对该通道的接收操作就会马上结束，所以，这样做就可以实现“等待所有的addNum函数都执行完毕”的功能。&nbsp;Context类型之所以受到了标准库中众多代码包的积极支持，主要是因为它是一种非常通用的同步工具。它的值不但可以被任意地扩散，而且还可以被用来传递额外的信息和信号。&nbsp;更具体地说，Context类型可以提供一类代表上下文的值。此类值是并发安全的，也就是说它可以被传播给多个 goroutine。&nbsp;由于Context类型实际上是一个接口类型，而context包中实现该接口的所有私有类型，都是基于某个数据类型的指针类型，所以，如此传播并不会影响该类型值的功能和安全。&nbsp;Context类型的值（以下简称Context值）是可以繁衍的，这意味着我们可以通过一个Context值产生出任意个子值。这些子值可以携带其父值的属性和数据，也可以响应我们通过其父值传达的信号。&nbsp;正因为如此，所有的Context值共同构成了一颗代表了上下文全貌的树形结构。这棵树的树根（或者称上下文根节点）是一个已经在context包中预定义好的Context值，它是全局唯一的。通过调用context.Background函数，我们就可以获取到它（我在coordinateWithContext函数中就是这么做的）。&nbsp;这里注意一下，这个上下文根节点仅仅是一个最基本的支点，它不提供任何额外的功能。也就是说，它既不可以被撤销（cancel），也不能携带任何数据。&nbsp;除此之外，context包中还包含了四个用于繁衍Context值的函数，即：WithCancel、WithDeadline、WithTimeout和WithValue。&nbsp;这些函数的第一个参数的类型都是context.Context，而名称都为parent。顾名思义，这个位置上的参数对应的都是它们将会产生的Context值的父值。&nbsp;WithCancel函数用于产生一个可撤销的parent的子值。在coordinateWithContext函数中，我通过调用该函数，获得了一个衍生自上下文根节点的Context值，和一个用于触发撤销信号的函数。&nbsp;而WithDeadline函数和WithTimeout函数则都可以被用来产生一个会定时撤销的parent的子值。至于WithValue函数，我们可以通过调用它，产生一个会携带额外数据的parent的子值。&nbsp;“可撤销的”在context包中代表着什么？“撤销”一个Context值又意味着什么？&nbsp;我相信很多初识context包的 Go 程序开发者，都会有这样的疑问。确实，“可撤销的”（cancelable）这个词在这里是比较抽象的，很容易让人迷惑。我这里再来解释一下。&nbsp;这需要从Context类型的声明讲起。这个接口中有两个方法与“撤销”息息相关。Done方法会返回一个元素类型为struct&#123;&#125;的接收通道。不过，这个接收通道的用途并不是传递元素值，而是让调用方去感知“撤销”当前Context值的那个信号。&nbsp;一旦当前的Context值被撤销，这里的接收通道就会被立即关闭。我们都知道，对于一个未包含任何元素值的通道来说，它的关闭会使任何针对它的接收操作立即结束。&nbsp;正因为如此，在coordinateWithContext函数中，基于调用表达式cxt.Done()的接收操作，才能够起到感知撤销信号的作用。&nbsp;除了让Context值的使用方感知到撤销信号，让它们得到“撤销”的具体原因，有时也是很有必要的。后者即是Context类型的Err方法的作用。该方法的结果是error类型的，并且其值只可能等于context.Canceled变量的值，或者context.DeadlineExceeded变量的值。&nbsp;前者用于表示手动撤销，而后者则代表：由于我们给定的过期时间已到，而导致的撤销。&nbsp;你可能已经感觉到了，对于Context值来说，“撤销”这个词如果当名词讲，指的其实就是被用来表达“撤销”状态的信号；如果当动词讲，指的就是对撤销信号的传达；而“可撤销的”指的则是具有传达这种撤销信号的能力。&nbsp;当我们通过调用context.WithCancel函数产生一个可撤销的Context值时，还会获得一个用于触发撤销信号的函数。&nbsp;通过调用这个函数，我们就可以触发针对这个Context值的撤销信号。一旦触发，撤销信号就会立即被传达给这个Context值，并由它的Done方法的结果值（一个接收通道）表达出来。&nbsp;撤销函数只负责触发信号，而对应的可撤销的Context值也只负责传达信号，它们都不会去管后边具体的“撤销”操作。实际上，我们的代码可以在感知到撤销信号之后，进行任意的操作，Context值对此并没有任何的约束。&nbsp;最后，若再深究的话，这里的“撤销”最原始的含义其实就是，终止程序针对某种请求（比如 HTTP 请求）的响应，或者取消对某种指令（比如 SQL 指令）的处理。这也是 Go 语言团队在创建context代码包，和Context类型时的初衷。&nbsp;撤销信号是如何在上下文树中传播的？&nbsp;context包中包含了四个用于繁衍Context值的函数。其中的WithCancel、WithDeadline和WithTimeout都是被用来基于给定的Context值产生可撤销的子值的。&nbsp;context包的WithCancel函数在被调用后会产生两个结果值。第一个结果值就是那个可撤销的Context值，而第二个结果值则是用于触发撤销信号的函数。&nbsp;在撤销函数被调用之后，对应的Context值会先关闭它内部的接收通道，也就是它的Done方法会返回的那个通道。&nbsp;然后，它会向它的所有子值（或者说子节点）传达撤销信号。这些子值会如法炮制，把撤销信号继续传播下去。最后，这个Context值会断开它与其父值之间的关联。&nbsp;&nbsp;我们通过调用context包的WithDeadline函数或者WithTimeout函数生成的Context值也是可撤销的。它们不但可以被手动撤销，还会依据在生成时被给定的过期时间，自动地进行定时撤销。这里定时撤销的功能是借助它们内部的计时器来实现的。&nbsp;当过期时间到达时，这两种Context值的行为与Context值被手动撤销时的行为是几乎一致的，只不过前者会在最后停止并释放掉其内部的计时器。&nbsp;最后要注意，通过调用context.WithValue函数得到的Context值是不可撤销的。撤销信号在被传播时，若遇到它们则会直接跨过，并试图将信号直接传给它们的子值。&nbsp;怎样通过Context值携带数据？怎样从中获取数据？&nbsp;既然谈到了context包的WithValue函数，我们就来说说Context值携带数据的方式。&nbsp;WithValue函数在产生新的Context值（以下简称含数据的Context值）的时候需要三个参数，即：父值、键和值。与“字典对于键的约束”类似，这里键的类型必须是可判等的。&nbsp;原因很简单，当我们从中获取数据的时候，它需要根据给定的键来查找对应的值。不过，这种Context值并不是用字典来存储键和值的，后两者只是被简单地存储在前者的相应字段中而已。&nbsp;Context类型的Value方法就是被用来获取数据的。在我们调用含数据的Context值的Value方法时，它会先判断给定的键，是否与当前值中存储的键相等，如果相等就把该值中存储的值直接返回，否则就到其父值中继续查找。&nbsp;如果其父值中仍然未存储相等的键，那么该方法就会沿着上下文根节点的方向一路查找下去。&nbsp;注意，除了含数据的Context值以外，其他几种Context值都是无法携带数据的。因此，Context值的Value方法在沿路查找的时候，会直接跨过那几种值。&nbsp;如果我们调用的Value方法的所属值本身就是不含数据的，那么实际调用的就将会是其父辈或祖辈的Value方法。这是由于这几种Context值的实际类型，都属于结构体类型，并且它们都是通过“将其父值嵌入到自身”，来表达父子关系的。&nbsp;最后，提醒一下，Context接口并没有提供改变数据的方法。因此，在通常情况下，我们只能通过在上下文树中添加含数据的Context值来存储新的数据，或者通过撤销此种值的父值丢弃掉相应的数据。如果你存储在这里的数据可以从外部改变，那么必须自行保证安全。&nbsp; &nbsp; 原文链接：https://time.geekbang.org/column/intro/112","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Go语言","slug":"Go语言","permalink":"http://wht6.github.io/tags/Go%E8%AF%AD%E8%A8%80/"}]},{"title":"树莓派安装openwrt","slug":"树莓派安装openwrt","date":"2021-06-06T01:20:00.000Z","updated":"2022-03-20T01:45:49.558Z","comments":true,"path":"posts/a37c.html","link":"","permalink":"http://wht6.github.io/posts/a37c.html","excerpt":"","text":"树莓派安装openwrt写入镜像openwrt下载 选择openwrt-19.07.6-brcm2708-bcm2709-rpi-2-ext4-factory.img.gz。 SDFormatter格式化SD卡，Win32DiskManager将镜像写入SD卡。 硬件连接硬件：树莓派3B+，SD卡，网线，读卡器，PC。 第一步：用一根网线连接树莓派和PC。（目的是使之位于同一局域网） 第二步：插入SD卡，启动树莓派。192.168.1.1访问路由管理界面。 第三步：配置路由器无线功能。 第四步：拔掉树莓派与PC之间的网线，将提供宽带的网线接入树莓派。 第五步：PC或手机通过无线连接树莓派，192.168.1.1访问路由管理界面，进一步配置网络。 具体细节配置无线的时候，默认的mode不能改，改了之后会自动变成client模式，而非我们期望的master模式。（AP LAN分为master和client，master就是接入点，client就是无线站点）这个原因可能是默认的mode的频段是树莓派自身所支持的，其他频段可能不支持。 我们配置无线的目的是使你的无线设备加入到当前局域网中，所以需要在AP和无线站点创建一条虚拟线路。现在的无线局域网都是基于802.11协议，我们需要配置的就是频段，模式，SSID，加密方式和密码。 配置好AP之后，无线设备通过SSID访问AP，并通过密码连接AP，连接成功后无线设备和树莓派就在同一个局域网了，此时AP仅仅起到交换机的作用。如果宽带是类似校园网的登陆验证方式此时可以直接使用了。 如果是拨号宽带，还需要配置路由器功能。首先配置网卡（eth0），pppoe模式，需要输入用户和密码，认证通过后运营商才会分配一个IP。除此之外，还要将有线接口和无线AP接口连接起来，然后开启局域网DHCP服务器，用于给局域网的设备分配内网地址(家用路由器NAT功能是自动开启的)。","categories":[{"name":"嵌入式","slug":"嵌入式","permalink":"http://wht6.github.io/categories/%E5%B5%8C%E5%85%A5%E5%BC%8F/"}],"tags":[{"name":"路由器","slug":"路由器","permalink":"http://wht6.github.io/tags/%E8%B7%AF%E7%94%B1%E5%99%A8/"}]},{"title":"域名系统DNS","slug":"域名系统DNS","date":"2021-06-06T00:20:00.000Z","updated":"2022-03-20T01:43:26.739Z","comments":true,"path":"posts/a6cd.html","link":"","permalink":"http://wht6.github.io/posts/a6cd.html","excerpt":"","text":"域名系统DNSDNS查询DNS（domain name system）的作用是将域名解析为IP地址。 DNS的查询方式： 而实际的查询流程往往是： 1 浏览器缓存——2 系统缓存（host文件）——3 路由器缓存——4 ISP服务器（本地DNS服务器）的DNS缓存——5 根域名服务器——6 顶级域名服务器——7 主域名服务器——保存结果到缓存中。 DNS记录DNS的域名记录的几种形式： A记录（address）正向解析。主机名于ip关联起来，通过域名找ip。 PTR记录（Pointer）反向解析，主机名于ip关联起来，通过ip找域名。 CNAME记录（canonical name）别名。允许多个域名映射到同一台服务器。 MX记录（mail exchange）指向邮件服务器。根据邮箱地址定位mail服务器。 NS记录（name sever）指定该域名是由哪个域名服务器解析的。 HOST文件系统的host文件包括常用的域名和对应的ip（其中包括你之前访问过的域名），相当于本地DNS缓存，解析域名的时候会先去查找本地host文件中是否有域名对应的ip。修改host文件可以屏蔽一些网站，比如修改域名的ip为127.0.0.1。 windows的host文件在路径C:\\Windows\\System32\\drivers\\etc中。 修改示例：127.0.0.1 www.baidu.com。保存后，命令行输入：ipconfig /flushdns刷新dns缓存，再去访问www.baidu.com就访问不到了。","categories":[{"name":"网络","slug":"网络","permalink":"http://wht6.github.io/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"DNS","slug":"DNS","permalink":"http://wht6.github.io/tags/DNS/"}]},{"title":"标准库中的sync包","slug":"标准库中的sync包","date":"2021-05-28T03:00:00.000Z","updated":"2022-04-02T08:39:47.919Z","comments":true,"path":"posts/3ed6.html","link":"","permalink":"http://wht6.github.io/posts/3ed6.html","excerpt":"","text":"竞态条件、临界区和同步工具&nbsp; “sync”的中文意思是“同步”。&nbsp;相比于 Go 语言宣扬的“用通讯的方式共享数据”，通过共享数据的方式来传递信息和协调线程运行的做法其实更加主流，毕竟大多数的现代编程语言，都是用后一种方式作为并发编程的解决方案的。&nbsp;一旦数据被多个线程共享，那么就很可能会产生争用和冲突的情况。这种情况也被称为竞态条件（race condition），这往往会破坏共享数据的一致性。&nbsp;共享数据的一致性代表着某种约定，即：多个线程对共享数据的操作总是可以达到它们各自预期的效果。&nbsp;如果这个一致性得不到保证，那么将会影响到一些线程中代码和流程的正确执行，甚至会造成某种不可预知的错误。这种错误一般都很难发现和定位，排查起来的成本也是非常高的，所以一定要尽量避免。&nbsp;举个例子，同时有多个线程连续向同一个缓冲区写入数据块，如果没有一个机制去协调这些线程的写入操作的话，那么被写入的数据块就很可能会出现错乱。比如，在线程 A 还没有写完一个数据块的时候，线程 B 就开始写入另外一个数据块了。&nbsp;显然，这两个数据块中的数据会被混在一起，并且已经很难分清了。因此，在这种情况下，我们就需要采取一些措施来协调它们对缓冲区的修改。这通常就会涉及同步。&nbsp;概括来讲，同步的用途有两个，一个是避免多个线程在同一时刻操作同一个数据块，另一个是协调多个线程，以避免它们在同一时刻执行同一个代码块。&nbsp;由于这样的数据块和代码块的背后都隐含着一种或多种资源（比如存储资源、计算资源、I/O 资源、网络资源等等），所以我们可以把它们看做是共享资源，或者说共享资源的代表。我们所说的同步其实就是在控制多个线程对共享资源的访问。&nbsp;一个线程在想要访问某一个共享资源的时候，需要先申请对该资源的访问权限，并且只有在申请成功之后，访问才能真正开始。&nbsp;而当线程对共享资源的访问结束时，它还必须归还对该资源的访问权限，若要再次访问仍需申请。&nbsp;你可以把这里所说的访问权限想象成一块令牌，线程一旦拿到了令牌，就可以进入指定的区域，从而访问到资源，而一旦线程要离开这个区域了，就需要把令牌还回去，绝不能把令牌带走。&nbsp;如果针对某个共享资源的访问令牌只有一块，那么在同一时刻，就最多只能有一个线程进入到那个区域，并访问到该资源。&nbsp;这时，我们可以说，多个并发运行的线程对这个共享资源的访问是完全串行的。只要一个代码片段需要实现对共享资源的串行化访问，就可以被视为一个临界区（critical section），也就是我刚刚说的，由于要访问到资源而必须进入的那个区域。&nbsp;比如，在我前面举的那个例子中，实现了数据块写入操作的代码就共同组成了一个临界区。如果针对同一个共享资源，这样的代码片段有多个，那么它们就可以被称为相关临界区。&nbsp;它们可以是一个内含了共享数据的结构体及其方法，也可以是操作同一块共享数据的多个函数。临界区总是需要受到保护的，否则就会产生竞态条件。施加保护的重要手段之一，就是使用实现了某种同步机制的工具，也称为同步工具。&nbsp;&nbsp;在 Go 语言中，可供我们选择的同步工具并不少。其中，最重要且最常用的同步工具当属互斥量（mutual exclusion，简称 mutex）。sync包中的Mutex就是与其对应的类型，该类型的值可以被称为互斥量或者互斥锁。&nbsp; 互斥锁Mutex和读写锁RWMutex&nbsp;一个互斥锁可以被用来保护一个临界区或者一组相关临界区。我们可以通过它来保证，在同一时刻只有一个 goroutine 处于该临界区之内。&nbsp;为了兑现这个保证，每当有 goroutine 想进入临界区时，都需要先对它进行锁定，并且，每个 goroutine 离开临界区时，都要及时地对它进行解锁。&nbsp;锁定操作可以通过调用互斥锁的Lock方法实现，而解锁操作可以调用互斥锁的Unlock方法。&nbsp;123456mu.Lock()_, err := writer.Write([]byte(data))if err != nil &#123; log.Printf(&quot;error: %s [%d]&quot;, err, id)&#125;mu.Unlock()&nbsp; 我们使用互斥锁时有哪些注意事项？ &nbsp; 不要重复锁定互斥锁； 不要忘记解锁互斥锁，必要时使用defer语句； 不要对尚未锁定或者已解锁的互斥锁解锁； 不要在多个函数之间直接传递互斥锁。 &nbsp;首先，你还是要把互斥锁看作是针对某一个临界区或某一组相关临界区的唯一访问令牌。&nbsp;虽然没有任何强制规定来限制，你用同一个互斥锁保护多个无关的临界区，但是这样做，一定会让你的程序变得很复杂，并且也会明显地增加你的心智负担。&nbsp;你要知道，对一个已经被锁定的互斥锁进行锁定，是会立即阻塞当前的 goroutine 的。这个 goroutine 所执行的流程，会一直停滞在调用该互斥锁的Lock方法的那行代码上。&nbsp;直到该互斥锁的Unlock方法被调用，并且这里的锁定操作成功完成，后续的代码（也就是临界区中的代码）才会开始执行。这也正是互斥锁能够保护临界区的原因所在。&nbsp;一旦，你把一个互斥锁同时用在了多个地方，就必然会有更多的 goroutine 争用这把锁。这不但会让你的程序变慢，还会大大增加死锁（deadlock）的可能性。&nbsp;所谓的死锁，指的就是当前程序中的主 goroutine，以及我们启用的那些 goroutine 都已经被阻塞。这些 goroutine 可以被统称为用户级的 goroutine。这就相当于整个程序都已经停滞不前了。&nbsp;Go 语言运行时系统是不允许这种情况出现的，只要它发现所有的用户级 goroutine 都处于等待状态，就会自行抛出一个带有如下信息的 panic：&nbsp;1fatal error: all goroutines are asleep - deadlock!&nbsp; 注意，这种由 Go 语言运行时系统自行抛出的 panic 都属于致命错误，都是无法被恢复的，调用recover函数对它们起不到任何作用。也就是说，一旦产生死锁，程序必然崩溃。 &nbsp;因此，我们一定要尽量避免这种情况的发生。而最简单、有效的方式就是让每一个互斥锁都只保护一个临界区或一组相关临界区。&nbsp;在这个前提之下，我们还需要注意，对于同一个 goroutine 而言，既不要重复锁定一个互斥锁，也不要忘记对它进行解锁。&nbsp;一个 goroutine 对某一个互斥锁的重复锁定，就意味着它自己锁死了自己。先不说这种做法本身就是错误的，在这种情况下，想让其他的 goroutine 来帮它解锁是非常难以保证其正确性的。&nbsp;我以前就在团队代码库中见到过这样的代码。那个作者的本意是先让一个 goroutine 自己锁死自己，然后再让一个负责调度的 goroutine 定时地解锁那个互斥锁，从而让前一个 goroutine 周期性地去做一些事情，比如每分钟检查一次服务器状态，或者每天清理一次日志。&nbsp;这个想法本身是没有什么问题的，但却选错了实现的工具。对于互斥锁这种需要精细化控制的同步工具而言，这样的任务并不适合它。&nbsp;在这种情况下，即使选用通道或者time.Ticker类型，然后自行实现功能都是可以的，程序的复杂度和我们的心智负担也会小很多，更何况还有不少已经很完备的解决方案可供选择。&nbsp;话说回来，其实我们说“不要忘记解锁互斥锁”的一个很重要的原因就是：避免重复锁定。&nbsp;因为在一个 goroutine 执行的流程中，可能会出现诸如“锁定、解锁、再锁定、再解锁”的操作，所以如果我们忘记了中间的解锁操作，那就一定会造成重复锁定。&nbsp;除此之外，忘记解锁还会使其他的 goroutine 无法进入到该互斥锁保护的临界区，这轻则会导致一些程序功能的失效，重则会造成死锁和程序崩溃。&nbsp;在很多时候，一个函数执行的流程并不是单一的，流程中间可能会有分叉，也可能会被中断。&nbsp;如果一个流程在锁定了某个互斥锁之后分叉了，或者有被中断的可能，那么就应该使用defer语句来对它进行解锁，而且这样的defer语句应该紧跟在锁定操作之后。这是最保险的一种做法。&nbsp;忘记解锁导致的问题有时候是比较隐秘的，并不会那么快就暴露出来。这也是我们需要特别关注它的原因。相比之下，解锁未锁定的互斥锁会立即引发 panic。&nbsp;并且，与死锁导致的 panic 一样，它们是无法被恢复的。因此，我们总是应该保证，对于每一个锁定操作，都要有且只有一个对应的解锁操作。&nbsp;换句话说，我们应该让它们成对出现。这也算是互斥锁的一个很重要的使用原则了。在很多时候，利用defer语句进行解锁可以更容易做到这一点。&nbsp;&nbsp;最后，可能你已经知道，Go 语言中的互斥锁是开箱即用的。换句话说，一旦我们声明了一个sync.Mutex类型的变量，就可以直接使用它了。&nbsp;不过要注意，该类型是一个结构体类型，属于值类型中的一种。把它传给一个函数、将它从函数中返回、把它赋给其他变量、让它进入某个通道都会导致它的副本的产生。&nbsp;并且，原值和它的副本，以及多个副本之间都是完全独立的，它们都是不同的互斥锁。&nbsp;如果你把一个互斥锁作为参数值传给了一个函数，那么在这个函数中对传入的锁的所有操作，都不会对存在于该函数之外的那个原锁产生任何的影响。&nbsp;所以，你在这样做之前，一定要考虑清楚，这种结果是你想要的吗？我想，在大多数情况下应该都不是。即使你真的希望，在这个函数中使用另外一个互斥锁也不要这样做，这主要是为了避免歧义。&nbsp;读写锁与互斥锁有哪些异同？&nbsp; 读写锁是读 / 写互斥锁的简称。在 Go 语言中，读写锁由sync.RWMutex类型的值代表。与sync.Mutex类型一样，这个类型也是开箱即用的。&nbsp; 顾名思义，读写锁是把对共享资源的“读操作”和“写操作”区别对待了。它可以对这两种操作施加不同程度的保护。换句话说，相比于互斥锁，读写锁可以实现更加细腻的访问控制。&nbsp; 一个读写锁中实际上包含了两个锁，即：读锁和写锁。sync.RWMutex类型中的Lock方法和Unlock方法分别用于对写锁进行锁定和解锁，而它的RLock方法和RUnlock方法则分别用于对读锁进行锁定和解锁。&nbsp; 另外，对于同一个读写锁来说有如下规则。&nbsp; 在写锁已被锁定的情况下再试图锁定写锁，会阻塞当前的 goroutine。 在写锁已被锁定的情况下试图锁定读锁，也会阻塞当前的 goroutine。 在读锁已被锁定的情况下试图锁定写锁，同样会阻塞当前的 goroutine。 在读锁已被锁定的情况下再试图锁定读锁，并不会阻塞当前的 goroutine。 &nbsp;换一个角度来说，对于某个受到读写锁保护的共享资源，多个写操作不能同时进行，写操作和读操作也不能同时进行，但多个读操作却可以同时进行。&nbsp;当然了，只有在我们正确使用读写锁的情况下，才能达到这种效果。还是那句话，我们需要让每一个锁都只保护一个临界区，或者一组相关临界区，并以此尽量减少误用的可能性。顺便说一句，我们通常把这种不能同时进行的操作称为互斥操作。&nbsp;再来看另一个方面。对写锁进行解锁，会唤醒“所有因试图锁定读锁，而被阻塞的 goroutine”，并且，这通常会使它们都成功完成对读锁的锁定。&nbsp;然而，对读锁进行解锁，只会在没有其他读锁锁定的前提下，唤醒“因试图锁定写锁，而被阻塞的 goroutine”；并且，最终只会有一个被唤醒的 goroutine 能够成功完成对写锁的锁定，其他的 goroutine 还要在原处继续等待。至于是哪一个 goroutine，那就要看谁的等待时间最长了。&nbsp;除此之外，读写锁对写操作之间的互斥，其实是通过它内含的一个互斥锁实现的。因此，也可以说，Go 语言的读写锁是互斥锁的一种扩展。&nbsp;最后，需要强调的是，与互斥锁类似，解锁“读写锁中未被锁定的写锁”，会立即引发 panic，对于其中的读锁也是如此，并且同样是不可恢复的。&nbsp;总之，读写锁与互斥锁的不同，都源于它把对共享资源的写操作和读操作区别对待了。这也使得它实现的互斥规则要更复杂一些。&nbsp;不过，正因为如此，我们可以使用它对共享资源的操作，实行更加细腻的控制。另外，由于这里的读写锁是互斥锁的一种扩展，所以在有些方面它还是沿用了互斥锁的行为模式。比如，在解锁未锁定的写锁或读锁时的表现，又比如，对写操作之间互斥的实现方式。&nbsp; 条件变量&nbsp;我们常常会把条件变量（conditional variable）这个同步工具拿来与互斥锁一起讨论。实际上，条件变量是基于互斥锁的，它必须有互斥锁的支撑才能发挥作用。&nbsp;条件变量并不是被用来保护临界区和共享资源的，它是用于协调想要访问共享资源的那些线程的。当共享资源的状态发生变化时，它可以被用来通知被互斥锁阻塞的线程。&nbsp;比如说，我们两个人在共同执行一项秘密任务，这需要在不直接联系和见面的前提下进行。我需要向一个信箱里放置情报，你需要从这个信箱中获取情报。这个信箱就相当于一个共享资源，而我们就分别是进行写操作的线程和进行读操作的线程。&nbsp;如果我在放置的时候发现信箱里还有未被取走的情报，那就不再放置，而先返回。另一方面，如果你在获取的时候发现信箱里没有情报，那也只能先回去了。这就相当于写的线程或读的线程阻塞的情况。&nbsp;虽然我们俩都有信箱的钥匙，但是同一时刻只能有一个人插入钥匙并打开信箱，这就是锁的作用了。更何况咱们俩是不能直接见面的，所以这个信箱本身就可以被视为一个临界区。&nbsp;尽管没有协调好，咱们俩仍然要想方设法的完成任务啊。所以，如果信箱里有情报，而你却迟迟未取走，那我就需要每过一段时间带着新情报去检查一次，若发现信箱空了，我就需要及时地把新情报放到里面。&nbsp;另一方面，如果信箱里一直没有情报，那你也要每过一段时间去打开看看，一旦有了情报就及时地取走。这么做是可以的，但就是太危险了，很容易被敌人发现。&nbsp;后来，我们又想了一个计策，各自雇佣了一个不起眼的小孩儿。如果早上七点有一个戴红色帽子的小孩儿从你家楼下路过，那么就意味着信箱里有了新情报。另一边，如果上午九点有一个戴蓝色帽子的小孩儿从我家楼下路过，那就说明你已经从信箱中取走了情报。&nbsp;这样一来，咱们执行任务的隐蔽性高多了，并且效率的提升非常显著。这两个戴不同颜色帽子的小孩儿就相当于条件变量，在共享资源的状态产生变化的时候，起到了通知的作用。&nbsp;当然了，我们是在用 Go 语言编写程序，而不是在执行什么秘密任务。因此，条件变量在这里的最大优势就是在效率方面的提升。当共享资源的状态不满足条件的时候，想操作它的线程再也不用循环往复地做检查了，只要等待通知就好了。&nbsp;条件变量的初始化离不开互斥锁，并且它的方法有的也是基于互斥锁的。&nbsp;条件变量提供的方法有三个：等待通知（wait）、单发通知（signal）和广播通知（broadcast）。&nbsp;我们在利用条件变量等待通知的时候，需要在它基于的那个互斥锁保护下进行。而在进行单发通知或广播通知的时候，却是恰恰相反的，也就是说，需要在对应的互斥锁解锁之后再做这两种操作。&nbsp;先来创建如下几个变量。&nbsp;1234var mailbox uint8var lock sync.RWMutexsendCond := sync.NewCond(&amp;lock)recvCond := sync.NewCond(lock.RLocker())&nbsp; 变量mailbox代表信箱，是uint8类型的。 若它的值为0则表示信箱中没有情报，而当它的值为1时则说明信箱中有情报。lock是一个类型为sync.RWMutex的变量，是一个读写锁，也可以被视为信箱上的那把锁。 &nbsp;另外，基于这把锁，我还创建了两个代表条件变量的变量，名字分别叫sendCond和recvCond。 它们都是*sync.Cond类型的，同时也都是由sync.NewCond函数来初始化的。&nbsp;与sync.Mutex类型和sync.RWMutex类型不同，sync.Cond类型并不是开箱即用的。我们只能利用sync.NewCond函数创建它的指针值。这个函数需要一个sync.Locker类型的参数值。&nbsp;条件变量是基于互斥锁的，它必须有互斥锁的支撑才能够起作用。因此，这里的参数值是不可或缺的，它会参与到条件变量的方法实现当中。&nbsp;sync.Locker其实是一个接口，在它的声明中只包含了两个方法定义，即：Lock()和Unlock()。sync.Mutex类型和sync.RWMutex类型都拥有Lock方法和Unlock方法，只不过它们都是指针方法。因此，这两个类型的指针类型才是sync.Locker接口的实现类型。&nbsp;我在为sendCond变量做初始化的时候，把基于lock变量的指针值传给了sync.NewCond函数。&nbsp;原因是，lock变量的Lock方法和Unlock方法分别用于对其中写锁的锁定和解锁，它们与sendCond变量的含义是对应的。sendCond是专门为放置情报而准备的条件变量，向信箱里放置情报，可以被视为对共享资源的写操作。&nbsp;相应的，recvCond变量代表的是专门为获取情报而准备的条件变量。 虽然获取情报也会涉及对信箱状态的改变，但是好在做这件事的人只会有你一个，而且我们也需要借此了解一下，条件变量与读写锁中的读锁的联用方式。所以，在这里，我们暂且把获取情报看做是对共享资源的读操作。&nbsp;因此，为了初始化recvCond这个条件变量，我们需要的是lock变量中的读锁，并且还需要是sync.Locker类型的。&nbsp;可是，lock变量中用于对读锁进行锁定和解锁的方法却是RLock和RUnlock，它们与sync.Locker接口中定义的方法并不匹配。&nbsp;好在sync.RWMutex类型的RLocker方法可以实现这一需求。我们只要在调用sync.NewCond函数时，传入调用表达式lock.RLocker()的结果值，就可以使该函数返回符合要求的条件变量了。&nbsp;为什么说通过lock.RLocker()得来的值就是lock变量中的读锁呢？实际上，这个值所拥有的Lock方法和Unlock方法，在其内部会分别调用lock变量的RLock方法和RUnlock方法。也就是说，前两个方法仅仅是后两个方法的代理而已。&nbsp;好了，我们现在有四个变量。一个是代表信箱的mailbox，一个是代表信箱上的锁的lock。还有两个是，代表了蓝帽子小孩儿的sendCond，以及代表了红帽子小孩儿的recvCond。&nbsp;&nbsp;我，现在是一个 goroutine（携带的go函数），想要适时地向信箱里放置情报并通知你，应该怎么做呢？&nbsp;1234567lock.Lock()for mailbox == 1 &#123; sendCond.Wait()&#125;mailbox = 1lock.Unlock()recvCond.Signal()&nbsp; 我肯定需要先调用lock变量的Lock方法。注意，这个Lock方法在这里意味的是：持有信箱上的锁，并且有打开信箱的权利，而不是锁上这个锁。 &nbsp;然后，我要检查mailbox变量的值是否等于1，也就是说，要看看信箱里是不是还存有情报。如果还有情报，那么我就回家去等蓝帽子小孩儿了。&nbsp;这就是那条for语句以及其中的调用表达式sendCond.Wait()所表示的含义了。你可能会问，为什么这里是for语句而不是if语句呢？我在后面会对此进行解释的。&nbsp;我们再往后看，如果信箱里没有情报，那么我就把新情报放进去，关上信箱、锁上锁，然后离开。用代码表达出来就是mailbox = 1和lock.Unlock()。&nbsp;离开之后我还要做一件事，那就是让红帽子小孩儿准时去你家楼下路过。也就是说，我会及时地通知你“信箱里已经有新情报了”，我们调用recvCond的Signal方法就可以实现这一步骤。&nbsp;另一方面，你现在是另一个 goroutine，想要适时地从信箱中获取情报，然后通知我。&nbsp;1234567lock.RLock()for mailbox == 0 &#123; recvCond.Wait()&#125;mailbox = 0lock.RUnlock()sendCond.Signal()&nbsp; 你跟我做的事情在流程上其实基本一致，只不过每一步操作的对象是不同的。你需要调用的是lock变量的RLock方法。因为你要进行的是读操作，并且会使用recvCond变量作为辅助。recvCond与lock变量的读锁是对应的。 &nbsp;在打开信箱后，你要关注的是信箱里是不是没有情报，也就是检查mailbox变量的值是否等于0。如果它确实等于0，那么你就需要回家去等红帽子小孩儿，也就是调用recvCond的Wait方法。这里使用的依然是for语句。&nbsp;如果信箱里有情报，那么你就应该取走情报，关上信箱、锁上锁，然后离开。对应的代码是mailbox = 0和lock.RUnlock()。之后，你还需要让蓝帽子小孩儿准时去我家楼下路过。这样我就知道信箱中的情报已经被你获取了。&nbsp;以上这些，就是对咱们俩要执行秘密任务的代码实现。其中的条件变量的用法需要你特别注意。&nbsp;再强调一下，只要条件不满足，我就会通过调用sendCond变量的Wait方法，去等待你的通知，只有在收到通知之后我才会再次检查信箱。&nbsp;另外，当我需要通知你的时候，我会调用recvCond变量的Signal方法。你使用这两个条件变量的方式正好与我相反。你可能也看出来了，利用条件变量可以实现单向的通知，而双向的通知则需要两个条件变量。这也是条件变量的基本使用规则。&nbsp; 条件变量的Wait方法&nbsp;条件变量的Wait方法做了什么？&nbsp;在了解了条件变量的使用方式之后，你可能会有这么几个疑问。&nbsp; 为什么先要锁定条件变量基于的互斥锁，才能调用它的Wait方法？ 为什么要用for语句来包裹调用其Wait方法的表达式，用if语句不行吗？ &nbsp; 你需要对这个Wait方法的内部机制有所了解才能回答上来。&nbsp; 条件变量的Wait方法主要做了四件事。&nbsp; 把调用它的 goroutine（也就是当前的 goroutine）加入到当前条件变量的通知队列中。 解锁当前的条件变量基于的那个互斥锁。 让当前的 goroutine 处于等待状态，等到通知到来时再决定是否唤醒它。此时，这个 goroutine 就会阻塞在调用这个Wait方法的那行代码上。 如果通知到来并且决定唤醒这个 goroutine，那么就在唤醒它之后重新锁定当前条件变量基于的互斥锁。自此之后，当前的 goroutine 就会继续执行后面的代码了。 &nbsp;因为条件变量的Wait方法在阻塞当前的 goroutine 之前，会解锁它基于的互斥锁，所以在调用该Wait方法之前，我们必须先锁定那个互斥锁，否则在调用这个Wait方法时，就会引发一个不可恢复的 panic。&nbsp;为什么条件变量的Wait方法要这么做呢？你可以想象一下，如果Wait方法在互斥锁已经锁定的情况下，阻塞了当前的 goroutine，那么又由谁来解锁呢？别的 goroutine 吗？&nbsp;先不说这违背了互斥锁的重要使用原则，即：成对的锁定和解锁，就算别的 goroutine 可以来解锁，那万一解锁重复了怎么办？由此引发的 panic 可是无法恢复的。&nbsp;如果当前的 goroutine 无法解锁，别的 goroutine 也都不来解锁，那么又由谁来进入临界区，并改变共享资源的状态呢？只要共享资源的状态不变，即使当前的 goroutine 因收到通知而被唤醒，也依然会再次执行这个Wait方法，并再次被阻塞。&nbsp;所以说，如果条件变量的Wait方法不先解锁互斥锁的话，那么就只会造成两种后果：不是当前的程序因 panic 而崩溃，就是相关的 goroutine 全面阻塞。&nbsp;再解释第二个疑问。很显然，if语句只会对共享资源的状态检查一次，而for语句却可以做多次检查，直到这个状态改变为止。那为什么要做多次检查呢？&nbsp;这主要是为了保险起见。如果一个 goroutine 因收到通知而被唤醒，但却发现共享资源的状态，依然不符合它的要求，那么就应该再次调用条件变量的Wait方法，并继续等待下次通知的到来。&nbsp;这种情况是很有可能发生的，具体如下面所示。&nbsp; 有多个 goroutine 在等待共享资源的同一种状态。比如，它们都在等mailbox变量的值不为0的时候再把它的值变为0，这就相当于有多个人在等着我向信箱里放置情报。虽然等待的 goroutine 有多个，但每次成功的 goroutine 却只可能有一个。别忘了，条件变量的Wait方法会在当前的 goroutine 醒来后先重新锁定那个互斥锁。在成功的 goroutine 最终解锁互斥锁之后，其他的 goroutine 会先后进入临界区，但它们会发现共享资源的状态依然不是它们想要的。这个时候，for循环就很有必要了。 共享资源可能有的状态不是两个，而是更多。比如，mailbox变量的可能值不只有0和1，还有2、3、4。这种情况下，由于状态在每次改变后的结果只可能有一个，所以，在设计合理的前提下，单一的结果一定不可能满足所有 goroutine 的条件。那些未被满足的 goroutine 显然还需要继续等待和检查。 有一种可能，共享资源的状态只有两个，并且每种状态都只有一个 goroutine 在关注，就像我们在主问题当中实现的那个例子那样。不过，即使是这样，使用for语句仍然是有必要的。原因是，在一些多 CPU 核心的计算机系统中，即使没有收到条件变量的通知，调用其Wait方法的 goroutine 也是有可能被唤醒的。这是由计算机硬件层面决定的，即使是操作系统（比如 Linux）本身提供的条件变量也会如此。 &nbsp;综上所述，在包裹条件变量的Wait方法的时候，我们总是应该使用for语句。&nbsp;条件变量的Signal方法和Broadcast方法有哪些异同？&nbsp;条件变量的Signal方法和Broadcast方法都是被用来发送通知的，不同的是，前者的通知只会唤醒一个因此而等待的 goroutine，而后者的通知却会唤醒所有为此等待的 goroutine。&nbsp;条件变量的Wait方法总会把当前的 goroutine 添加到通知队列的队尾，而它的Signal方法总会从通知队列的队首开始，查找可被唤醒的 goroutine。所以，因Signal方法的通知，而被唤醒的 goroutine 一般都是最早等待的那一个。&nbsp;这两个方法的行为决定了它们的适用场景。如果你确定只有一个 goroutine 在等待通知，或者只需唤醒任意一个 goroutine 就可以满足要求，那么使用条件变量的Signal方法就好了。&nbsp;否则，使用Broadcast方法总没错，只要你设置好各个 goroutine 所期望的共享资源状态就可以了。&nbsp;此外，再次强调一下，与Wait方法不同，条件变量的Signal方法和Broadcast方法并不需要在互斥锁的保护下执行。恰恰相反，我们最好在解锁条件变量基于的那个互斥锁之后，再去调用它的这两个方法。这更有利于程序的运行效率。&nbsp;最后，请注意，条件变量的通知具有即时性。也就是说，如果发送通知的时候没有 goroutine 为此等待，那么该通知就会被直接丢弃。在这之后才开始等待的 goroutine 只可能被后面的通知唤醒。&nbsp;&nbsp; 原文链接：https://time.geekbang.org/column/intro/112","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Go语言","slug":"Go语言","permalink":"http://wht6.github.io/tags/Go%E8%AF%AD%E8%A8%80/"}]},{"title":"测试的基本规则和流程","slug":"测试的基本规则和流程","date":"2021-05-22T02:00:00.000Z","updated":"2022-04-02T08:39:12.975Z","comments":true,"path":"posts/cd2d.html","link":"","permalink":"http://wht6.github.io/posts/cd2d.html","excerpt":"","text":"对于程序或软件的测试也分很多种，比如：单元测试、API 测试、集成测试、灰度测试，等等。在篇会主要针对单元测试进行讲解。 &nbsp; 单元测试的类别&nbsp;单元测试，它又称程序员测试。顾名思义，这就是程序员们本该做的自我检查工作之一。&nbsp;Go 语言的缔造者们从一开始就非常重视程序测试，并且为 Go 程序的开发者们提供了丰富的 API 和工具。利用这些 API 和工具，我们可以创建测试源码文件，并为命令源码文件和库源码文件中的程序实体，编写测试用例。&nbsp;在 Go 语言中，一个测试用例往往会由一个或多个测试函数来代表，不过在大多数情况下，每个测试用例仅用一个测试函数就足够了。测试函数往往用于描述和保障某个程序实体的某方面功能，比如，该功能在正常情况下会因什么样的输入，产生什么样的输出，又比如，该功能会在什么情况下报错或表现异常，等等。&nbsp;我们可以为 Go 程序编写三类测试，即：功能测试（test）、基准测试（benchmark，也称性能测试），以及示例测试（example）。&nbsp;对于前两类测试，从名称上你就应该可以猜到它们的用途。而示例测试严格来讲也是一种功能测试，只不过它更关注程序打印出来的内容。&nbsp;一般情况下，一个测试源码文件只会针对于某个命令源码文件，或库源码文件（以下简称被测源码文件）做测试，所以我们总会（并且应该）把它们放在同一个代码包内。&nbsp; 测试源码文件的主名称应该以被测源码文件的主名称为前导，并且必须以“_test”为后缀。例如，如果被测源码文件的名称为 demo52.go，那么针对它的测试源码文件的名称就应该是 demo52_test.go。 &nbsp; 每个测试源码文件都必须至少包含一个测试函数。并且，从语法上讲，每个测试源码文件中，都可以包含用来做任何一类测试的测试函数，即使把这三类测试函数都塞进去也没有问题。我通常就是这么做的，只要把控好测试函数的分组和数量就可以了。&nbsp;我们可以依据这些测试函数针对的不同程序实体，把它们分成不同的逻辑组，并且，利用注释以及帮助类的变量或函数来做分割。同时，我们还可以依据被测源码文件中程序实体的先后顺序，来安排测试源码文件中测试函数的顺序。&nbsp;此外，不仅仅对测试源码文件的名称，对于测试函数的名称和签名，Go 语言也是有明文规定的。&nbsp;Go 语言对测试函数的名称和签名都有哪些规定？&nbsp; 对于功能测试函数来说，其名称必须以Test为前缀，并且参数列表中只应有一个*testing.T类型的参数声明。 对于性能测试函数来说，其名称必须以Benchmark为前缀，并且唯一参数的类型必须是*testing.B类型的。 对于示例测试函数来说，其名称必须以Example为前缀，但对函数的参数列表没有强制规定。 &nbsp;首先需要记住一点，只有测试源码文件的名称对了，测试函数的名称和签名也对了，当我们运行go test命令的时候，其中的测试代码才有可能被运行。 &nbsp;go test命令在开始运行时，会先做一些准备工作，比如，确定内部需要用到的命令，检查我们指定的代码包或源码文件的有效性，以及判断我们给予的标记是否合法，等等。&nbsp;在准备工作顺利完成之后，go test命令就会针对每个被测代码包，依次地进行构建、执行包中符合要求的测试函数，清理临时文件，打印测试结果。这就是通常情况下的主要测试流程。&nbsp;请注意上述的“依次”二字。对于每个被测代码包，go test命令会串行地执行测试流程中的每个步骤。&nbsp;但是，为了加快测试速度，它通常会并发地对多个被测代码包进行功能测试，只不过，在最后打印测试结果的时候，它会依照我们给定的顺序逐个进行，这会让我们感觉到它是在完全串行地执行测试流程。&nbsp;另一方面，由于并发的测试会让性能测试的结果存在偏差，所以性能测试一般都是串行进行的。更具体地说，只有在所有构建步骤都做完之后，go test命令才会真正地开始进行性能测试。&nbsp;并且，下一个代码包性能测试的进行，总会等到上一个代码包性能测试的结果打印完成才会开始，而且性能测试函数的执行也都会是串行的。&nbsp;一旦清楚了 Go 程序测试的具体过程，我们的一些疑惑就自然有了答案。比如，那个名叫TestIntroduce的测试函数为什么没执行，又比如，为什么即使是简单的性能测试，执行起来也会比功能测试慢，等等。&nbsp; 测试结果&nbsp;怎样解释功能测试的测试结果？&nbsp;先来看下面的测试命令和结果：&nbsp; 12$ go test puzzlers&#x2F;article20&#x2F;q2ok puzzlers&#x2F;article20&#x2F;q2 0.008s &nbsp; 以$符号开头表明此行展现的是我输入的命令。在这里，我输入了go test puzzlers/article20/q2，这表示我想对导入路径为puzzlers/article20/q2的代码包进行测试。代码下面一行就是此次测试的简要结果。 &nbsp;这个简要结果有三块内容。最左边的ok表示此次测试成功，也就是说没有发现测试结果不如预期的情况。&nbsp;当然了，这里全由我们编写的测试代码决定，我们总是认定测试代码本身没有 Bug，并且忠诚地落实了我们的测试意图。在测试结果的中间，显示的是被测代码包的导入路径。&nbsp;而在最右边，展现的是此次对该代码包的测试所耗费的时间，这里显示的0.008s，即 8 毫秒。不过，当我们紧接着第二次运行这个命令的时候，输出的测试结果会略有不同，如下所示：&nbsp;12$ go test puzzlers&#x2F;article20&#x2F;q2ok puzzlers&#x2F;article20&#x2F;q2 (cached)&nbsp; 可以看到，结果最右边的不再是测试耗时，而是(cached)。这表明，由于测试代码与被测代码都没有任何变动，所以go test命令直接把之前缓存测试成功的结果打印出来了。 &nbsp;go 命令通常会缓存程序构建的结果，以便在将来的构建中重用。我们可以通过运行go env GOCACHE命令来查看缓存目录的路径。缓存的数据总是能够正确地反映出当时的各种源码文件、构建环境、编译器选项等等的真实情况。&nbsp;一旦有任何变动，缓存数据就会失效，go 命令就会再次真正地执行操作。所以我们并不用担心打印出的缓存数据不是实时的结果。go 命令会定期地删除最近未使用的缓存数据，但是，如果你想手动删除所有的缓存数据，运行一下go clean -cache命令就好了。&nbsp;对于测试成功的结果，go 命令也是会缓存的。运行go clean -testcache将会删除所有的测试结果缓存。不过，这样做肯定不会删除任何构建结果缓存。&nbsp; 此外，设置环境变量GODEBUG的值也可以稍稍地改变 go 命令的缓存行为。比如，设置值为gocacheverify=1将会导致 go 命令绕过任何的缓存数据，而真正地执行操作并重新生成所有结果，然后再去检查新的结果与现有的缓存数据是否一致。 &nbsp;总之，我们并不用在意缓存数据的存在，因为它们肯定不会妨碍go test命令打印正确的测试结果。 &nbsp;你可能会问，如果测试失败，命令打印的结果将会是怎样的？如果功能测试函数的那个唯一参数被命名为t，那么当我们在其中调用t.Fail方法时，虽然当前的测试函数会继续执行下去，但是结果会显示该测试失败。如下所示：&nbsp;12345$ go test puzzlers&#x2F;article20&#x2F;q2--- FAIL: TestFail (0.00s) demo53_test.go:49: Failed.FAILFAIL puzzlers&#x2F;article20&#x2F;q2 0.007s&nbsp; 我们运行的命令与之前是相同的，但是我新增了一个功能测试函数TestFail，并在其中调用了t.Fail方法。测试结果显示，对被测代码包的测试，由于TestFail函数的测试失败而宣告失败。 &nbsp;注意，对于失败测试的结果，go test命令并不会进行缓存，所以，这种情况下的每次测试都会产生全新的结果。另外，如果测试失败了，那么go test命令将会导致：失败的测试函数中的常规测试日志一并被打印出来。&nbsp;在这里的测试结果中，之所以显示了“demo53_test.go:49: Failed.”这一行，是因为我在TestFail函数中的调用表达式t.Fail()的下边编写了代码t.Log(&quot;Failed.&quot;)。&nbsp;t.Log方法以及t.Logf方法的作用，就是打印常规的测试日志，只不过当测试成功的时候，go test命令就不会打印这类日志了。如果你想在测试结果中看到所有的常规测试日志，那么可以在运行go test命令的时候加入标记-v。&nbsp;若我们想让某个测试函数在执行的过程中立即失败，则可以在该函数中调用t.FailNow方法。&nbsp;我在下面把TestFail函数中的t.Fail()改为t.FailNow()。&nbsp;与t.Fail()不同，在t.FailNow()执行之后，当前函数会立即终止执行。换句话说，该行代码之后的所有代码都会失去执行机会。在这样修改之后，我再次运行上面的命令，得到的结果如下：&nbsp;123--- FAIL: TestFail (0.00s)FAILFAIL puzzlers&#x2F;article20&#x2F;q2 0.008s&nbsp; 显然，之前显示在结果中的常规测试日志并没有出现在这里。 &nbsp;顺便说一下，如果你想在测试失败的同时打印失败测试日志，那么可以直接调用t.Error方法或者t.Errorf方法。&nbsp;前者相当于t.Log方法和t.Fail方法的连续调用，而后者也与之类似，只不过它相当于先调用了t.Logf方法。&nbsp;除此之外，还有t.Fatal方法和t.Fatalf方法，它们的作用是在打印失败错误日志之后立即终止当前测试函数的执行并宣告测试失败。更具体地说，这相当于它们在最后都调用了t.FailNow方法。&nbsp;怎样解释性能测试的测试结果？&nbsp;性能测试与功能测试的结果格式有很多相似的地方。我们在这里仅关注前者的特殊之处。请看下面的打印结果。&nbsp;1234567$ go test -bench&#x3D;. -run&#x3D;^$ puzzlers&#x2F;article20&#x2F;q3goos: darwingoarch: amd64pkg: puzzlers&#x2F;article20&#x2F;q3BenchmarkGetPrimes-8 500000 2314 ns&#x2F;opPASSok puzzlers&#x2F;article20&#x2F;q3 1.192s&nbsp; 我在运行go test命令的时候加了两个标记。第一个标记及其值为-bench=.，只有有了这个标记，命令才会进行性能测试。该标记的值.表明需要执行任意名称的性能测试函数，当然了，函数名称还是要符合 Go 程序测试的基本规则的。 &nbsp;第二个标记及其值是-run=^$，这个标记用于表明需要执行哪些功能测试函数，这同样也是以函数名称为依据的。该标记的值^$意味着：只执行名称为空的功能测试函数，换句话说，不执行任何功能测试函数。&nbsp;你可能已经看出来了，这两个标记的值都是正则表达式。实际上，它们只能以正则表达式为值。此外，如果运行go test命令的时候不加-run标记，那么就会使它执行被测代码包中的所有功能测试函数。&nbsp;再来看测试结果，重点说一下倒数第三行的内容。BenchmarkGetPrimes-8被称为单个性能测试的名称，它表示命令执行了性能测试函数BenchmarkGetPrimes，并且当时所用的最大 P 数量为8。&nbsp;最大 P 数量相当于可以同时运行 goroutine 的逻辑 CPU 的最大个数。这里的逻辑 CPU，也可以被称为 CPU 核心，但它并不等同于计算机中真正的 CPU 核心，只是 Go 语言运行时系统内部的一个概念，代表着它同时运行 goroutine 的能力。&nbsp;顺便说一句，一台计算机的 CPU 核心的个数，意味着它能在同一时刻执行多少条程序指令，代表着它并行处理程序指令的能力。&nbsp;我们可以通过调用 runtime.GOMAXPROCS函数改变最大 P 数量，也可以在运行go test命令时，加入标记-cpu来设置一个最大 P 数量的列表，以供命令在多次测试时使用。&nbsp;在性能测试名称右边的是，go test命令最后一次执行性能测试函数（即BenchmarkGetPrimes函数）的时候，被测函数（即GetPrimes函数）被执行的实际次数。这是什么意思呢？&nbsp;go test命令在执行性能测试函数的时候会给它一个正整数，若该测试函数的唯一参数的名称为b，则该正整数就由b.N代表。我们应该在测试函数中配合着编写代码，比如：&nbsp;123for i := 0; i &lt; b.N; i++ &#123; GetPrimes(1000)&#125;&nbsp; 我在一个会迭代b.N次的循环中调用了GetPrimes函数，并给予它参数值1000。go test命令会先尝试把b.N设置为1，然后执行测试函数。 &nbsp;如果测试函数的执行时间没有超过上限（此上限默认为 1 秒），那么命令就会改大b.N的值，然后再次执行测试函数，如此往复，直到这个时间大于或等于上限为止。&nbsp;当某次执行的时间大于或等于上限时，我们就说这是命令此次对该测试函数的最后一次执行。这时的b.N的值就会被包含在测试结果中，也就是上述测试结果中的500000。&nbsp;我们可以简称该值为执行次数，但要注意，它指的是被测函数的执行次数，而不是性能测试函数的执行次数。&nbsp;最后再看这个执行次数的右边，2314 ns/op表明单次执行GetPrimes函数的平均耗时为2314纳秒。这其实就是通过将最后一次执行测试函数时的执行时间，除以（被测函数的）执行次数而得出的。&nbsp; &nbsp; -cpu的功能&nbsp;go test命令的标记-cpu，它是用来设置测试执行最大 P 数量的列表的。&nbsp; P 是 processor 的缩写，每个 processor 都是一个可以承载若干个 G，且能够使这些 G 适时地与 M 进行对接并得到真正运行的中介。 正是由于 P 的存在，G 和 M 才可以呈现出多对多的关系，并能够及时、灵活地进行组合和分离。 这里的 G 就是 goroutine 的缩写，可以被理解为 Go 语言自己实现的用户级线程。M 即为 machine 的缩写，代表着系统级线程，或者说操作系统内核级别的线程。 &nbsp;Go 语言并发编程模型中的 P，正是 goroutine 的数量能够数十万计的关键所在。P 的数量意味着 Go 程序背后的运行时系统中，会有多少个用于承载可运行的 G 的队列存在。&nbsp;每一个队列都相当于一条流水线，它会源源不断地把可运行的 G 输送给空闲的 M，并使这两者对接。&nbsp;一旦对接完成，被对接的 G 就真正地运行在操作系统的内核级线程之上了。每条流水线之间虽然会有联系，但都是独立运作的。&nbsp;因此，最大 P 数量就代表着 Go 语言运行时系统同时运行 goroutine 的能力，也可以被视为其中逻辑 CPU 的最大个数。而go test命令的-cpu标记正是用于设置这个最大个数的。&nbsp;也许你已经知道，在默认情况下，最大 P 数量就等于当前计算机 CPU 核心的实际数量。&nbsp;当然了，前者也可以大于或者小于后者，如此可以在一定程度上模拟拥有不同的 CPU 核心数的计算机。&nbsp;所以，也可以说，使用-cpu标记可以模拟：被测程序在计算能力不同计算机中的表现。&nbsp;怎样设置-cpu标记的值，以及它会对测试流程产生什么样的影响？&nbsp;标记-cpu的值应该是一个正整数的列表，该列表的表现形式为：以英文半角逗号分隔的多个整数字面量，比如1,2,4。&nbsp;针对于此值中的每一个正整数，go test命令都会先设置最大 P 数量为该数，然后再执行测试函数。&nbsp;如果测试函数有多个，那么go test命令会依照此方式逐个执行。&nbsp; 以1,2,4为例，go test命令会先以1,2,4为最大 P 数量分别去执行第一个测试函数，之后再用同样的方式执行第二个测试函数，以此类推。 &nbsp;不论我们是否追加了-cpu标记，go test命令执行测试函数时流程都是相同的，只不过具体执行步骤会略有不同。&nbsp;go test命令在进行准备工作的时候会读取-cpu标记的值，并把它转换为一个以int为元素类型的切片，我们也可以称它为逻辑 CPU 切片。&nbsp;如果该命令发现我们并没有追加这个标记，那么就会让逻辑 CPU 切片只包含一个元素值，即最大 P 数量的默认值，也就是当前计算机 CPU 核心的实际数量。&nbsp;在准备执行某个测试函数的时候，无论该函数是功能测试函数，还是性能测试函数，go test命令都会迭代逻辑 CPU 切片，并且在每次迭代时，先依据当前的元素值设置最大 P 数量，然后再去执行测试函数。&nbsp;注意，对于性能测试函数来说，这里可能不只执行了一次。&nbsp;概括来讲，go test命令每一次对性能测试函数的执行，都是一个探索的过程。它会在测试函数的执行时间上限不变的前提下，尝试找到被测程序的最大执行次数。&nbsp;在这个过程中，性能测试函数可能会被执行多次。为了以后描述方便，我们把这样一个探索的过程称为：对性能测试函数的一次探索式执行，这其中包含了对该函数的若干次执行，当然，肯定也包括了对被测程序更多次的执行。&nbsp;说到多次执行测试函数，我们就不得不提及另外一个标记，即-count。-count标记是专门用于重复执行测试函数的。它的值必须大于或等于0，并且默认值为1。&nbsp;如果我们在运行go test命令的时候追加了-count 5，那么对于每一个测试函数，命令都会在预设的不同条件下（比如不同的最大 P 数量下）分别重复执行五次。&nbsp;如果我们把前文所述的-cpu标记、-count标记，以及探索式执行联合起来看，就可以用一个公式来描述单个性能测试函数，在go test命令的一次运行过程中的执行次数，即：&nbsp; 性能测试函数的执行次数 = -cpu标记的值中正整数的个数 x -count标记的值 x 探索式执行中测试函数的实际执行次数 &nbsp;对于功能测试函数来说，这个公式会更加简单一些，即：&nbsp; 功能测试函数的执行次数 = -cpu标记的值中正整数的个数 x -count标记的值 &nbsp;&nbsp;你也许遇到过这种情况，在对 Go 程序执行某种自动化测试的过程中，测试日志会显得特别多，而且好多都是重复的。&nbsp;这时，我们首先就应该想到，上面这些导致测试函数多次执行的标记和流程。我们往往需要检查这些标记的使用是否合理、日志记录是否有必要等等，从而对测试日志进行精简。&nbsp;比如，对于功能测试函数来说，我们通常没有必要重复执行它，即使是在不同的最大 P 数量下也是如此。注意，这里所说的重复执行指的是，在被测程序的输入（比如说被测函数的参数值）相同情况下的多次执行。&nbsp;有些时候，在输入完全相同的情况下，被测程序会因其他外部环境的不同，而表现出不同的行为。这时我们需要考虑的往往应该是：这个程序在设计上是否合理，而不是通过重复执行测试来检测风险。&nbsp;还有些时候，我们的程序会无法避免地依赖一些外部环境，比如数据库或者其他服务。这时，我们依然不应该让测试的反复执行成为检测手段，而应该在测试中通过仿造（mock）外部环境，来规避掉它们的不确定性。&nbsp;其实，单元测试的意思就是：对单一的功能模块进行边界清晰的测试，并且不掺杂任何对外部环境的检测。这也是“单元”二字要表达的主要含义。&nbsp;正好相反，对于性能测试函数来说，我们常常需要反复地执行，并以此试图抹平当时的计算资源调度的细微差别对被测程序性能的影响。通过-cpu标记，我们还能够模拟被测程序在计算能力不同计算机中的性能表现。&nbsp;不过要注意，这里设置的最大 P 数量，最好不要超过当前计算机 CPU 核心的实际数量。因为一旦超出计算机实际的并行处理能力，Go 程序在性能上就无法再得到显著地提升了。&nbsp;这就像一个漏斗，不论我们怎样灌水，水的漏出速度总是有限的。更何况，为了管理过多的 P，Go 语言运行时系统还会耗费额外的计算资源。&nbsp;显然，上述模拟得出的程序性能一定是不准确的。不过，这或多或少可以作为一个参考，因为，这样模拟出的性能一般都会低于程序在计算环境中的实际性能。&nbsp;-parallel标记的作用是什么？&nbsp;在运行go test命令的时候，可以追加标记-parallel，该标记的作用是：设置同一个被测代码包中的功能测试函数的最大并发执行数。该标记的默认值是测试运行时的最大 P 数量（这可以通过调用表达式runtime.GOMAXPROCS(0)获得）。&nbsp;对于功能测试，为了加快测试速度，命令通常会并发地测试多个被测代码包。&nbsp;但是，在默认情况下，对于同一个被测代码包中的多个功能测试函数，命令会串行地执行它们。除非我们在一些功能测试函数中显式地调用t.Parallel方法。&nbsp;这个时候，这些包含了t.Parallel方法调用的功能测试函数就会被go test命令并发地执行，而并发执行的最大数量正是由-parallel标记值决定的。不过要注意，同一个功能测试函数的多次执行之间一定是串行的。&nbsp;强调一下，-parallel标记对性能测试是无效的。当然了，对于性能测试来说，也是可以并发进行的，不过机制上会有所不同。&nbsp;性能测试函数中的计时器是做什么用的?&nbsp;testing包的testing.B类型有这么几个指针方法：StartTimer、StopTimer和ResetTimer。这些方法都是用于操作当前的性能测试函数专属的计时器的。&nbsp;所谓的计时器，是一个逻辑上的概念，它其实是testing.B类型中一些字段的统称。这些字段用于记录：当前测试函数在当次执行过程中耗费的时间、分配的堆内存的字节数以及分配次数。&nbsp;下面会以测试函数的执行时间为例，来说明此计时器的用法。不过，你需要知道的是，这三个方法在开始记录、停止记录或重新记录执行时间的同时，也会对堆内存分配字节数和分配次数的记录起到相同的作用。&nbsp;实际上，go test命令本身就会用到这样的计时器。当准备执行某个性能测试函数的时候，命令会重置并启动该函数专属的计时器。一旦这个函数执行完毕，命令又会立即停止这个计时器。&nbsp;如此一来，命令就能够准确地记录下（我们在前面多次提到的）测试函数执行时间了。然后，命令就会将这个时间与执行时间上限进行比较，并决定是否在改大b.N的值之后，再次执行测试函数。&nbsp;这就是对性能测试函数的探索式执行。显然，如果我们在测试函数中自行操作这个计时器，就一定会影响到这个探索式执行的结果。也就是说，这会让命令找到被测程序的最大执行次数有所不同。&nbsp;12345678910func BenchmarkGetPrimes(b *testing.B) &#123; b.StopTimer() time.Sleep(time.Millisecond * 500) // 模拟某个耗时但与被测程序关系不大的操作。 max := 10000 b.StartTimer() for i := 0; i &lt; b.N; i++ &#123; GetPrimes(max) &#125;&#125;&nbsp; 需要注意的是该函数体中的前四行代码。我先停止了当前测试函数的计时器，然后通过调用time.Sleep函数，模拟了一个比较耗时的额外操作，并且在给变量max赋值之后又启动了该计时器。 &nbsp;你可以想象一下，我们需要耗费额外的时间去确定max变量的值，虽然在后面它会被传入GetPrimes函数，但是，针对GetPrimes函数本身的性能测试并不应该包含确定参数值的过程。&nbsp;因此，我们需要把这个过程所耗费的时间，从当前测试函数的执行时间中去除掉。这样就能够避免这一过程对测试结果的不良影响了。&nbsp;每当这个测试函数执行完毕后，go test命令拿到的执行时间都只应该包含调用GetPrimes函数所耗费的那些时间。只有依据这个时间做出的后续判断，以及找到被测程序的最大执行次数才是准确的。&nbsp;在性能测试函数中，我们可以通过对b.StartTimer和b.StopTimer方法的联合运用，再去除掉任何一段代码的执行时间。&nbsp;相比之下，b.ResetTimer方法的灵活性就要差一些了，它只能用于：去除在调用它之前那些代码的执行时间。不过，无论在调用它的时候，计时器是不是正在运行，它都可以起作用。&nbsp;扩展：-benchmem标记的作用是在性能测试完成后打印内存分配统计信息。-benchtime标记的作用是设定测试函数的执行时间上限。 &nbsp;&nbsp; 原文链接：https://time.geekbang.org/column/intro/112","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Go语言","slug":"Go语言","permalink":"http://wht6.github.io/tags/Go%E8%AF%AD%E8%A8%80/"}]},{"title":"LaTex常用命令","slug":"LaTex常用命令","date":"2021-05-13T07:00:00.000Z","updated":"2022-03-20T01:46:51.554Z","comments":true,"path":"posts/b9ca.html","link":"","permalink":"http://wht6.github.io/posts/b9ca.html","excerpt":"","text":"LaTex常用命令正文命令公式命令 字符 命令 字符 命令 向量 \\vec or \\\\boldsymbol 帽 \\hat or \\widehat 分式 \\frac{}{} 上横线 \\overline 根号 \\sqrt{} 上波浪 \\widetilde 度数 ^\\circ 大写字母空心 \\mathbb 一重积分 \\int_{}^{} 数字空心 \\mathbbm 极限 \\lim_{} or \\lim\\limits 乘号 \\times 到 \\to 无穷 \\infty 求和 \\sum_{}^{}","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://wht6.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"实例分割框架：PolarMask解读","slug":"实例分割框架：PolarMask解读","date":"2021-05-13T07:00:00.000Z","updated":"2022-03-20T01:45:35.276Z","comments":true,"path":"posts/26fe.html","link":"","permalink":"http://wht6.github.io/posts/26fe.html","excerpt":"","text":"实例分割框架：PolarMask解读论文创新PolarMask将实例分割问题表述为通过实例中心分类和极坐标中的密集距离回归来预测实例轮廓。 图中显示了实例轮廓的两种表示方法，左边是笛卡尔表示法，右边是极坐标表示法。 选择极坐标系的原因： 1）极坐标原点可视为目标中心。 2）轮廓点可以由极坐标原点出发的距离和角度唯一确定。 3）角度是固定的，根据角度更容易复原出轮廓。 FCOS简介FCOS是一个box-free的目标检测框架。FCOS希望像FCN那样逐像素的预测边界框，它做法是选择真实框中的像素点为正样本，为每个正样本像素点预测一个4D的向量和一个类别标签。4D向量是像素点到边界框四条边的距离。如果一个像素点落在多个边界框中，则认为其是模糊样本。FCOS证明使用FPN可以极大的消除模糊样本带来的影响。论文发现这个方法会产生大量低质量的框，这些框所对应给像素点皆距离目标中心点较远。因此，FCOS添加了一个center-ness Branch来过滤低质量的框。 为每个属于正样本的像素点预测一个4D向量：$\\boldsymbol t=(l,t,r,b)$，这里$l,t,r,b$是像素点到预测边界框的距离。 除此之外，还需要为每个像素点预测一个类别标签，这个与FCN是一致的，即将类别放到通道维度上。COCO数据集有80类，所有预测类别的分支的输出是80维。 FCOS会使用更多的前景样本进行预测，而box based的方法会受限与锚框的数量。 FPN样本的分配方式，P3到P7每一层有一个最大回归距离，$ max(l’,t’,r’,b’) $介于$m_i$与$m_{i-1}$之间（$ l’,t’,r’,b’ $是像素点到真实边界框的距离），则会被设置为正样本。$m_i$是FPN每层的最大回归距离。 这样大部分重叠的框（即模糊样本）会被分散到不同的FPN层，如果经过FPN分配之后还有一个位置出现两个框的情况，则选择较小的框作为回归的目标。FPN权值共享来减少参数。由于回归的值必须是正值，论文使用exp(x)。虽然共享参数，但是论文给每个FPN的Level设置一个$s_i$，$exp(xs_i)$取代exp(x)，使之性能稍有提升。 此时还有一个问题是远离目标中心点的像素会产生大量低质量的框。 因为设计了一个中心度，其作用并非改善这些低质量的框，而是直接删除。 中心度目标的计算方法： centerness^*=\\sqrt{\\frac{min(l^*,r^*)}{max(l^*,r^*)}\\times \\frac{min(t^*,b^*)}{max(t^*,b^*)}}cennterness分支只有一层，与最终的分类并列。目的是希望像素学习一个正确的位置。在推断阶段，用centerness乘以类别得分来降低远离中心的预测框的得分，之后大概率会被NMS给剔除。centerness用的BCE损失，我觉得应该回归，但仔细想想分类也说的通，预测一个像素是不是中心点概率，对于正样本点其概率并不一定是1，而是centerness计算出来的，相当于一个软标签。 实现细节输入一个图像，在每个角度上预测一个正样本位置（候选实例中心）到实例轮廓的距离，得到若干预测点，组装之后形成最终的mask。网络框架基于FCOS。（看了PolarMask的框架后发现与FCOS几乎一样，似乎FCOS就是专门为PolarMask设计的） 采样方式是以中心点开始，以固定角度间隔发射n条射线。（如果角度间隔是$10^\\circ$，则一共得到36条射线，36个采样点。）因为角度已经预定义，所以只需考虑中心点与中心点到采样点的距离。 中心点为目标的质心。（论文中还对质心和框心进行了对比，结论是质心优于框心） 正样本选择：如果(x,y)落入任何实例质心的周围区域，则将其视为中心样本。正像素采样区域定义为特征图从质心到左、上、右和下的1.5倍步幅。因此，每个实例在质心附近有大约9-16个像素作为中心实例。 距离回归的两点说明：1）如果一条射线与实例的轮廓有多个相交点，则直接选择长度最大的一条作为回归目标。2）某条射线与轮廓没有相交的点，回归目标设置成最小值$10^{-6}$。论文认为这些极端情况是限制极坐标表示上限达到100%AP的主要障碍。论文解释了不应因此证明极坐标表示劣于二值图像表示。 However, it is not supposed to be seen as Polar Representation being inferior to the non-parametric Pixel-wise Representation. The evidence is two-fold. First, even the Pixel-wise Representation still has certain gap with the upper bound of 100% AP in practice, since some operation, such as down-sampling, is indispensable. Second, current performance is far away from the upper bound regardless of the Pixel-wise Representation or Polar Representation. Therefore, the research effort is suggested to better spend on improving the practical performance of models, rather than the theoretical upper bound. 直接对距离回归会出现一个问题，每个实例都有n条射线，可能会导致距离回归损失与分类损失之间的不平衡。其次，同一个实例的n条射线具有相关性，应该整体进行训练而不是看作一个独立的回归样本。 Polar Centerness:FCOS的centerness只适用于框的抑制。polar centerness的计算方式是 polarcenterness=\\sqrt{\\frac{min(\\{d_1,d_2,...d_n\\})}{max(\\{d_1,d_2,...d_n\\})}}polarness值越接近1，越靠近中心点。最后类别分支的得分会乘以polarcenterness的值来减小远离中心的预测mask的得分，从而在后边利用NMS过滤掉。 显然二值mask的IOU是便于计算的，但是结算轮廓的IOU则比较困难，所以论文给出了Polar IOU的计算方式。 在极坐标系中，Mask IOU可以这样计算： IOU=\\frac{\\int_{0}^{2\\pi}\\frac{1}{2}d_{min}^2d\\theta}{\\int_{0}^{2\\pi}\\frac{1}{2}d_{max}^2d\\theta}d是回归目标，$d’$是预测线长度，$\\theta$是角度。$d_{min}=min(d,d’)$，$d_{max}=max(d,d’)$下面是离散形式： IOU=\\lim_{N\\to \\infty}\\frac{\\sum_{i=1}^N\\frac{1}{2}d_{min}^2\\Delta \\theta_i}{\\sum_{i=1}^N\\frac{1}{2}d_{max}^2\\Delta \\theta_i}论文发现简化后的计算相较于完整的计算只有$\\pm0.1$AP的差异，使用的简化计算如下： Polar\\quad IOU=\\frac{\\sum_{i=1}^Nd_{min}}{\\sum_{i=1}^Nd_{max}} PolarMask的backbone和FPN与FCOS一样，FPN也是按回归距离分配到FPN不同的层次。 损失是类别分支与回归分支的联合损失。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://wht6.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"实例分割","slug":"实例分割","permalink":"http://wht6.github.io/tags/%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2/"}]},{"title":"实例分割框架：SOLO解读","slug":"实例分割框架：SOLO解读","date":"2021-05-07T11:00:00.000Z","updated":"2022-03-20T01:46:05.700Z","comments":true,"path":"posts/c6fd.html","link":"","permalink":"http://wht6.github.io/posts/c6fd.html","excerpt":"","text":"实例分割框架：SOLO解读SOLOSOLO思想SOLO通过引入一个“实例类别”的概念，根据实例的位置和大小为实例中的每个像素分配一个“实例类别”，将实例分割任务转换成了一个单阶端的分类任务。 这里可能会难以理解“实例类别”的概念，这个概念其实并不难理解。首先对于图像中的每个目标，都有一个语义类别，比如猫或狗等。但是如果图像上有多只猫，我们就无法通过语义类别来分开不同的猫，因为它们具有相同的语义类别——猫。但是如果我们给每只猫加上一个位置和大小的标签呢，不同的猫显然具有不同的大小和位置。论文这里就把“语义类别+位置和大小”起了个名字，叫做实例类别。 这里作者为了证明图像中的大多数目标确实具有不同的位置和大小，也进行了调研。 Take the challenging MS COCO dataset [16] for example. There are in total 36; 780 objects in the validation subset, 98:3% of object pairs have center distance greater than 30 pixels. As for the rest 1:7% of object pairs, 40:5% of them have size ratio greater than 1.5×. To conclude, in most cases two instances in an image either have different center locations or have different object sizes. SOLO框架 SOLO将图像划分成S*S的网格，每个网格代表一个中心位置类。如同语义分割框架FCN那样，SOLO将中心位置类编码到通道轴上。其中每个通道负责一个中心位置类，相关的通道需要预测中心属于此位置的实例mask（如同FCN属于猫类的通道负责预测猫的mask那样）。 如图中所示，下边的Mask Branch即将中心位置类编码到通道轴的结果，每一个网格对应一个通道，所以一共有$S^2$个通道。 而上边的Category Branch则是和语义分割FCN相似，但是并非为每个像素分类而是为每个网格进行语义分类。（论文假设每个网格只属于一个单独的实例） 实现细节1）这时你可能会问为什么没考虑尺寸信息呢，当出现相同位置且不同尺寸的两个对象怎么办呢？ 这里SOLO使用了经典的FPN（特征金字塔网络）结构，通过将不同的尺寸的对象分配给不同FPN等级（level）来预测。此时，不同尺寸的对象就被分开处理了。具体做法是计算每个ground truth的面积，将不同面积的实例分配到不同的FPN等级预测。 FPN的不同层参数共享（7层），最后一层参数不共享。 Weights for the head are shared across different levels. Grid number may varies at different pyramids. Only the last conv is not shared in this scenario. 2）论文中网格和通道的对应方式：第k个通道对应第i*S+j个网格，如此就可以一一对应了。 3）如果像FCN那样对中心位置分类，则会因为卷积的空间不变性使得分割对位置不敏感。因此，论文使用了CoorConv的方法将归一化的坐标作为两个通道加入Mask Branch（其中一个通道是x坐标，一个通道是y坐标）。 4）最后，结合每个网格生成的结果得到一些实例掩码，在用NMS选出最好的分割结果。 5）正负样本的选择，如果网格落在真实掩膜（ground truth mask）的中心范围，那么视为正样本，其他的为负样本。在代码中，作者将gt_box缩小到1/5（以质心为中心），然后找出覆盖的网格，这些网格就是正样本。接着把类别标签与mask标签放到网格对应的通道上。 1234# mass center 计算质心gt_masks_pt = torch.from_numpy(gt_masks).to(device=device)center_ws, center_hs = center_of_mass(gt_masks_pt)valid_mask_flags = gt_masks_pt.sum(dim=-1).sum(dim=-1) &gt; 0 1234567891011# 计算质心范围覆盖的网格# left, top, right, downtop_box = max(0, int(((center_h - half_h) / upsampled_size[0]) // (1. / num_grid)))down_box = min(num_grid - 1, int(((center_h + half_h) / upsampled_size[0]) // (1. / num_grid)))left_box = max(0, int(((center_w - half_w) / upsampled_size[1]) // (1. / num_grid)))right_box = min(num_grid - 1, int(((center_w + half_w) / upsampled_size[1]) // (1. / num_grid)))top = max(top_box, coord_h-1)down = min(down_box, coord_h+1)left = max(coord_w-1, left_box)right = min(right_box, coord_w+1) 1234567891011121314# 放置真实标签# catecate_label[top:(down+1), left:(right+1)] = gt_label# insseg_mask = mmcv.imrescale(seg_mask, scale=1. / output_stride)seg_mask = torch.from_numpy(seg_mask).to(device=device)for i in range(top, down+1): for j in range(left, right+1): label = int(i * num_grid + j) ins_label[label, :seg_mask.shape[0], :seg_mask.shape[1]] = seg_mask # 存储在s*s的某个通道上 ins_ind_label[label] = True # 哪个通道有实例ins_label_list.append(ins_label)cate_label_list.append(cate_label)ins_ind_label_list.append(ins_ind_label) 6）损失函数： L=L_{cate}+\\lambda L_{mask}其中$L_{cate}$是类别分支的损失（Focal loss），$L_{mask}$是mask分支的损失（Dice loss）。 7）置信度分输计算： 对mask中所有得分大于0.5的像素求和取平均得到mask的置信度分数。 将mask的置信度分数乘以类别得分得到类别的置信度分数。 可以看出，SOLO将实例分割转变成两个分类任务，设计了一个无检测、无聚类的端到端学习框架。 作者给出的改进思路是可以借鉴最新的优秀语义分割方法来改进SOLO的两个分类任务 The proposed SOLO only needs to solve two pixel-level classification tasks, thus it may be possible to borrow some of therecent advances in semantic segmentation for improving SOLO. SOLO其实也存在一些缺点，不利于检测尺寸小的目标和目标密集的区域。 SOLOv2SOLOv2创新论文认为有三个瓶颈限制了SOLO的表现： 1）低效的掩膜预测。因为掩膜分支有S*S个通道，会消耗许多资源和算力。并且S在不同的fpn的level值是不同的，每个level的最后一层并不共享参数，这些都导致了掩膜预测效率的低下。 2）掩膜精细程度差。精细的掩膜需要更高分辨率的mask处理目标的边缘信息。但是大分辨率的mask以为了更复杂的计算。 3）Mask NMS速度慢。相比于box NMS，Mask NMS会消耗更多的时间。 However, three main bottlenecks limit the performance of SOLO: a) inefficient mask representation and learning; b) not high enough resolution for finer mask predictions; c) slow mask NMS. 因此，论文提出了两个创新： 1）使用动态卷积的方法预测掩膜。 2）设计能够并行计算的Matrix NMS代替传统的NMS。 实现方案SOLO在生成掩膜的过程中，最后一层的输入特征为$F \\in {\\mathbb{R}^{H \\times W \\times E}}$，通过一层卷积层输出为$S^2$个通道。这个过程可以建模为： {M_{i,j}} = {G_{i,j}} \\circledast F其中，${G_{i,j}} \\in {\\mathbb{R}^{1 \\times 1 \\times E}}$是卷积核，${M_{i,j}} \\in {\\mathbb{R}^{H \\times W}}$是位置在（i, j）处的实例掩膜。 M是很大的，消耗许多存储与算力。而实例的位置体现在网格中是稀疏的。所以在$S^2$个卷积核中只有很小一部分发挥作用，而其他的都是冗余的。 针对上面的问题，论文提出了动态卷积的方法，一个分支预测卷积核G，一个分支预测feature map，最后feature map与学习到的卷积核做卷积生成mask M。 Mask kernel G在给定backbone与FPN的情况下，为每个FPN的等级预测动态卷积核G。将FPN生成的$H\\times W\\times C$的feature map对齐到$S\\times S\\times C$，然后经过四层卷积，最后经过一个$3\\times 3\\times D$的卷积层得到动态卷积核G。为了保持位置敏感性，在第一个卷积层加入坐标通道。同样每个FPN等级共享参数。 Mask Feature FFPN每个level经过一个$3\\times 3$卷积层，组归一化，RELU和2次的双线性上采样后将P2到P5的特征融合。融合的过程是像素级求和，然后$1\\times 1$卷积，组归一化，RELU。在FPN的P5层加入了坐标通道提升位置敏感性。 在预测阶段，对于类别分支用0.1的阈值剔除得分低的网格。在用网格对应的动态卷积核去预测Mask。（减少计算量就体现在这里） Matrix NMS这里先简单介绍传统的NMS与改进的Soft NMS。 NMS1）对目标按照得分排序 2）保留得分最高的目标 3）删除与目标最相近的其他目标（通常是根据IOU，删除与目标IOU大于某个阈值的框） 4）复1-3直到目标结合中不存在目标 NMS有个很大的缺点，容易误删。特别是两个相似而又靠近的目标，容易删除较小的目标。 Soft NMSSoft NMS剔除的改进方法是将得分乘以一个惩罚项decay来降低相似目标的得分，而非直接暴力地删除。 原来的NMS可以描述如下：将IOU大于阈值的目标的得分全部置为0。 {s_i} = \\left\\{ {\\begin{array}{*{20}{c}} {{s_i},\\quad iou(M,{b_i}) < {N_t}} \\\\ {0,\\quad iou(M,{b_i}) \\geqslant {N_t}} \\end{array}} \\right.Soft NMS的改进，线性加权的方式，decay是与iou有关的线性函数$f(iou)=1-iou$： {s_i} = \\left\\{ {\\begin{array}{*{20}{c}} {{s_i},\\quad iou(M,{b_i}) < {N_t}} \\\\ {{s_i}(1 - iou(M,{b_i})),\\quad iou(M,{b_i}) \\geqslant {N_t}} \\end{array}} \\right.还有一种高斯加权的方式，这里不做过多介绍。 总之，思路都是IOU越大，得分降低的越多。 下面介绍Matrix NMS的思路。 Matrix NMS的思路是在Soft NMS的思路上延申。 论文认为一个给定Mask的惩罚项decay与两方面有关： 1）每个与它同类的得分比它高的预测Mask对它的惩罚。这个即$f(iou)$，iou越大，$f(iou)$越小，惩罚力度越强。 2）每个与它同类的且得分比它高的Mask自身被抑制的概率。而一个mask被抑制的概率与自身相关的最大IOU呈正相关。论文将其设置为$1/f(iou_{max})$，iou越大，越可能被抑制。 直观理解就是我会被得分高的Mask抑制，但是如果这个得分高的Mask自身被抑制，它很有可能就不是最优的mask，我受它的抑制就需要适当的减弱，就乘以一个与它自身最大IOU成正相关的一个系数，即其被抑制的概率。 因为要并行处理，所以Matrix NMS的计算全部是矩阵计算。 实现细节1）最后动态卷积核的生成并没有经过激活函数。 2）损失函数与SOLO的损失函数的计算方式一样。 3）Matirx NMS矩阵计算的详细过程： 筛选top-N的mask，按照得分排序，接着组成$N \\times N$的矩阵。 计算所有的IOU。将每一类得分最高的mask的那一列iou置为0。iou为0即可认为没有相似的mask，主要目的还是保留得分最高的mask。 计算一个label_matrix。其中同一类的为1，不同类为0。（这里可能会对0产生疑问，0即不被抑制，因为不同类吗，后边会在行最小中删除掉） 计算列的最大IOU。带入公式计算decay。 选择行最小的decay为行对应mask的decay。 decay*score更新score。选出top-k为最终结果。 4）SOLOv2类别分支改为了四层卷积加最后输出，原本SOLO的深度是7外加一层输出。 5）类别分支与kernel分支还是5个level的fpn，共享参数，不同level的feature map是S*S，S即网格数。依然是[40, 36, 24, 16, 12]。 6）Mask feature分支，融合了P2到P5的特征，并没有使用P6的特征。特征融合之后，输出256个通道，相当于网格划分成16*16，然后再与学习的动态卷积核卷积，接着与真实的mask计算loss。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://wht6.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"实例分割","slug":"实例分割","permalink":"http://wht6.github.io/tags/%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2/"}]},{"title":"错误处理和程序异常","slug":"错误处理","date":"2021-05-01T02:00:00.000Z","updated":"2022-04-10T10:05:20.336Z","comments":true,"path":"posts/3d09.html","link":"","permalink":"http://wht6.github.io/posts/3d09.html","excerpt":"","text":"错误处理error类型其实是一个接口类型，也是一个 Go 语言的内建类型。在这个接口类型的声明中只包含了一个方法Error。这个方法不接受任何参数，但是会返回一个string类型的结果。 &nbsp; 它的作用是返回错误信息的字符串表示形式。我们使用error类型的方式通常是，在函数声明的结果列表的最后，声明一个该类型的结果，同时在调用这个函数之后，先判断它返回的最后一个结果值是否“不为nil”。&nbsp;如果这个值“不为nil”，那么就进入错误处理流程，否则就继续进行正常的流程。下面是一个例子。&nbsp;123456789101112131415161718192021222324252627package main import ( &quot;errors&quot; &quot;fmt&quot;) func echo(request string) (response string, err error) &#123; if request == &quot;&quot; &#123; err = errors.New(&quot;empty request&quot;) return &#125; response = fmt.Sprintf(&quot;echo: %s&quot;, request) return&#125; func main() &#123; for _, req := range []string&#123;&quot;&quot;, &quot;hello!&quot;&#125; &#123; fmt.Printf(&quot;request: %s\\n&quot;, req) resp, err := echo(req) if err != nil &#123; fmt.Printf(&quot;error: %s\\n&quot;, err) continue &#125; fmt.Printf(&quot;response: %s\\n&quot;, resp) &#125;&#125;&nbsp; 我们先看echo函数的声明。echo函数接受一个string类型的参数request，并会返回两个结果。 &nbsp;这两个结果都是有名称的，第一个结果response也是string类型的，它代表了这个函数正常执行后的结果值。第二个结果err就是error类型的，它代表了函数执行出错时的结果值，同时也包含了具体的错误信息。&nbsp;当echo函数被调用时，它会先检查参数request的值。如果该值为空字符串，那么它就会通过调用errors.New函数，为结果err赋值，然后忽略掉后边的操作并直接返回。&nbsp;此时，结果response的值也会是一个空字符串。如果request的值并不是空字符串，那么它就为结果response赋一个适当的值，然后返回，此时的结果err的值会是nil。&nbsp;再来看main函数中的代码。我在每次调用echo函数之后都会把它返回的结果值赋给变量resp和err，并且总是先检查err的值是否“不为nil”，如果是，就打印错误信息，否则就打印常规的响应信息。&nbsp;这里值得注意的地方有两个。第一，在echo函数和main函数中，我都使用到了卫述语句。我在前面讲函数用法的时候也提到过卫述语句。简单地讲，它就是被用来检查后续操作的前置条件并进行相应处理的语句。&nbsp;对于echo函数来说，它进行常规操作的前提是：传入的参数值一定要符合要求。而对于调用echo函数的程序来说，进行后续操作的前提就是echo函数的执行不能出错。&nbsp;我们在进行错误处理的时候经常会用到卫述语句，以至于有些人会吐槽说：“我的程序满屏都是卫述语句，简直是太难看了！”不过，我倒认为这有可能是程序设计上的问题。每个编程语言的理念和风格几乎都会有明显的不同，我们常常需要顺应它们的纹理去做设计，而不是用其他语言的编程思想来编写当下语言的程序。&nbsp;再来说第二个值得注意的地方。我在生成error类型值的时候用到了errors.New函数。这是一种最基本的生成错误值的方式。我们调用它的时候传入一个由字符串代表的错误信息，它会给返回给我们一个包含了这个错误信息的error类型值。该值的静态类型当然是error，而动态类型则是一个在errors包中的，包级私有的类型*errorString。&nbsp;显然，errorString类型拥有的一个指针方法实现了error接口中的Error方法。这个方法在被调用后，会原封不动地返回我们之前传入的错误信息。实际上，error类型值的Error方法就相当于其他类型值的String方法。&nbsp;我们已经知道，通过调用fmt.Printf函数，并给定占位符%s就可以打印出某个值的字符串表示形式。对于其他类型的值来说，只要我们能为这个类型编写一个String方法，就可以自定义它的字符串表示形式。而对于error类型值，它的字符串表示形式则取决于它的Error方法。&nbsp;在上述情况下，fmt.Printf函数如果发现被打印的值是一个error类型的值，那么就会去调用它的Error方法。fmt包中的这类打印函数其实都是这么做的。&nbsp;顺便提一句，当我们想通过模板化的方式生成错误信息，并得到错误值时，可以使用fmt.Errorf函数。该函数所做的其实就是先调用fmt.Sprintf函数，得到确切的错误信息；再调用errors.New函数，得到包含该错误信息的error类型值，最后返回该值。&nbsp; 判断错误值的类型&nbsp;由于error是一个接口类型，所以即使同为error类型的错误值，它们的实际类型也可能不同。怎样判断一个错误值具体代表的是哪一类错误？&nbsp; 对于类型在已知范围内的一系列错误值，一般使用类型断言表达式或类型switch语句来判断； 对于已有相应变量且类型相同的一系列错误值，一般直接使用判等操作来判断； 对于没有相应变量且类型未知的一系列错误值，只能使用其错误信息的字符串表示形式来做判断。&nbsp; 类型在已知范围内的错误值其实是最容易分辨的。就拿os包中的几个代表错误的类型os.PathError、os.LinkError、os.SyscallError和os/exec.Error来说，它们的指针类型都是error接口的实现类型，同时它们也都包含了一个名叫Err，类型为error接口类型的代表潜在错误的字段。 &nbsp;如果我们得到一个error类型值，并且知道该值的实际类型肯定是它们中的某一个，那么就可以用类型switch语句去做判断。例如：&nbsp;12345678910111213func underlyingError(err error) error &#123; switch err := err.(type) &#123; case *os.PathError: return err.Err case *os.LinkError: return err.Err case *os.SyscallError: return err.Err case *exec.Error: return err.Err &#125; return err&#125;&nbsp; 函数underlyingError的作用是：获取和返回已知的操作系统相关错误的潜在错误值。其中的类型switch语句中有若干个case子句，分别对应了上述几个错误类型。当它们被选中时，都会把函数参数err的Err字段作为结果值返回。如果它们都未被选中，那么该函数就会直接把参数值作为结果返回，即放弃获取潜在错误值。 &nbsp;只要类型不同，我们就可以如此分辨。但是在错误值类型相同的情况下，这些手段就无能为力了。在 Go 语言的标准库中也有不少以相同方式创建的同类型的错误值。&nbsp;我们还拿os包来说，其中不少的错误值都是通过调用errors.New函数来初始化的，比如：os.ErrClosed、os.ErrInvalid以及os.ErrPermission，等等。&nbsp;注意，与前面讲到的那些错误类型不同，这几个都是已经定义好的、确切的错误值。os包中的代码有时候会把它们当做潜在错误值，封装进前面那些错误类型的值中。&nbsp;如果我们在操作文件系统的时候得到了一个错误值，并且知道该值的潜在错误值肯定是上述值中的某一个，那么就可以用普通的switch语句去做判断，当然了，用if语句和判等操作符也是可以的。例如：&nbsp;123456789101112131415printError := func(i int, err error) &#123; if err == nil &#123; fmt.Println(&quot;nil error&quot;) return &#125; err = underlyingError(err) switch err &#123; case os.ErrClosed: fmt.Printf(&quot;error(closed)[%d]: %s\\n&quot;, i, err) case os.ErrInvalid: fmt.Printf(&quot;error(invalid)[%d]: %s\\n&quot;, i, err) case os.ErrPermission: fmt.Printf(&quot;error(permission)[%d]: %s\\n&quot;, i, err) &#125;&#125;&nbsp; 这个由printError变量代表的函数会接受一个error类型的参数值。该值总会代表某个文件操作相关的错误，这是我故意地以不正确的方式操作文件后得到的。 &nbsp;虽然我不知道这些错误值的类型的范围，但却知道它们或它们的潜在错误值一定是某个已经在os包中定义的值。&nbsp;所以，我先用underlyingError函数得到它们的潜在错误值，当然也可能只得到原错误值而已。然后，我用switch语句对错误值进行判等操作，三个case子句分别对应我刚刚提到的那三个已存在于os包中的错误值。如此一来，我就能分辨出具体错误了。&nbsp;对于上面这两种情况，我们都有明确的方式去解决。但是，如果我们对一个错误值可能代表的含义知之甚少，那么就只能通过它拥有的错误信息去做判断了。&nbsp;好在我们总是能通过错误值的Error方法，拿到它的错误信息。其实os包中就有做这种判断的函数，比如：os.IsExist、os.IsNotExist和os.IsPermission。&nbsp; 构建错误值体系&nbsp;怎样根据实际情况给予恰当的错误值？&nbsp;构建错误值体系的基本方式有两种，即：创建立体的错误类型体系和创建扁平的错误值列表。&nbsp;先说错误类型体系。由于在 Go 语言中实现接口是非侵入式的，所以我们可以做得很灵活。比如，在标准库的net代码包中，有一个名为Error的接口类型。它算是内建接口类型error的一个扩展接口，因为error是net.Error的嵌入接口。&nbsp;net.Error接口除了拥有error接口的Error方法之外，还有两个自己声明的方法：Timeout和Temporary。&nbsp;net包中有很多错误类型都实现了net.Error接口，比如：&nbsp; *net.OpError； *net.AddrError； net.UnknownNetworkError等等。&nbsp; 你可以把这些错误类型想象成一棵树，内建接口error就是树的根，而net.Error接口就是一个在根上延伸的第一级非叶子节点。&nbsp;同时，你也可以把这看做是一种多层分类的手段。当net包的使用者拿到一个错误值的时候，可以先判断它是否是net.Error类型的，也就是说该值是否代表了一个网络相关的错误。&nbsp;如果是，那么我们还可以再进一步判断它的类型是哪一个更具体的错误类型，这样就能知道这个网络相关的错误具体是由于操作不当引起的，还是因为网络地址问题引起的，又或是由于网络协议不正确引起的。&nbsp;当我们细看net包中的这些具体错误类型的实现时，还会发现，与os包中的一些错误类型类似，它们也都有一个名为Err、类型为error接口类型的字段，代表的也是当前错误的潜在错误。&nbsp;所以说，这些错误类型的值之间还可以有另外一种关系，即：链式关系。比如说，使用者调用net.DialTCP之类的函数时，net包中的代码可能会返回给他一个*net.OpError类型的错误值，以表示由于他的操作不当造成了一个错误。&nbsp;同时，这些代码还可能会把一个*net.AddrError或net.UnknownNetworkError类型的值赋给该错误值的Err字段，以表明导致这个错误的潜在原因。如果，此处的潜在错误值的Err字段也有非nil的值，那么将会指明更深层次的错误原因。如此一级又一级就像链条一样最终会指向问题的根源。&nbsp;把以上这些内容总结成一句话就是，用类型建立起树形结构的错误体系，用统一字段建立起可追根溯源的链式错误关联。这是 Go 语言标准库给予我们的优秀范本，非常有借鉴意义。&nbsp;不过要注意，如果你不想让包外代码改动你返回的错误值的话，一定要小写其中字段的名称首字母。你可以通过暴露某些方法让包外代码有进一步获取错误信息的权限，比如编写一个可以返回包级私有的err字段值的公开方法Err。&nbsp;相比于立体的错误类型体系，扁平的错误值列表就要简单得多了。当我们只是想预先创建一些代表已知错误的错误值时候，用这种扁平化的方式就很恰当了。错误列表其实就是若干个名称不同但类型相同的错误值集合。&nbsp;不过，由于error是接口类型，所以通过errors.New函数生成的错误值只能被赋给变量，而不能赋给常量，又由于这些代表错误的变量需要给包外代码使用，所以其访问权限只能是公开的。&nbsp;这就带来了一个问题，如果有恶意代码改变了这些公开变量的值，那么程序的功能就必然会受到影响。因为在这种情况下我们往往会通过判等操作来判断拿到的错误值具体是哪一个错误，如果这些公开变量的值被改变了，那么相应的判等操作的结果也会随之改变。&nbsp;这里有两个解决方案。第一个方案是，先私有化此类变量，也就是说，让它们的名称首字母变成小写，然后编写公开的用于获取错误值以及用于判等错误值的函数。&nbsp;比如，对于错误值os.ErrClosed，先改写它的名称，让其变成os.errClosed，然后再编写ErrClosed函数和IsErrClosed函数。&nbsp;当然了，这不是说让你去改动标准库中已有的代码，这样做的危害会很大，甚至是致命的。我只能说，对于你可控的代码，最好还是要尽量收紧访问权限。&nbsp;再来说第二个方案，此方案存在于syscall包中。该包中有一个类型叫做Errno，该类型代表了系统调用时可能发生的底层错误。这个错误类型是error接口的实现类型，同时也是对内建类型uintptr的再定义类型。&nbsp;由于uintptr可以作为常量的类型，所以syscall.Errno自然也可以。syscall包中声明有大量的Errno类型的常量，每个常量都对应一种系统调用错误。syscall包外的代码可以拿到这些代表错误的常量，但却无法改变它们。&nbsp;我们可以仿照这种声明方式来构建我们自己的错误值列表，这样就可以保证错误值的只读特性了。&nbsp; 程序异常运行时恐慌panic&nbsp;这种程序异常被叫做 panic，我把它翻译为运行时恐慌。其中的“恐慌”二字是由 panic 直译过来的，而之所以前面又加上了“运行时”三个字，是因为这种异常只会在程序运行的时候被抛出来。&nbsp;比如说，一个 Go 程序里有一个切片，它的长度是 5，也就是说该切片中的元素值的索引分别为0、1、2、3、4，但是，我在程序里却想通过索引5访问其中的元素值，显而易见，这样的访问是不正确的。&nbsp;Go 程序，确切地说是程序内嵌的 Go 语言运行时系统，会在执行到这行代码的时候抛出一个“index out of range”的 panic，用以提示你索引越界了。&nbsp;当然了，这不仅仅是个提示。当 panic 被抛出之后，如果我们没有在程序里添加任何保护措施的话，程序（或者说代表它的那个进程）就会在打印出 panic 的详细情况（以下简称 panic 详情）之后，终止运行。&nbsp;现在，就让我们来看一下这样的 panic 详情中都有什么。&nbsp; 123456panic: runtime error: index out of range goroutine 1 [running]:main.main() &#x2F;Users&#x2F;haolin&#x2F;GeekTime&#x2F;Golang_Puzzlers&#x2F;src&#x2F;puzzlers&#x2F;article19&#x2F;q0&#x2F;demo47.go:5 +0x3dexit status 2 &nbsp; 这份详情的第一行是“panic: runtime error: index out of range”。其中的“runtime error”的含义是，这是一个runtime代码包中抛出的 panic。在这个 panic 中，包含了一个runtime.Error接口类型的值。runtime.Error接口内嵌了error接口，并做了一点点扩展，runtime包中有不少它的实现类型。 &nbsp;实际上，此详情中的“panic：”右边的内容，正是这个 panic 包含的runtime.Error类型值的字符串表示形式。&nbsp;此外，panic 详情中，一般还会包含与它的引发原因有关的 goroutine 的代码执行信息。正如前述详情中的“goroutine 1 [running]”，它表示有一个 ID 为1的 goroutine 在此 panic 被引发的时候正在运行。&nbsp;注意，这里的 ID 其实并不重要，因为它只是 Go 语言运行时系统内部给予的一个 goroutine 编号，我们在程序中是无法获取和更改的。&nbsp;&nbsp;我们再看下一行，“main.main()”表明了这个 goroutine 包装的go函数就是命令源码文件中的那个main函数，也就是说这里的 goroutine 正是主 goroutine。再下面的一行，指出的就是这个 goroutine 中的哪一行代码在此 panic 被引发时正在执行。&nbsp;这包含了此行代码在其所属的源码文件中的行数，以及这个源码文件的绝对路径。这一行最后的+0x3d代表的是：此行代码相对于其所属函数的入口程序计数偏移量。不过，一般情况下它的用处并不大。&nbsp;最后，“exit status 2”表明我的这个程序是以退出状态码2结束运行的。在大多数操作系统中，只要退出状态码不是0，都意味着程序运行的非正常结束。在 Go 语言中，因 panic 导致程序结束运行的退出状态码一般都会是2。&nbsp;综上所述，我们从上边的这个 panic 详情可以看出，作为此 panic 的引发根源的代码处于 demo47.go 文件中的第 5 行，同时被包含在main包（也就是命令源码文件所在的代码包）的main函数中。&nbsp;从 panic 被引发到程序终止运行的大致过程是什么？&nbsp;我们先说一个大致的过程：某个函数中的某行代码有意或无意地引发了一个 panic。这时，初始的 panic 详情会被建立起来，并且该程序的控制权会立即从此行代码转移至调用其所属函数的那行代码上，也就是调用栈中的上一级。&nbsp;这也意味着，此行代码所属函数的执行随即终止。紧接着，控制权并不会在此有片刻的停留，它又会立即转移至再上一级的调用代码处。控制权如此一级一级地沿着调用栈的反方向传播至顶端，也就是我们编写的最外层函数那里。&nbsp;这里的最外层函数指的是go函数，对于主 goroutine 来说就是main函数。但是控制权也不会停留在那里，而是被 Go 语言运行时系统收回。&nbsp;随后，程序崩溃并终止运行，承载程序这次运行的进程也会随之死亡并消失。与此同时，在这个控制权传播的过程中，panic 详情会被逐渐地积累和完善，并会在程序终止之前被打印出来。&nbsp;panic 可能是我们在无意间（或者说一不小心）引发的，如前文所述的索引越界。这类 panic 是真正的、在我们意料之外的程序异常。不过，除此之外，我们还是可以有意地引发 panic。&nbsp;Go 语言的内建函数panic是专门用于引发 panic 的。panic函数使程序开发者可以在程序运行期间报告异常。&nbsp;注意，这与从函数返回错误值的意义是完全不同的。当我们的函数返回一个非nil的错误值时，函数的调用方有权选择不处理，并且不处理的后果往往是不致命的。&nbsp;这里的“不致命”的意思是，不至于使程序无法提供任何功能（也可以说僵死）或者直接崩溃并终止运行（也就是真死）。&nbsp;但是，当一个 panic 发生时，如果我们不施加任何保护措施，那么导致的直接后果就是程序崩溃，就像前面描述的那样，这显然是致命的。&nbsp;panic 详情会在控制权传播的过程中，被逐渐地积累和完善，并且，控制权会一级一级地沿着调用栈的反方向传播至顶端。&nbsp;因此，在针对某个 goroutine 的代码执行信息中，调用栈底端的信息会先出现，然后是上一级调用的信息，以此类推，最后才是此调用栈顶端的信息。&nbsp;比如，main函数调用了caller1函数，而caller1函数又调用了caller2函数，那么caller2函数中代码的执行信息会先出现，然后是caller1函数中代码的执行信息，最后才是main函数的信息。&nbsp; 12345678goroutine 1 [running]:main.caller2() &#x2F;Users&#x2F;haolin&#x2F;GeekTime&#x2F;Golang_Puzzlers&#x2F;src&#x2F;puzzlers&#x2F;article19&#x2F;q1&#x2F;demo48.go:22 +0x91main.caller1() &#x2F;Users&#x2F;haolin&#x2F;GeekTime&#x2F;Golang_Puzzlers&#x2F;src&#x2F;puzzlers&#x2F;article19&#x2F;q1&#x2F;demo48.go:15 +0x66main.main() &#x2F;Users&#x2F;haolin&#x2F;GeekTime&#x2F;Golang_Puzzlers&#x2F;src&#x2F;puzzlers&#x2F;article19&#x2F;q1&#x2F;demo48.go:9 +0x66exit status 2 &nbsp; &nbsp;深入地了解此过程，以及正确地解读 panic 详情应该是我们的必备技能，这在调试 Go 程序或者为 Go 程序排查错误的时候非常重要。&nbsp;如果一个 panic 是我们在无意间引发的，那么其中的值只能由 Go 语言运行时系统给定。但是，当我们使用panic函数有意地引发一个 panic 的时候，却可以自行指定其包含的值。&nbsp;怎样让 panic 包含一个值，以及应该让它包含什么样的值？&nbsp;这其实很简单，在调用panic函数时，把某个值作为参数传给该函数就可以了。由于panic函数的唯一一个参数是空接口（也就是interface&#123;&#125;）类型的，所以从语法上讲，它可以接受任何类型的值。&nbsp;但是，我们最好传入error类型的错误值，或者其他的可以被有效序列化的值。这里的“有效序列化”指的是，可以更易读地去表示形式转换。&nbsp;还记得吗？对于fmt包下的各种打印函数来说，error类型值的Error方法与其他类型值的String方法是等价的，它们的唯一结果都是string类型的。&nbsp;我们在通过占位符%s打印这些值的时候，它们的字符串表示形式分别都是这两种方法产出的。&nbsp;一旦程序异常了，我们就一定要把异常的相关信息记录下来，这通常都是记到程序日志里。&nbsp;我们在为程序排查错误的时候，首先要做的就是查看和解读程序日志；而最常用也是最方便的日志记录方式，就是记下相关值的字符串表示形式。&nbsp;所以，如果你觉得某个值有可能会被记到日志里，那么就应该为它关联String方法。如果这个值是error类型的，那么让它的Error方法返回你为它定制的字符串表示形式就可以了。&nbsp;对于此，你可能会想到fmt.Sprintf，以及fmt.Fprintf这类可以格式化并输出参数的函数。&nbsp;是的，它们本身就可以被用来输出值的某种表示形式。不过，它们在功能上，肯定远不如我们自己定义的Error方法或者String方法。因此，为不同的数据类型分别编写这两种方法总是首选。&nbsp;可是，这与传给panic函数的参数值又有什么关系呢？其实道理是相同的。至少在程序崩溃的时候，panic 包含的那个值字符串表示形式会被打印出来。&nbsp;另外，我们还可以施加某种保护措施，避免程序的崩溃。这个时候，panic 包含的值会被取出，而在取出之后，它一般都会被打印出来或者记录到日志里。&nbsp; panic恢复&nbsp;怎样施加应对panic的保护措施，从而避免程序崩溃？&nbsp;Go 语言的内建函数recover专用于恢复 panic，或者说平息运行时恐慌。recover函数无需任何参数，并且会返回一个空接口类型的值。&nbsp;如果用法正确，这个值实际上就是即将恢复的 panic 包含的值。并且，如果这个 panic 是因我们调用panic函数而引发的，那么该值同时也会是我们此次调用panic函数时，传入的参数值副本。请注意，这里强调用法的正确。我们先来看看什么是不正确的用法。&nbsp; 123456789101112131415package main import ( &quot;fmt&quot; &quot;errors&quot;) func main() &#123; fmt.Println(&quot;Enter function main.&quot;) // 引发 panic。 panic(errors.New(&quot;something wrong&quot;)) p := recover() fmt.Printf(&quot;panic: %s\\n&quot;, p) fmt.Println(&quot;Exit function main.&quot;)&#125; &nbsp; 在上面这个main函数中，我先通过调用panic函数引发了一个 panic，紧接着想通过调用recover函数恢复这个 panic。可结果呢？你一试便知，程序依然会崩溃，这个recover函数调用并不会起到任何作用，甚至都没有机会执行。 &nbsp;还记得吗？我提到过 panic 一旦发生，控制权就会讯速地沿着调用栈的反方向传播。所以，在panic函数调用之后的代码，根本就没有执行的机会。&nbsp;那如果我把调用recover函数的代码提前呢？也就是说，先调用recover函数，再调用panic函数会怎么样呢？&nbsp;这显然也是不行的，因为，如果在我们调用recover函数时未发生 panic，那么该函数就不会做任何事情，并且只会返回一个nil。&nbsp;换句话说，这样做毫无意义。那么，到底什么才是正确的recover函数用法呢？这就不得不提到defer语句了。&nbsp;顾名思义，defer语句就是被用来延迟执行代码的。延迟到什么时候呢？这要延迟到该语句所在的函数即将执行结束的那一刻，无论结束执行的原因是什么。&nbsp;这与go语句有些类似，一个defer语句总是由一个defer关键字和一个调用表达式组成。&nbsp;这里存在一些限制，有一些调用表达式是不能出现在这里的，包括：针对 Go 语言内建函数的调用表达式，以及针对unsafe包中的函数的调用表达式。&nbsp;顺便说一下，对于go语句中的调用表达式，限制也是一样的。另外，在这里被调用的函数可以是有名称的，也可以是匿名的。我们可以把这里的函数叫做defer函数或者延迟函数。注意，被延迟执行的是defer函数，而不是defer语句。&nbsp;我刚才说了，无论函数结束执行的原因是什么，其中的defer函数调用都会在它即将结束执行的那一刻执行。即使导致它执行结束的原因是一个 panic 也会是这样。正因为如此，我们需要联用defer语句和recover函数调用，才能够恢复一个已经发生的 panic。&nbsp;我们来看一下经过修正的代码。&nbsp; 1234567891011121314151617181920package main import ( &quot;fmt&quot; &quot;errors&quot;) func main() &#123; fmt.Println(&quot;Enter function main.&quot;) defer func()&#123; fmt.Println(&quot;Enter defer function.&quot;) if p := recover(); p != nil &#123; fmt.Printf(&quot;panic: %s\\n&quot;, p) &#125; fmt.Println(&quot;Exit defer function.&quot;) &#125;() // 引发 panic。 panic(errors.New(&quot;something wrong&quot;)) fmt.Println(&quot;Exit function main.&quot;)&#125; &nbsp; 在这个main函数中，我先编写了一条defer语句，并在defer函数中调用了recover函数。仅当调用的结果值不为nil时，也就是说只有 panic 确实已发生时，我才会打印一行以“panic:”为前缀的内容。 &nbsp;紧接着，我调用了panic函数，并传入了一个error类型值。这里一定要注意，我们要尽量把defer语句写在函数体的开始处，因为在引发 panic 的语句之后的所有语句，都不会有任何执行机会。&nbsp;也只有这样，defer函数中的recover函数调用才会拦截，并恢复defer语句所属的函数，及其调用的代码中发生的所有 panic。&nbsp;至此，我向你展示了两个很典型的recover函数的错误用法，以及一个基本的正确用法。&nbsp;我希望你能够记住错误用法背后的缘由，同时也希望你能真正地理解联用defer语句和recover函数调用的真谛。&nbsp;如果一个函数中有多条defer语句，那么那几个defer函数调用的执行顺序是怎样的？&nbsp;如果只用一句话回答的话，那就是：在同一个函数中，defer函数调用的执行顺序与它们分别所属的defer语句的出现顺序（更严谨地说，是执行顺序）完全相反。&nbsp;当一个函数即将结束执行时，其中的写在最下边的defer函数调用会最先执行，其次是写在它上边、与它的距离最近的那个defer函数调用，以此类推，最上边的defer函数调用会最后一个执行。&nbsp;如果函数中有一条for语句，并且这条for语句中包含了一条defer语句，那么，显然这条defer语句的执行次数，就取决于for语句的迭代次数。&nbsp;并且，同一条defer语句每被执行一次，其中的defer函数调用就会产生一次，而且，这些函数调用同样不会被立即执行。&nbsp;那么问题来了，这条for语句中产生的多个defer函数调用，会以怎样的顺序执行呢？&nbsp;为了彻底搞清楚，我们需要弄明白defer语句执行时发生的事情。&nbsp;其实也并不复杂，在defer语句每次执行的时候，Go 语言会把它携带的defer函数及其参数值另行存储到一个队列中。&nbsp;这个队列与该defer语句所属的函数是对应的，并且，它是先进后出（FILO）的，相当于一个栈。&nbsp;在需要执行某个函数中的defer函数调用的时候，Go 语言会先拿到对应的队列，然后从该队列中一个一个地取出defer函数及其参数值，并逐个执行调用。&nbsp;这正是我说“defer函数调用与其所属的defer语句的执行顺序完全相反”的原因了。&nbsp; 1234567891011package mainimport &quot;fmt&quot;func main() &#123; defer fmt.Println(&quot;first defer&quot;) for i := 0; i &lt; 3; i++ &#123; defer fmt.Printf(&quot;defer in for [%d]\\n&quot;, i) &#125; defer fmt.Println(&quot;last defer&quot;)&#125; &nbsp; 这段代码的输出结果如下： &nbsp; 12345last deferdefer in for [2]defer in for [1]defer in for [0]first defer &nbsp; 同一条defer语句每被执行一次，就会产生一个延迟执行的defer函数调用。 &nbsp;defer 函数的执行时刻是在直接包含它的那个函数即将执行完毕的时候，也可以理解为下一刻就要返回结果值（如果有的话）的时候。 &nbsp;&nbsp; 原文链接：https://time.geekbang.org/column/intro/112","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Go语言","slug":"Go语言","permalink":"http://wht6.github.io/tags/Go%E8%AF%AD%E8%A8%80/"}]},{"title":"获得因特网的存在：拼好拼图","slug":"获得因特网的存在：拼好拼图","date":"2021-04-28T16:00:00.000Z","updated":"2022-03-20T01:44:46.142Z","comments":true,"path":"posts/9iyu.html","link":"","permalink":"http://wht6.github.io/posts/9iyu.html","excerpt":"","text":"获得因特网的存在：拼好拼图 假定你刚刚创建一个有一些服务器的小型公司网络，包括一个描述你所在公司产品和服务的公共web服务器、公司雇员获得电子邮件报文的电子邮件服务器和一台DNS服务器。你自然地希望整个世界能够在你的web站点上冲浪，知道你那些令人兴奋的产品和服务。此外，你将希望公司雇员能够向遍及全球的潜在用户发送和接收电子邮件。 为了满足这些目标，你先要获得因特网连通性。为做到这一点，要与一个本地ISP签订合同并与之连接。你的公司将要有一台网关路由器，将其与本地ISP相连接。这种连接可以是通过现有的电话基础设施的DSL连接、一条到ISP的租用线，或在第1章中描述的许多其他接入方案之一。你的本地ISP也将为你提供一个IP地址范围，例如由256个地址组成的/24地址范围。一旦有了自己的物理连接和IP地址范围，你将在该地址范围内分配IP地址：一个给你的web服务器，一个给你的邮件服务器，一个给你的DNS服务器，一个给你的网关路由器，其他IP地址给你公司网络中的其他服务器和网络设备。 除了与一个ISP签约外，你还需要与一个因特网注册机构签约，以便为你的公司获得一个域名，如第二章所述。例如，如果你的公司名为Xanadu公司，你当然试图获得域名xanadu.com。你的公司还必须在DNS系统中存在。特别是，因为外部要与你的DNS服务器联系以获得服务器的IP地址，你还需要注册你的DNS服务器的IP地址。你的注册机构将你的DNS服务器（域名和对应的地址）放入.com顶级域名服务器中的一个表项中，如在第二章中所述。完成这个步骤后，知道你域名(如xanadu.com)的任何用户将能够通过DNS系统获得你DNS服务器的IP地址。 为了使人们能够发现你web服务器的IP地址，你需要在你的DNS服务器中包括一个将你的web服务器名字（www.xanadu.com)映射为其IP地址的表项。你还要有用于其他公共可用的公司服务器的类似表项，包括你的邮件服务器。如此一来，如果Alice要浏览你的web服务器，DNS系统将联系你的DNS服务器，找出你的web服务器的IP地址，并将其提交Alice。Alice则能够与你的web服务器直接创建一条TCP连接。 然而，允许来自全世界的外部者接入你的web服务器仍存在其他必要的、决定性的步骤。考虑下列情况，假设Alice知道你的web服务器的IP地址，当她向那个IP地址发送一个IP数据报（如一个TCP SYN报文段）。这个报文段将通过因特网进行路由，访问位于许多不同AS中的一系列路由器，并最终达到你的web服务器。当这些路由器中的任一个接收到该报文段，将去其转发表中的查找表项，以决定它转发该报文段的出口。因此，每台路由器需要知道你公司的/24前缀（或某些聚合项）的存在。一台路由器怎样才能知道你公司的前缀呢？如我们刚才所见，它从BGP知道了前缀！特别是，当你的公司联系一个本地ISP并分配到一个前缀（如一个地址范围）时，你的本地ISP将使用BGP来向它连接的ISP通告该前缀（或包括你的前缀的某些聚合），因而能够以你的web和邮件服务器为目的地适当地转发数据报。 ————《计算机网络：自顶向下方法》","categories":[{"name":"网络","slug":"网络","permalink":"http://wht6.github.io/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[]},{"title":"go语句与基本流程控制语句","slug":"go语句及其执行规则","date":"2021-04-23T02:00:00.000Z","updated":"2022-04-10T10:00:41.813Z","comments":true,"path":"posts/fd28.html","link":"","permalink":"http://wht6.github.io/posts/fd28.html","excerpt":"","text":"Go语句及其执行规则 Don’t communicate by sharing memory; share memory by communicating. &nbsp; 从 Go 语言编程的角度解释，这句话的意思就是：不要通过共享数据来通讯，恰恰相反，要以通讯的方式共享数据。&nbsp;我们已经知道，通道（也就是 channel）类型的值，可以被用来以通讯的方式共享数据。更具体地说，它一般被用来在不同的 goroutine 之间传递数据。那么 goroutine 到底代表着什么呢？&nbsp;简单来说，goroutine 代表着并发编程模型中的用户级线程。你可能已经知道，操作系统本身提供了进程和线程，这两种并发执行程序的工具。&nbsp; 进程与线程&nbsp;进程，描述的就是程序的执行过程，是运行着的程序的代表。换句话说，一个进程其实就是某个程序运行时的一个产物。如果说静静地躺在那里的代码就是程序的话，那么奔跑着的、正在发挥着既有功能的代码就可以被称为进程。&nbsp;我们的电脑为什么可以同时运行那么多应用程序？我们的手机为什么可以有那么多 App 同时在后台刷新？这都是因为在它们的操作系统之上有多个代表着不同应用程序或 App 的进程在同时运行。&nbsp;再来说说线程。首先，线程总是在进程之内的，它可以被视为进程中运行着的控制流（或者说代码执行的流程）。&nbsp;一个进程至少会包含一个线程。如果一个进程只包含了一个线程，那么它里面的所有代码都只会被串行地执行。每个进程的第一个线程都会随着该进程的启动而被创建，它们可以被称为其所属进程的主线程。&nbsp;相对应的，如果一个进程中包含了多个线程，那么其中的代码就可以被并发地执行。除了进程的第一个线程之外，其他的线程都是由进程中已存在的线程创建出来的。&nbsp;也就是说，主线程之外的其他线程都只能由代码显式地创建和销毁。这需要我们在编写程序的时候进行手动控制，操作系统以及进程本身并不会帮我们下达这样的指令，它们只会忠实地执行我们的指令。&nbsp;不过，在 Go 程序当中，Go 语言的运行时（runtime）系统会帮助我们自动地创建和销毁系统级的线程。这里的系统级线程指的就是我们刚刚说过的操作系统提供的线程。&nbsp;而对应的用户级线程指的是架设在系统级线程之上的，由用户（或者说我们编写的程序）完全控制的代码执行流程。用户级线程的创建、销毁、调度、状态变更以及其中的代码和数据都完全需要我们的程序自己去实现和处理。&nbsp;这带来了很多优势，比如，因为它们的创建和销毁并不用通过操作系统去做，所以速度会很快，又比如，由于不用等着操作系统去调度它们的运行，所以往往会很容易控制并且可以很灵活。&nbsp;但是，劣势也是有的，最明显也最重要的一个劣势就是复杂。如果我们只使用了系统级线程，那么我们只要指明需要新线程执行的代码片段，并且下达创建或销毁线程的指令就好了，其他的一切具体实现都会由操作系统代劳。&nbsp;但是，如果使用用户级线程，我们就不得不既是指令下达者，又是指令执行者。我们必须全权负责与用户级线程有关的所有具体实现。&nbsp;操作系统不但不会帮忙，还会要求我们的具体实现必须与它正确地对接，否则用户级线程就无法被并发地，甚至正确地运行。毕竟我们编写的所有代码最终都需要通过操作系统才能在计算机上执行。这听起来就很麻烦，不是吗？&nbsp;不过别担心，Go 语言不但有着独特的并发编程模型，以及用户级线程 goroutine，还拥有强大的用于调度 goroutine、对接系统级线程的调度器。&nbsp;这个调度器是 Go 语言运行时系统的重要组成部分，它主要负责统筹调配 Go 并发编程模型中的三个主要元素，即：G（goroutine 的缩写）、P（processor 的缩写）和 M（machine 的缩写）。&nbsp;其中的 M 指代的就是系统级线程。而 P 指的是一种可以承载若干个 G，且能够使这些 G 适时地与 M 进行对接，并得到真正运行的中介。&nbsp;从宏观上说，G 和 M 由于 P 的存在可以呈现出多对多的关系。当一个正在与某个 M 对接并运行着的 G，需要因某个事件（比如等待 I/O 或锁的解除）而暂停运行的时候，调度器总会及时地发现，并把这个 G 与那个 M 分离开，以释放计算资源供那些等待运行的 G 使用。&nbsp;而当一个 G 需要恢复运行的时候，调度器又会尽快地为它寻找空闲的计算资源（包括 M）并安排运行。另外，当 M 不够用时，调度器会帮我们向操作系统申请新的系统级线程，而当某个 M 已无用时，调度器又会负责把它及时地销毁掉。&nbsp;正因为调度器帮助我们做了很多事，所以我们的 Go 程序才总是能高效地利用操作系统和计算机资源。程序中的所有 goroutine 也都会被充分地调度，其中的代码也都会被并发地运行，即使这样的 goroutine 有数以十万计，也仍然可以如此。&nbsp;&nbsp;Go 语言实现了一套非常完善的运行时系统，保证了我们的程序在高并发的情况下依旧能够稳定、高效地运行。&nbsp; 1234567891011package main import &quot;fmt&quot; func main() &#123; for i := 0; i &lt; 10; i++ &#123; go func() &#123; fmt.Println(i) &#125;() &#125;&#125; &nbsp; 在main函数中写了一条for语句。这条for语句中的代码会迭代运行 10 次，并有一个局部变量i代表着当次迭代的序号，该序号是从0开始的。 &nbsp;在这条for语句中仅有一条go语句，这条go语句中也仅有一条语句。这条最里面的语句调用了fmt.Println函数并想要打印出变量i的值。&nbsp;这个程序很简单，三条语句逐条嵌套。我的具体问题是：这个命令源码文件被执行后会打印出什么内容？&nbsp;答案是：不会有任何内容被打印出来。&nbsp;与一个进程总会有一个主线程类似，每一个独立的 Go 程序在运行时也总会有一个主 goroutine。这个主 goroutine 会在 Go 程序的运行准备工作完成后被自动地启用，并不需要我们做任何手动的操作。&nbsp;想必你已经知道，每条go语句一般都会携带一个函数调用，这个被调用的函数常常被称为go函数。而主 goroutine 的go函数就是那个作为程序入口的main函数。&nbsp;一定要注意，go函数真正被执行的时间，总会与其所属的go语句被执行的时间不同。当程序执行到一条go语句的时候，Go 语言的运行时系统，会先试图从某个存放空闲的 G 的队列中获取一个 G（也就是 goroutine），它只有在找不到空闲 G 的情况下才会去创建一个新的 G。&nbsp;这也是为什么我总会说“启用”一个 goroutine，而不说“创建”一个 goroutine 的原因。已存在的 goroutine 总是会被优先复用。&nbsp;然而，创建 G 的成本也是非常低的。创建一个 G 并不会像新建一个进程或者一个系统级线程那样，必须通过操作系统的系统调用来完成，在 Go 语言的运行时系统内部就可以完全做到了，更何况一个 G 仅相当于为需要并发执行代码片段服务的上下文环境而已。&nbsp;在拿到了一个空闲的 G 之后，Go 语言运行时系统会用这个 G 去包装当前的那个go函数（或者说该函数中的那些代码），然后再把这个 G 追加到某个存放可运行的 G 的队列中。&nbsp;这类队列中的 G 总是会按照先入先出的顺序，很快地由运行时系统内部的调度器安排运行。虽然这会很快，但是由于上面所说的那些准备工作还是不可避免的，所以耗时还是存在的。&nbsp;因此，go函数的执行时间总是会明显滞后于它所属的go语句的执行时间。当然了，这里所说的“明显滞后”是对于计算机的 CPU 时钟和 Go 程序来说的。我们在大多数时候都不会有明显的感觉。&nbsp;在说明了原理之后，我们再来看这种原理下的表象。请记住，只要go语句本身执行完毕，Go 程序完全不会等待go函数的执行，它会立刻去执行后边的语句。这就是所谓的异步并发地执行。&nbsp;这里“后边的语句”指的一般是for语句中的下一个迭代。然而，当最后一个迭代运行的时候，这个“后边的语句”是不存在的。&nbsp;上面程序中的那条for语句会以很快的速度执行完毕。当它执行完毕时，那 10 个包装了go函数的 goroutine 往往还没有获得运行的机会。&nbsp;请注意，go函数中的那个对fmt.Println函数的调用是以for语句中的变量i作为参数的。你可以想象一下，如果当for语句执行完毕的时候，这些go函数都还没有执行，那么它们引用的变量i的值将会是什么？&nbsp;它们都会是10，对吗？那么这道题的答案会是“打印出 10 个10”，是这样吗？&nbsp;在确定最终的答案之前，你还需要知道一个与主 goroutine 有关的重要特性，即：一旦主 goroutine 中的代码（也就是main函数中的那些代码）执行完毕，当前的 Go 程序就会结束运行。&nbsp;如此一来，如果在 Go 程序结束的那一刻，还有 goroutine 未得到运行机会，那么它们就真的没有运行机会了，它们中的代码也就不会被执行了。&nbsp;我们刚才谈论过，当for语句的最后一个迭代运行的时候，其中的那条go语句即是最后一条语句。所以，在执行完这条go语句之后，主 goroutine 中的代码也就执行完了，Go 程序会立即结束运行。那么，如果这样的话，还会有任何内容被打印出来吗？&nbsp;严谨地讲，Go 语言并不会去保证这些 goroutine 会以怎样的顺序运行。由于主 goroutine 会与我们手动启用的其他 goroutine 一起接受调度，又因为调度器很可能会在 goroutine 中的代码只执行了一部分的时候暂停，以期所有的 goroutine 有更公平的运行机会。&nbsp;所以哪个 goroutine 先执行完、哪个 goroutine 后执行完往往是不可预知的，除非我们使用了某种 Go 语言提供的方式进行了人为干预。然而，在这段代码中，我们并没有进行任何人为干预。 &nbsp; 一旦主 goroutine 中的代码执行完毕，当前的 Go 程序就会结束运行，无论其他的 goroutine 是否已经在运行了。那么，怎样才能做到等其他的 goroutine 运行完毕之后，再让主 goroutine 结束运行呢？&nbsp;其实有很多办法可以做到这一点。其中，最简单粗暴的办法就是让主 goroutine“小睡”一会儿。&nbsp;123456for i := 0; i &lt; 10; i++ &#123; go func() &#123; fmt.Println(i) &#125;()&#125;time.Sleep(time.Millisecond * 500)&nbsp; 在for语句的后边，我调用了time包的Sleep函数，并把time.Millisecond * 500的结果作为参数值传给了它。time.Sleep函数的功能就是让当前的 goroutine（在这里就是主 goroutine）暂停运行一段时间，直到到达指定的恢复运行时间。 &nbsp;我们可以把一个相对的时间传给该函数，就像我在这里传入的“500 毫秒”那样。time.Sleep函数会在被调用时用当前的绝对时间，再加上相对时间计算出在未来的恢复运行时间。显然，一旦到达恢复运行时间，当前的 goroutine 就会从“睡眠”中醒来，并开始继续执行后边的代码。&nbsp;这个办法是可行的，只要“睡眠”的时间不要太短就好。不过，问题恰恰就在这里，我们让主 goroutine“睡眠”多长时间才是合适的呢？如果“睡眠”太短，则很可能不足以让其他的 goroutine 运行完毕，而若“睡眠”太长则纯属浪费时间，这个时间就太难把握了。&nbsp;你是否想到了通道呢？我们先创建一个通道，它的长度应该与我们手动启用的 goroutine 的数量一致。在每个手动启用的 goroutine 即将运行完毕的时候，我们都要向该通道发送一个值。&nbsp;注意，这些发送表达式应该被放在它们的go函数体的最后面。对应的，我们还需要在main函数的最后从通道接收元素值，接收的次数也应该与手动启用的 goroutine 的数量保持一致。&nbsp;123456789101112131415161718192021package mainimport ( &quot;fmt&quot;)func main() &#123; num := 10 sign := make(chan struct&#123;&#125;, num) for i := 0; i &lt; num; i++ &#123; go func() &#123; fmt.Println(i) sign &lt;- struct&#123;&#125;&#123;&#125; &#125;() &#125; for j := 0; j &lt; num; j++ &#123; &lt;-sign &#125;&#125;&nbsp; 其中有一个细节你需要注意。我在声明通道sign的时候是以chan struct&#123;&#125;作为其类型的。其中的类型字面量struct&#123;&#125;有些类似于空接口类型interface&#123;&#125;，它代表了既不包含任何字段也不拥有任何方法的空结构体类型。 &nbsp;注意，struct&#123;&#125;类型值的表示法只有一个，即：struct&#123;&#125;&#123;&#125;。并且，它占用的内存空间是0字节。确切地说，这个值在整个 Go 程序中永远都只会存在一份。虽然我们可以无数次地使用这个值字面量，但是用到的却都是同一个值。&nbsp;当我们仅仅把通道当作传递某种简单信号的介质的时候，用struct&#123;&#125;作为其元素类型是再好不过的了。&nbsp;有没有比使用通道更好的方法？如果你知道标准库中的代码包sync的话，那么可能会想到sync.WaitGroup类型。没错，这是一个更好的答案。&nbsp;怎样让我们启用的多个 goroutine 按照既定的顺序运行？&nbsp;怎样做到让从0到9这几个整数按照自然数的顺序打印出来？你可能会说，我不用 goroutine 不就可以了嘛。没错，这样是可以，但是如果我不考虑这样做呢。你应该怎么解决这个问题？&nbsp;首先，我们需要稍微改造一下for语句中的那个go函数，要让它接受一个int类型的参数，并在调用它的时候把变量i的值传进去。为了不改动这个go函数中的其他代码，我们可以把它的这个参数也命名为i。&nbsp;12345for i := 0; i &lt; 10; i++ &#123; go func(i int) &#123; fmt.Println(i) &#125;(i)&#125;&nbsp; 只有这样，Go 语言才能保证每个 goroutine 都可以拿到一个唯一的整数。其原因与go函数的执行时机有关。 &nbsp;我在前面已经讲过了。在go语句被执行时，我们传给go函数的参数i会先被求值，如此就得到了当次迭代的序号。之后，无论go函数会在什么时候执行，这个参数值都不会变。也就是说，go函数中调用的fmt.Println函数打印的一定会是那个当次迭代的序号。&nbsp;然后，我们在着手改造for语句中的go函数。&nbsp;12345678for i := uint32(0); i &lt; 10; i++ &#123; go func(i uint32) &#123; fn := func() &#123; fmt.Println(i) &#125; trigger(i, fn) &#125;(i)&#125;&nbsp; 我在go函数中先声明了一个匿名的函数，并把它赋给了变量fn。这个匿名函数做的事情很简单，只是调用fmt.Println函数以打印go函数的参数i的值。 &nbsp;在这之后，我调用了一个名叫trigger的函数，并把go函数的参数i和刚刚声明的变量fn作为参数传给了它。注意，for语句声明的局部变量i和go函数的参数i的类型都变了，都由int变为了uint32。至于为什么，我一会儿再说。&nbsp;再来说trigger函数。该函数接受两个参数，一个是uint32类型的参数i, 另一个是func()类型的参数fn。你应该记得，func()代表的是既无参数声明也无结果声明的函数类型。&nbsp;12345678910trigger := func(i uint32, fn func()) &#123; for &#123; if n := atomic.LoadUint32(&amp;count); n == i &#123; fn() atomic.AddUint32(&amp;count, 1) break &#125; time.Sleep(time.Nanosecond) &#125;&#125;&nbsp; trigger函数会不断地获取一个名叫count的变量的值（提前声明变量count，默认值为0），并判断该值是否与参数i的值相同。如果相同，那么就立即调用fn代表的函数，然后把count变量的值加1，最后显式地退出当前的循环。否则，我们就先让当前的 goroutine“睡眠”一个纳秒再进入下一个迭代。 &nbsp;注意，我操作变量count的时候使用的都是原子操作。这是由于trigger函数会被多个 goroutine 并发地调用，所以它用到的非本地变量count，就被多个用户级线程共用了。因此，对它的操作就产生了竞态条件（race condition），破坏了程序的并发安全性。&nbsp;所以，我们总是应该对这样的操作加以保护，在sync/atomic包中声明了很多用于原子操作的函数。另外，由于我选用的原子操作函数对被操作的数值的类型有约束，所以我才对count以及相关的变量和参数的类型进行了统一的变更（由int变为了uint32）。&nbsp;纵观count变量、trigger函数以及改造后的for语句和go函数，我要做的是，让count变量成为一个信号，它的值总是下一个可以调用打印函数的go函数的序号。&nbsp;这个序号其实就是启用 goroutine 时，那个当次迭代的序号。也正因为如此，go函数实际的执行顺序才会与go语句的执行顺序完全一致。此外，这里的trigger函数实现了一种自旋（spinning）。除非发现条件已满足，否则它会不断地进行检查。&nbsp;最后要说的是，因为我依然想让主 goroutine 最后一个运行完毕，所以还需要加一行代码。不过既然有了trigger函数，我就没有再使用通道。&nbsp;1trigger(10, func()&#123;&#125;)&nbsp; 调用trigger函数完全可以达到相同的效果。由于当所有我手动启用的 goroutine 都运行完毕之后，count的值一定会是10，所以我就把10作为了第一个参数值。又由于我并不想打印这个10，所以我把一个什么都不做的函数作为了第二个参数值。 &nbsp;总之，通过上述的改造，我使得异步发起的go函数得到了同步地（或者说按照既定顺序地）执行，你也可以动手自己试一试，感受一下。 基本流程控制语句if语句、for语句和switch语句都属于 Go 语言的基本流程控制语句。 &nbsp; 携带range子句的for语句&nbsp; 1234567numbers1 := []int&#123;1, 2, 3, 4, 5, 6&#125;for i := range numbers1 &#123; if i == 3 &#123; numbers1[i] |= i &#125;&#125;fmt.Println(numbers1) &nbsp; 先声明了一个元素类型为int的切片类型的变量numbers1，在该切片中有 6 个元素值，分别是从1到6的整数。用一条携带range子句的for语句去迭代numbers1变量中的所有元素值。 &nbsp;在这条for语句中，只有一个迭代变量i。我在每次迭代时，都会先去判断i的值是否等于3，如果结果为true，那么就让numbers1的第i个元素值与i本身做按位或的操作，再把操作结果作为numbers1的新的第i个元素值。最后我会打印出numbers1的值。&nbsp;这段代码打印的内容是[1 2 3 7 5 6]。&nbsp;当for语句被执行的时候，在range关键字右边的numbers1会先被求值。&nbsp;这个位置上的代码被称为range表达式。range表达式的结果值可以是数组、数组的指针、切片、字符串、字典或者允许接收操作的通道中的某一个，并且结果值只能有一个。&nbsp;对于不同种类的range表达式结果值，for语句的迭代变量的数量可以有所不同。&nbsp;就拿我们这里的numbers1来说，它是一个切片，那么迭代变量就可以有两个，右边的迭代变量代表当次迭代对应的某一个元素值，而左边的迭代变量则代表该元素值在切片中的索引值。&nbsp;那么，如果像本题代码中的for语句那样，只有一个迭代变量的情况意味着什么呢？这意味着，该迭代变量只会代表当次迭代对应的元素值的索引值。&nbsp;更宽泛地讲，当只有一个迭代变量的时候，数组、数组的指针、切片和字符串的元素值都是无处安放的，我们只能拿到按照从小到大顺序给出的一个个索引值。&nbsp;因此，这里的迭代变量i的值会依次是从0到5的整数。当i的值等于3的时候，与之对应的是切片中的第 4 个元素值4。对4和3进行按位或操作得到的结果是7。这就是答案中的第 4 个整数是7的原因了。&nbsp;稍稍修改一下上面的代码。&nbsp; 12345678910numbers2 := [...]int&#123;1, 2, 3, 4, 5, 6&#125;maxIndex2 := len(numbers2) - 1for i, e := range numbers2 &#123; if i == maxIndex2 &#123; numbers2[0] += e &#125; else &#123; numbers2[i+1] += e &#125;&#125;fmt.Println(numbers2) &nbsp; 注意，我把迭代的对象换成了numbers2。numbers2中的元素值同样是从1到6的 6 个整数，并且元素类型同样是int，但它是一个数组而不是一个切片。 &nbsp;在for语句中，我总是会对紧挨在当次迭代对应的元素后边的那个元素，进行重新赋值，新的值会是这两个元素的值之和。当迭代到最后一个元素时，我会把此range表达式结果值中的第一个元素值，替换为它的原值与最后一个元素值的和，最后，我会打印出numbers2的值。&nbsp;这段代码打印的内容是[7 3 5 7 9 11]。&nbsp;当for语句被执行的时候，在range关键字右边的numbers2会先被求值。&nbsp;这里需要注意两点：&nbsp; range表达式只会在for语句开始执行时被求值一次，无论后边会有多少次迭代； range表达式的求值结果会被复制，也就是说，被迭代的对象是range表达式结果值的副本而不是原值。 &nbsp;基于这两个规则，我们接着往下看。在第一次迭代时，我改变的是numbers2的第二个元素的值，新值为3，也就是1和2之和。 &nbsp;但是，被迭代的对象的第二个元素却没有任何改变，毕竟它与numbers2已经是毫不相关的两个数组了。因此，在第二次迭代时，会把numbers2的第三个元素的值修改为5，即被迭代对象的第二个元素值2和第三个元素值3的和。 以此类推，之后的numbers2的元素值依次会是7、9和11。当迭代到最后一个元素时，我会把numbers2的第一个元素的值修改为1和6之和。&nbsp;把numbers2的值由一个数组改成一个切片，其中的元素值都不要变。打印的结果又会是什么？&nbsp;打印的结果是[22 3 6 10 15 21]。原因是切片与数组是不同的，前者是引用类型的，而后者是值类型的。（我的理解：引用类型可以看做指针，对于切片，其指向的是底层数组，因为range表达式的求值结果会被复制，迭代对象是结果值的副本，所以指针也会被复制，但是指针的副本仍然是指向底层数组的，所以对切片副本的修改，会影响到底层数组）&nbsp; switch表达式和case表达式&nbsp; 123456789value1 := [...]int8&#123;0, 1, 2, 3, 4, 5, 6&#125;switch 1 + 3 &#123;case value1[0], value1[1]: fmt.Println(&quot;0 or 1&quot;)case value1[2], value1[3]: fmt.Println(&quot;2 or 3&quot;)case value1[4], value1[5], value1[6]: fmt.Println(&quot;4 or 5 or 6&quot;)&#125; &nbsp; 先声明了一个数组类型的变量value1，该变量的元素类型是int8。在后边的switch语句中，被夹在switch关键字和左花括号&#123;之间的是1 + 3，这个位置上的代码被称为switch表达式。这个switch语句还包含了三个case子句，而每个case子句又各包含了一个case表达式和一条打印语句。 &nbsp;所谓的case表达式一般由case关键字和一个表达式列表组成，表达式列表中的多个表达式之间需要有英文逗号,分割，比如，上面代码中的case value1[0], value1[1]就是一个case表达式，其中的两个子表达式都是由索引表达式表示的。&nbsp;另外的两个case表达式分别是case value1[2], value1[3]和case value1[4], value1[5], value1[6]。&nbsp;此外，在这里的每个case子句中的那些打印语句，会分别打印出不同的内容，这些内容用于表示case子句被选中的原因，比如，打印内容0 or 1表示当前case子句被选中是因为switch表达式的结果值等于0或1中的某一个。另外两条打印语句会分别打印出2 or 3和4 or 5 or 6。&nbsp;只要switch表达式的结果值与某个case表达式中的任意一个子表达式的结果值相等，该case表达式所属的case子句就会被选中。&nbsp;并且，一旦某个case子句被选中，其中的附带在case表达式后边的那些语句就会被执行。与此同时，其他的所有case子句都会被忽略。&nbsp;当然了，如果被选中的case子句附带的语句列表中包含了fallthrough语句，那么紧挨在它下边的那个case子句附带的语句也会被执行。&nbsp;正因为存在上述判断相等的操作（以下简称判等操作），switch语句对switch表达式的结果类型，以及各个case表达式中子表达式的结果类型都是有要求的。毕竟，在 Go 语言中，只有类型相同的值之间才有可能被允许进行判等操作。&nbsp;如果switch表达式的结果值是无类型的常量，比如1 + 3的求值结果就是无类型的常量4，那么这个常量会被自动地转换为此种常量的默认类型的值，比如整数4的默认类型是int，又比如浮点数3.14的默认类型是float64。&nbsp;因此，由于上述代码中的switch表达式的结果类型是int，而那些case表达式中子表达式的结果类型却是int8，它们的类型并不相同，所以这条switch语句是无法通过编译的。&nbsp; 123456789value2 := [...]int8&#123;0, 1, 2, 3, 4, 5, 6&#125;switch value2[4] &#123;case 0, 1: fmt.Println(&quot;0 or 1&quot;)case 2, 3: fmt.Println(&quot;2 or 3&quot;)case 4, 5, 6: fmt.Println(&quot;4 or 5 or 6&quot;)&#125; &nbsp; 其中的变量value2与value1的值是完全相同的。但不同的是，我把switch表达式换成了value2[4]，并把下边那三个case表达式分别换为了case 0, 1、case 2, 3和case 4, 5, 6。 &nbsp;如此一来，switch表达式的结果值是int8类型的，而那些case表达式中子表达式的结果值却是无类型的常量了。这与之前的情况恰恰相反。那么，这样的switch语句可以通过编译吗？&nbsp;答案是肯定的。因为，如果case表达式中子表达式的结果值是无类型的常量，那么它的类型会被自动地转换为switch表达式的结果类型，又由于上述那几个整数都可以被转换为int8类型的值，所以对这些表达式的结果值进行判等操作是没有问题的。&nbsp;当然了，如果这里说的自动转换没能成功，那么switch语句照样通不过编译。&nbsp;&nbsp;通过上面这两道题，你应该可以搞清楚switch表达式和case表达式之间的联系了。由于需要进行判等操作，所以前者和后者中的子表达式的结果类型需要相同。&nbsp;switch语句会进行有限的类型转换，但肯定不能保证这种转换可以统一它们的类型。还要注意，如果这些表达式的结果类型有某个接口类型，那么一定要小心检查它们的动态值是否都具有可比性（或者说是否允许判等操作）。&nbsp;因为，如果答案是否定的，虽然不会造成编译错误，但是后果会更加严重：引发 panic（也就是运行时恐慌）。&nbsp;switch语句在case子句的选择上是具有唯一性的。&nbsp;正因为如此，switch语句不允许case表达式中的子表达式结果值存在相等的情况，不论这些结果值相等的子表达式，是否存在于不同的case表达式中，都会是这样的结果。具体请看这段代码：&nbsp; 123456789value3 := [...]int8&#123;0, 1, 2, 3, 4, 5, 6&#125;switch value3[4] &#123;case 0, 1, 2: fmt.Println(&quot;0 or 1 or 2&quot;)case 2, 3, 4: fmt.Println(&quot;2 or 3 or 4&quot;)case 4, 5, 6: fmt.Println(&quot;4 or 5 or 6&quot;)&#125; &nbsp; 变量value3的值同value1，依然是由从0到6的 7 个整数组成的数组，元素类型是int8。switch表达式是value3[4]，三个case表达式分别是case 0, 1, 2、case 2, 3, 4和case 4, 5, 6。 &nbsp;由于在这三个case表达式中存在结果值相等的子表达式，所以这个switch语句无法通过编译。不过，好在这个约束本身还有个约束，那就是只针对结果值为常量的子表达式。&nbsp;比如，子表达式1+1和2不能同时出现，1+3和4也不能同时出现。有了这个约束的约束，我们就可以想办法绕过这个对子表达式的限制了。再看一段代码：&nbsp; 123456789value5 := [...]int8&#123;0, 1, 2, 3, 4, 5, 6&#125;switch value5[4] &#123;case value5[0], value5[1], value5[2]: fmt.Println(&quot;0 or 1 or 2&quot;)case value5[2], value5[3], value5[4]: fmt.Println(&quot;2 or 3 or 4&quot;)case value5[4], value5[5], value5[6]: fmt.Println(&quot;4 or 5 or 6&quot;)&#125; &nbsp; 变量名换成了value5，但这不是重点。重点是，我把case表达式中的常量都换成了诸如value5[0]这样的索引表达式。 &nbsp;虽然第一个case表达式和第二个case表达式都包含了value5[2]，并且第二个case表达式和第三个case表达式都包含了value5[4]，但这已经不是问题了。这条switch语句可以成功通过编译。&nbsp;不过，这种绕过方式对用于类型判断的switch语句（以下简称为类型switch语句）就无效了。因为类型switch语句中的case表达式的子表达式，都必须直接由类型字面量表示，而无法通过间接的方式表示。代码如下：&nbsp; 123456789value6 := interface&#123;&#125;(byte(127))switch t := value6.(type) &#123;case uint8, uint16: fmt.Println(&quot;uint8 or uint16&quot;)case byte: fmt.Printf(&quot;byte&quot;)default: fmt.Printf(&quot;unsupported type: %T&quot;, t)&#125; &nbsp; 变量value6的值是空接口类型的。该值包装了一个byte类型的值127。我在后面使用类型switch语句来判断value6的实际类型，并打印相应的内容。 &nbsp;这里有两个普通的case子句，还有一个default case子句。前者的case表达式分别是case uint8, uint16和case byte。你还记得吗？byte类型是uint8类型的别名类型。&nbsp;因此，它们两个本质上是同一个类型，只是类型名称不同罢了。在这种情况下，这个类型switch语句是无法通过编译的，因为子表达式byte和uint8重复了。&nbsp;普通case子句的编写顺序很重要，最上边的case子句中的子表达式总是会被最先求值，在判等的时候顺序也是这样。因此，如果某些子表达式的结果值有重复并且它们与switch表达式的结果值相等，那么位置靠上的case子句总会被选中。&nbsp;&nbsp; 原文链接：https://time.geekbang.org/column/intro/112","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Go语言","slug":"Go语言","permalink":"http://wht6.github.io/tags/Go%E8%AF%AD%E8%A8%80/"}]},{"title":"接口类型和指针","slug":"接口类型的合理使用","date":"2021-04-18T08:00:00.000Z","updated":"2022-04-10T09:53:05.958Z","comments":true,"path":"posts/874b.html","link":"","permalink":"http://wht6.github.io/posts/874b.html","excerpt":"","text":"接口类型的合理使用在 Go 语言的语境中，当我们在谈论“接口”的时候，一定指的是接口类型。因为接口类型与其他数据类型不同，它是没法被实例化的。 &nbsp; 更具体地说，我们既不能通过调用new函数或make函数创建出一个接口类型的值，也无法用字面量来表示一个接口类型的值。&nbsp;对于某一个接口类型来说，如果没有任何数据类型可以作为它的实现，那么该接口的值就不可能存在。&nbsp;通过关键字type和interface，我们可以声明出接口类型。&nbsp;接口类型的类型字面量与结构体类型的看起来有些相似，它们都用花括号包裹一些核心信息。只不过，结构体类型包裹的是它的字段声明，而接口类型包裹的是它的方法定义。&nbsp;这里要注意的是：接口类型声明中的这些方法所代表的就是该接口的方法集合。一个接口的方法集合就是它的全部特征。&nbsp;对于任何数据类型，只要它的方法集合中完全包含了一个接口的全部特征（即全部的方法），那么它就一定是这个接口的实现类型。比如下面这样：&nbsp;12345type Pet interface &#123; SetName(name string) Name() string Category() string&#125;&nbsp; 这里声明了一个接口类型Pet，它包含了 3 个方法定义，方法名称分别为SetName、Name和Category。这 3 个方法共同组成了接口类型Pet的方法集合。 &nbsp;只要一个数据类型的方法集合中有这 3 个方法，那么它就一定是Pet接口的实现类型。这是一种无侵入式的接口实现方式。这种方式还有一个专有名词，叫“Duck typing”，中文常译作“鸭子类型”。&nbsp;怎样判定一个数据类型的某一个方法实现的就是某个接口类型中的某个方法呢？&nbsp;这有两个充分必要条件，一个是“两个方法的签名需要完全一致”，另一个是“两个方法的名称要一模一样”。显然，这比判断一个函数是否实现了某个函数类型要更加严格一些。&nbsp;123456789101112131415type Dog struct &#123; name string // 名字。&#125;func (dog *Dog) SetName(name string) &#123; dog.name = name&#125;func (dog Dog) Name() string &#123; return dog.name&#125;func (dog Dog) Category() string &#123; return &quot;dog&quot;&#125;&nbsp; 声明的类型Dog附带了 3 个方法。其中有 2 个值方法，分别是Name和Category，另外还有一个指针方法SetName。 &nbsp;这就意味着，Dog类型本身的方法集合中只包含了 2 个方法，也就是所有的值方法。而它的指针类型*Dog方法集合却包含了 3 个方法，&nbsp;也就是说，它拥有Dog类型附带的所有值方法和指针方法。又由于这 3 个方法恰恰分别是Pet接口中某个方法的实现，所以*Dog类型就成为了Pet接口的实现类型。&nbsp;12dog := Dog&#123;&quot;little pig&quot;&#125;var pet Pet = &amp;dog&nbsp; 正因为如此，可以声明并初始化一个Dog类型的变量dog，然后把它的指针值赋给类型为Pet的变量pet。 &nbsp;对于一个接口类型的变量来说，例如上面的变量pet，我们赋给它的值可以被叫做它的实际值（也称动态值），而该值的类型可以被叫做这个变量的实际类型（也称动态类型）。&nbsp;比如，我们把取址表达式&amp;dog的结果值赋给了变量pet，这时这个结果值就是变量pet的动态值，而此结果值的类型*Dog就是该变量的动态类型。&nbsp;动态类型这个叫法是相对于静态类型而言的。对于变量pet来讲，它的静态类型就是Pet，并且永远是Pet，但是它的动态类型却会随着我们赋给它的动态值而变化。&nbsp;比如，只有我把一个*Dog类型的值赋给变量pet之后，该变量的动态类型才会是*Dog。如果还有一个Pet接口的实现类型*Fish，并且我又把一个此类型的值赋给了pet，那么它的动态类型就会变为*Fish。&nbsp;还有，在我们给一个接口类型的变量赋予实际的值之前，它的动态类型是不存在的。&nbsp; 接口变量赋值&nbsp;当我们为一个接口变量赋值时会发生什么？&nbsp;把Pet接口的声明简化了一下。从中去掉了Pet接口的那个名为SetName的方法。这样一来，Dog类型也就变成Pet接口的实现类型了。&nbsp;1234type Pet interface &#123; Name() string Category() string&#125;&nbsp; 声明并初始化了一个Dog类型的变量dog，这时它的name字段的值是&quot;little pig&quot;。然后，把该变量赋给了一个Pet类型的变量pet。最后我通过调用dog的方法SetName把它的name字段的值改成了&quot;monster&quot;。 &nbsp;123dog := Dog&#123;&quot;little pig&quot;&#125;var pet Pet = dogdog.SetName(&quot;monster&quot;)&nbsp; 在以上代码执行后，pet变量的字段name的值依然是&quot;little pig&quot;。原因是： &nbsp;首先，由于dog的SetName方法是指针方法，所以该方法持有的接收者就是指向dog的指针值的副本，因而其中对接收者的name字段的设置就是对变量dog的改动。那么当dog.SetName(&quot;monster&quot;)执行之后，dog的name字段的值就一定是&quot;monster&quot;。如果你理解到了这一层，那么请小心前方的陷阱。&nbsp;为什么dog的name字段值变了，而pet的却没有呢？这里有一条通用的规则需要你知晓：如果我们使用一个变量给另外一个变量赋值，那么真正赋给后者的，并不是前者持有的那个值，而是该值的一个副本。&nbsp;例如，我声明并初始化了一个Dog类型的变量dog1，这时它的name是&quot;little pig&quot;。然后，我在把dog1赋给变量dog2之后，修改了dog1的name字段的值。这时，dog2的name字段的值是什么？&nbsp;123dog1 := Dog&#123;&quot;little pig&quot;&#125;dog2 := dog1dog1.name = &quot;monster&quot;&nbsp; 这个问题与前面那道题几乎一样，只不过这里没有涉及接口类型。这时的dog2的name仍然会是&quot;little pig&quot;。这就是我刚刚告诉你的那条通用规则的又一个体现。 &nbsp;当你知道了这条通用规则之后，确实可以把前面那道题做对。不过，如果当我问你为什么的时候你只说出了这一个原因，那么，我只能说你仅仅答对了一半。&nbsp;那么另一半是什么？这就需要从接口类型值的存储方式和结构说起了。我在前面说过，接口类型本身是无法被值化的。在我们赋予它实际的值之前，它的值一定会是nil，这也是它的零值。&nbsp;反过来讲，一旦它被赋予了某个实现类型的值，它的值就不再是nil了。不过要注意，即使我们像前面那样把dog的值赋给了pet，pet的值与dog的值也是不同的。这不仅仅是副本与原值的那种不同。&nbsp;当我们给一个接口变量赋值的时候，该变量的动态类型会与它的动态值一起被存储在一个专用的数据结构中。&nbsp;严格来讲，这样一个变量的值其实是这个专用数据结构的一个实例，而不是我们赋给该变量的那个实际的值。所以我才说，pet的值与dog的值肯定是不同的，无论是从它们存储的内容，还是存储的结构上来看都是如此。不过，我们可以认为，这时pet的值中包含了dog值的副本。&nbsp;我们就把这个专用的数据结构叫做iface吧，在 Go 语言的runtime包中它其实就叫这个名字。&nbsp;iface的实例会包含两个指针，一个是指向类型信息的指针，另一个是指向动态值的指针。这里的类型信息是由另一个专用数据结构的实例承载的，其中包含了动态值的类型，以及使它实现了接口的方法和调用它们的途径，等等。&nbsp;总之，接口变量被赋予动态值的时候，存储的是包含了这个动态值的副本的一个结构更加复杂的值。&nbsp;接口变量的值在什么情况下才真正为nil？&nbsp;12345678910var dog1 *Dogfmt.Println(&quot;The first dog is nil. [wrap1]&quot;)dog2 := dog1fmt.Println(&quot;The second dog is nil. [wrap1]&quot;)var pet Pet = dog2if pet == nil &#123; fmt.Println(&quot;The pet is nil. [wrap1]&quot;)&#125; else &#123; fmt.Println(&quot;The pet is not nil. [wrap1]&quot;)&#125;&nbsp; 我先声明了一个*Dog类型的变量dog1，并且没有对它进行初始化。这时该变量的值是什么？显然是nil。然后我把该变量赋给了dog2，后者的值此时也必定是nil，对吗？ &nbsp;现在问题来了：当我把dog2赋给Pet类型的变量pet之后，变量pet的值会是什么？答案是nil吗？&nbsp;如果你真正理解了我在上一个问题的解析中讲到的知识，尤其是接口变量赋值及其值的数据结构那部分，那么这道题就不难回答。你可以先思考一下，然后再接着往下看。&nbsp;当我们把dog2的值赋给变量pet的时候，dog2的值会先被复制，不过由于在这里它的值是nil，所以就没必要复制了。&nbsp;然后，Go 语言会用我上面提到的那个专用数据结构iface的实例包装这个dog2的值的副本，这里是nil。&nbsp;虽然被包装的动态值是nil，但是pet的值却不会是nil，因为这个动态值只是pet值的一部分而已。&nbsp;顺便说一句，这时的pet的动态类型就存在了，是*Dog。我们可以通过fmt.Printf函数和占位符%T来验证这一点，另外reflect包的TypeOf函数也可以起到类似的作用。&nbsp;在 Go 语言中，我们把由字面量nil表示的值叫做无类型的nil。这是真正的nil，因为它的类型也是nil的。虽然dog2的值是真正的nil，但是当我们把这个变量赋给pet的时候，Go 语言会把它的类型和值放在一起考虑。&nbsp;也就是说，这时 Go 语言会识别出赋予pet的值是一个*Dog类型的nil。然后，Go 语言就会用一个iface的实例包装它，包装后的产物肯定就不是nil了。&nbsp;只要我们把一个有类型的nil赋给接口变量，那么这个变量的值就一定不会是那个真正的nil。因此，当我们使用判等符号==判断pet是否与字面量nil相等的时候，答案一定会是false。&nbsp;那么，怎样才能让一个接口变量的值真正为nil呢？要么只声明它但不做初始化，要么直接把字面量nil赋给它。&nbsp; 接口类型的嵌入&nbsp;接口类型间的嵌入也被称为接口的组合。结构体类型的嵌入字段，这其实就是在说结构体类型间的嵌入。&nbsp;接口类型间的嵌入要更简单一些，因为它不会涉及方法间的“屏蔽”。只要组合的接口之间有同名的方法就会产生冲突，从而无法通过编译，即使同名方法的签名彼此不同也会是如此。因此，接口的组合根本不可能导致“屏蔽”现象的出现。&nbsp;与结构体类型间的嵌入很相似，我们只要把一个接口类型的名称直接写到另一个接口类型的成员列表中就可以了。比如：&nbsp;123456789type Animal interface &#123; ScientificName() string Category() string&#125; type Pet interface &#123; Animal Name() string&#125;&nbsp; 接口类型Pet包含了两个成员，一个是代表了另一个接口类型的Animal，一个是方法Name的定义。它们都被包含在Pet的类型声明的花括号中，并且都各自独占一行。此时，Animal接口包含的所有方法也就成为了Pet接口的方法。 &nbsp;Go 语言团队鼓励我们声明体量较小的接口，并建议我们通过这种接口间的组合来扩展程序、增加程序的灵活性。&nbsp;这是因为相比于包含很多方法的大接口而言，小接口可以更加专注地表达某一种能力或某一类特征，同时也更容易被组合在一起。&nbsp;Go 语言标准库代码包io中的ReadWriteCloser接口和ReadWriter接口就是这样的例子，它们都是由若干个小接口组合而成的。以io.ReadWriteCloser接口为例，它是由io.Reader、io.Writer和io.Closer这三个接口组成的。&nbsp;这三个接口都只包含了一个方法，是典型的小接口。它们中的每一个都只代表了一种能力，分别是读出、写入和关闭。我们编写这几个小接口的实现类型通常都会很容易。并且，一旦我们同时实现了它们，就等于实现了它们的组合接口io.ReadWriteCloser。&nbsp;即使我们只实现了io.Reader和io.Writer，那么也等同于实现了io.ReadWriter接口，因为后者就是前两个接口组成的。可以看到，这几个io包中的接口共同组成了一个接口矩阵。它们既相互关联又独立存在。 指针对于基本类型Dog来说，*Dog就是它的指针类型。而对于一个Dog类型，值不为nil的变量dog，取址表达式&amp;dog的结果就是该变量的值（也就是基本值）的指针值。 &nbsp; 1234567type Dog struct &#123; name string&#125; func (dog *Dog) SetName(name string) &#123; dog.name = name&#125; &nbsp; 如果一个方法的接收者是*Dog类型的，那么该方法就是基本类型Dog的一个指针方法。 &nbsp;在这种情况下，这个方法的接收者实际上就是当前的基本值的指针值。我们可以通过指针值无缝地访问到基本值包含的任何字段，以及调用与之关联的任何方法。这应该就是我们在编写 Go 程序的过程中，用得最频繁的“指针”了。&nbsp;从传统意义上说，指针是一个指向某个确切的内存地址的值。这个内存地址可以是任何数据或代码的起始地址，比如，某个变量、某个字段或某个函数。&nbsp;在 Go 语言中还有其他几样东西可以代表“指针”。其中最贴近传统意义的当属uintptr类型了。该类型实际上是一个数值类型，也是 Go 语言内建的数据类型之一。&nbsp;根据当前计算机的计算架构的不同，它可以存储 32 位或 64 位的无符号整数，可以代表任何指针的位（bit）模式，也就是原始的内存地址。&nbsp;再来看 Go 语言标准库中的unsafe包。unsafe包中有一个类型叫做Pointer，也代表了“指针”。&nbsp;unsafe.Pointer可以表示任何指向可寻址的值的指针，同时它也是前面提到的指针值和uintptr值之间的桥梁。也就是说，通过它，我们可以在这两种值之上进行双向的转换。这里有一个很关键的词——可寻址的（addressable）。在我们继续说unsafe.Pointer之前，需要先要搞清楚这个词的确切含义。&nbsp; 不可寻址的值&nbsp;以下列表中的值都是不可寻址的。&nbsp; 常量的值。 基本类型值的字面量。 算术操作的结果值。 对各种字面量的索引表达式和切片表达式的结果值。不过有一个例外，对切片字面量的索引结果值却是可寻址的。 对字符串变量的索引表达式和切片表达式的结果值。 对字典变量的索引表达式的结果值。 函数字面量和方法字面量，以及对它们的调用表达式的结果值。 结构体字面量的字段值，也就是对结构体字面量的选择表达式的结果值。 类型转换表达式的结果值。 类型断言表达式的结果值。 接收表达式的结果值。&nbsp; 常量的值总是会被存储到一个确切的内存区域中，并且这种值肯定是不可变的。基本类型值的字面量也是一样，其实它们本就可以被视为常量，只不过没有任何标识符可以代表它们罢了。 &nbsp;第一个关键词：不可变的。由于 Go 语言中的字符串值也是不可变的，所以对于一个字符串类型的变量来说，基于它的索引或切片的结果值也都是不可寻址的，因为即使拿到了这种值的内存地址也改变不了什么。&nbsp;算术操作的结果值属于一种临时结果。在我们把这种结果值赋给任何变量或常量之前，即使能拿到它的内存地址也是没有任何意义的。&nbsp;第二个关键词：临时结果。这个关键词能被用来解释很多现象。我们可以把各种对值字面量施加的表达式的求值结果都看做是临时结果。&nbsp;我们都知道，Go 语言中的表达式有很多种，其中常用的包括以下几种。&nbsp; 用于获得某个元素的索引表达式。 用于获得某个切片（片段）的切片表达式。 用于访问某个字段的选择表达式。 用于调用某个函数或方法的调用表达式。 用于转换值的类型的类型转换表达式。 用于判断值的类型的类型断言表达式。 向通道发送元素值或从通道那里接收元素值的接收表达式。&nbsp; 我们把以上这些表达式施加在某个值字面量上一般都会得到一个临时结果。比如，对数组字面量和字典字面量的索引结果值，又比如，对数组字面量和切片字面量的切片结果值。它们都属于临时结果，都是不可寻址的。 &nbsp;一个需要特别注意的例外是，对切片字面量的索引结果值是可寻址的。因为不论怎样，每个切片值都会持有一个底层数组，而这个底层数组中的每个元素值都是有一个确切的内存地址的。&nbsp;你可能会问，那么对切片字面量的切片结果值为什么却是不可寻址的？这是因为切片表达式总会返回一个新的切片值，而这个新的切片值在被赋给变量之前属于临时结果。&nbsp;你可能已经注意到了，我一直在说针对数组值、切片值或字典值的字面量的表达式会产生临时结果。如果针对的是数组类型或切片类型的变量，那么索引或切片的结果值就都不属于临时结果了，是可寻址的。&nbsp;这主要因为变量的值本身就不是“临时的”。对比而言，值字面量在还没有与任何变量（或者说任何标识符）绑定之前是没有落脚点的，我们无法以任何方式引用到它们。这样的值就是“临时的”。&nbsp;再说一个例外。我们通过对字典类型的变量施加索引表达式，得到的结果值不属于临时结果，可是，这样的值却是不可寻址的。原因是，字典中的每个键 - 元素对的存储位置都可能会变化，而且这种变化外界是无法感知的。&nbsp;我们都知道，字典中总会有若干个哈希桶用于均匀地储存键 - 元素对。当满足一定条件时，字典可能会改变哈希桶的数量，并适时地把其中的键 - 元素对搬运到对应的新的哈希桶中。&nbsp;在这种情况下，获取字典中任何元素值的指针都是无意义的，也是不安全的。我们不知道什么时候那个元素值会被搬运到何处，也不知道原先的那个内存地址上还会被存放什么别的东西。所以，这样的值就应该是不可寻址的。&nbsp;第三个关键词：不安全的。“不安全的”操作很可能会破坏程序的一致性，引发不可预知的错误，从而严重影响程序的功能和稳定性。&nbsp;再来看函数。函数在 Go 语言中是一等公民，所以我们可以把代表函数或方法的字面量或标识符赋给某个变量、传给某个函数或者从某个函数传出。但是，这样的函数和方法都是不可寻址的。一个原因是函数就是代码，是不可变的。&nbsp;另一个原因是，拿到指向一段代码的指针是不安全的。此外，对函数或方法的调用结果值也是不可寻址的，这是因为它们都属于临时结果。&nbsp;至于典型回答中最后列出的那几种值，由于都是针对值字面量的某种表达式的结果值，所以都属于临时结果，都不可寻址。&nbsp;总结一下。&nbsp; 不可变的值不可寻址。常量、基本类型的值字面量、字符串变量的值、函数以及方法的字面量都是如此。其实这样规定也有安全性方面的考虑。 绝大多数被视为临时结果的值都是不可寻址的。算术操作的结果值属于临时结果，针对值字面量的表达式结果值也属于临时结果。但有一个例外，对切片字面量的索引结果值虽然也属于临时结果，但却是可寻址的。 若拿到某值的指针可能会破坏程序的一致性，那么就是不安全的，该值就不可寻址。由于字典的内部机制，对字典的索引结果值的取址操作都是不安全的。另外，获取由字面量或标识符代表的函数或方法的地址显然也是不安全的。&nbsp; 最后说一句，如果我们把临时结果赋给一个变量，那么它就是可寻址的了。如此一来，取得的指针指向的就是这个变量持有的那个值了。 &nbsp; 不可寻址的值使用上的限制&nbsp;首当其冲的当然是无法使用取址操作符&amp;获取它们的指针了。不过，对不可寻址的值施加取址操作都会使编译器报错，所以倒是不用太担心，你只要记住我在前面讲述的那几条规律，并在编码的时候提前注意一下就好了。&nbsp; 123func New(name string) Dog &#123; return Dog&#123;name&#125;&#125; &nbsp; 编写一个函数New。这个函数会接受一个名为name的string类型的参数，并会用这个参数初始化一个Dog类型的值，最后返回该值。我现在要问的是：如果我调用该函数，并直接以链式的手法调用其结果值的指针方法SetName，那么可以达到预期的效果吗？ &nbsp; 1New(&quot;little pig&quot;).SetName(&quot;monster&quot;) &nbsp; 首先，调用New函数所得到的结果值属于临时结果，是不可寻址的。 &nbsp;其次，我们可以在一个基本类型的值上调用它的指针方法，这是因为 Go 语言会自动地帮我们转译。&nbsp;更具体地说，对于一个Dog类型的变量dog来说，调用表达式dog.SetName(&quot;monster&quot;)会被自动地转译为(&amp;dog).SetName(&quot;monster&quot;)，即：先取dog的指针值，再在该指针值上调用SetName方法。&nbsp;由于New函数的调用结果值是不可寻址的，所以无法对它进行取址操作。因此，上边这行链式调用会让编译器报告两个错误，一个是果，即：不能在New(&quot;little pig&quot;)的结果值上调用指针方法。一个是因，即：不能取得New(&quot;little pig&quot;)的地址。&nbsp;除此之外，我们都知道，Go 语言中的++和--并不属于操作符，而分别是自增语句和自减语句的重要组成部分。&nbsp;虽然 Go 语言规范中的语法定义是，只要在++或--的左边添加一个表达式，就可以组成一个自增语句或自减语句，但是，它还明确了一个很重要的限制，那就是这个表达式的结果值必须是可寻址的。这就使得针对值字面量的表达式几乎都无法被用在这里。&nbsp;不过这有一个例外，虽然对字典字面量和字典变量索引表达式的结果值都是不可寻址的，但是这样的表达式却可以被用在自增语句和自减语句中。&nbsp;与之类似的规则还有两个。一个是，在赋值语句中，赋值操作符左边的表达式的结果值必须可寻址的，但是对字典的索引结果值也是可以的。&nbsp;另一个是，在带有range子句的for语句中，在range关键字左边的表达式的结果值也都必须是可寻址的，不过对字典的索引结果值同样可以被用在这里。以上这三条规则我们合并起来记忆就可以了。&nbsp; 通过unsafe.Pointer操纵可寻址的值&nbsp;unsafe.Pointer是像*Dog类型的值这样的指针值和uintptr值之间的桥梁，那么我们怎样利用unsafe.Pointer的中转和uintptr的底层操作来操纵像dog这样的值呢？&nbsp;首先说明，这是一项黑科技。它可以绕过 Go 语言的编译器和其他工具的重重检查，并达到潜入内存修改数据的目的。这并不是一种正常的编程手段，使用它会很危险，很有可能造成安全隐患。&nbsp;我们总是应该优先使用常规代码包中提供的 API 去编写程序，当然也可以把像reflect以及go/ast这样的代码包作为备选项。作为上层应用的开发者，请谨慎地使用unsafe包中的任何程序实体。&nbsp;不过既然说到这里了，我们还是要来一探究竟的。请看下面的代码：&nbsp; 123dog := Dog&#123;&quot;little pig&quot;&#125;dogP := &amp;dogdogPtr := uintptr(unsafe.Pointer(dogP)) &nbsp; 这里先声明了一个Dog类型的变量dog，然后用取址操作符&amp;，取出了它的指针值，并把它赋给了变量dogP。 &nbsp;最后，我使用了两个类型转换，先把dogP转换成了一个unsafe.Pointer类型的值，然后紧接着又把后者转换成了一个uintptr的值，并把它赋给了变量dogPtr。这背后隐藏着一些转换规则，如下：&nbsp; 一个指针值（比如*Dog类型的值）可以被转换为一个unsafe.Pointer类型的值，反之亦然。 一个uintptr类型的值也可以被转换为一个unsafe.Pointer类型的值，反之亦然。 一个指针值无法被直接转换成一个uintptr类型的值，反过来也是如此。&nbsp; 所以，对于指针值和uintptr类型值之间的转换，必须使用unsafe.Pointer类型的值作为中转。那么，我们把指针值转换成uintptr类型的值有什么意义吗？ &nbsp; 12namePtr := dogPtr + unsafe.Offsetof(dogP.name)nameP := (*string)(unsafe.Pointer(namePtr)) &nbsp; 这里需要与unsafe.Offsetof函数搭配使用才能看出端倪。unsafe.Offsetof函数用于获取两个值在内存中的起始存储地址之间的偏移量，以字节为单位。 &nbsp;这两个值一个是某个字段的值，另一个是该字段值所属的那个结构体值。我们在调用这个函数的时候，需要把针对字段的选择表达式传给它，比如dogP.name。&nbsp;有了这个偏移量，又有了结构体值在内存中的起始存储地址（这里由dogPtr变量代表），把它们相加我们就可以得到dogP的name字段值的起始存储地址了。这个地址由变量namePtr代表。&nbsp;此后，我们可以再通过两次类型转换把namePtr的值转换成一个*string类型的值，这样就得到了指向dogP的name字段值的指针值。&nbsp;你可能会问，我直接用取址表达式&amp;(dogP.name)不就能拿到这个指针值了吗？干嘛绕这么大一圈呢？你可以想象一下，如果我们根本就不知道这个结构体类型是什么，也拿不到dogP这个变量，那么还能去访问它的name字段吗？&nbsp;答案是，只要有namePtr就可以。它就是一个无符号整数，但同时也是一个指向了程序内部数据的内存地址。它可能会给我们带来一些好处，比如可以直接修改埋藏得很深的内部数据。&nbsp;但是，一旦我们有意或无意地把这个内存地址泄露出去，那么其他人就能够肆意地改动dogP.name的值，以及周围的内存地址上存储的任何数据了。&nbsp;引用类型的值的指针值是有意义的吗？从存储和传递的角度看，没有意义。因为引用类型的值已经相当于指向某个底层数据结构的指针了。当然，引用类型的值不只是指针那么简单。 &nbsp; &nbsp; 原文链接：https://time.geekbang.org/column/intro/112","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Go语言","slug":"Go语言","permalink":"http://wht6.github.io/tags/Go%E8%AF%AD%E8%A8%80/"}]},{"title":"Go语言的函数和结构体","slug":"Go语言的函数","date":"2021-04-15T09:00:00.000Z","updated":"2022-04-10T09:48:23.658Z","comments":true,"path":"posts/24dd.html","link":"","permalink":"http://wht6.github.io/posts/24dd.html","excerpt":"","text":"函数在 Go 语言中，函数可是一等的（first-class）公民，函数类型也是一等的数据类型。这是什么意思呢？&nbsp;简单来说，这意味着函数不但可以用于封装代码、分割功能、解耦逻辑，还可以化身为普通的值，在其他函数间传递、赋予变量、做类型判断和转换等等，就像切片和字典的值那样。 &nbsp; 而更深层次的含义就是：函数值可以由此成为能够被随意传播的独立逻辑组件（或者说功能模块）。&nbsp;对于函数类型来说，它是一种对一组输入、输出进行模板化的重要工具，它比接口类型更加轻巧、灵活，它的值也借此变成了可被热替换的逻辑组件。&nbsp; 123456789101112131415package main import &quot;fmt&quot; type Printer func(contents string) (n int, err error) func printToStd(contents string) (bytesNum int, err error) &#123; return fmt.Println(contents)&#125; func main() &#123; var p Printer p = printToStd p(&quot;something&quot;)&#125; &nbsp; 这里，我先声明了一个函数类型，名叫Printer。 &nbsp;注意这里的写法，在类型声明的名称右边的是func关键字，我们由此就可知道这是一个函数类型的声明。&nbsp;在func右边的就是这个函数类型的参数列表和结果列表。其中，参数列表必须由圆括号包裹，而只要结果列表中只有一个结果声明，并且没有为它命名，我们就可以省略掉外围的圆括号。&nbsp;在func右边的就是这个函数类型的参数列表和结果列表。其中，参数列表必须由圆括号包裹，而只要结果列表中只有一个结果声明，并且没有为它命名，我们就可以省略掉外围的圆括号。&nbsp;书写函数签名的方式与函数声明的是一致的。只是紧挨在参数列表左边的不是函数名称，而是关键字func。这里函数名称和func互换了一下位置而已。&nbsp; 函数的签名其实就是函数的参数列表和结果列表的统称，它定义了可用来鉴别不同函数的那些特征，同时也定义了我们与函数交互的方式。 &nbsp;注意，各个参数和结果的名称不能算作函数签名的一部分，甚至对于结果声明来说，没有名称都可以。 &nbsp;只要两个函数的参数列表和结果列表中的元素顺序及其类型是一致的，我们就可以说它们是一样的函数，或者说是实现了同一个函数类型的函数。&nbsp;严格来说，函数的名称也不能算作函数签名的一部分，它只是我们在调用函数时，需要给定的标识符而已。&nbsp;我在下面声明的函数printToStd的签名与Printer的是一致的，因此前者是后者的一个实现，即使它们的名称以及有的结果名称是不同的。&nbsp;通过main函数中的代码，我们就可以证实这两者的关系了，我顺利地把printToStd函数赋给了Printer类型的变量p，并且成功地调用了它。&nbsp; 高阶函数&nbsp;高阶函数可以满足下面的两个条件：&nbsp;1. 接受其他的函数作为参数传入；2. 把其他的函数作为结果返回。&nbsp;只要满足了其中任意一个特点，我们就可以说这个函数是一个高阶函数。高阶函数也是函数式（functional programming）编程中的重要概念和特征。&nbsp;下面编写calculate函数来实现两个整数间的加减乘除运算，且两个整数和具体的操作都由该函数的调用方给出。&nbsp;首先，我们来声明一个名叫operate的函数类型，它有两个参数和一个结果，都是int类型的。&nbsp;1type operate func(x, y int) int&nbsp; 然后，我们编写calculate函数的签名部分。这个函数除了需要两个int类型的参数之外，还应该有一个operate类型的参数。 &nbsp;该函数的结果应该有两个，一个是int类型的，代表真正的操作结果，另一个应该是error类型的，因为如果那个operate类型的参数值为nil，那么就应该直接返回一个错误。&nbsp;函数类型属于引用类型，它的值可以为nil，而这种类型的零值恰恰就是nil。&nbsp;123456func calculate(x int, y int, op operate) (int, error) &#123; if op == nil &#123; return 0, errors.New(&quot;invalid operation&quot;) &#125; return op(x, y), nil&#125;&nbsp; calculate函数实现起来就很简单了。我们需要先用卫述语句检查一下参数，如果operate类型的参数op为nil，那么就直接返回0和一个代表了具体错误的error类型值。 &nbsp; 卫述语句是指被用来检查关键的先决条件的合法性，并在检查未通过的情况下立即终止当前代码块执行的语句。在 Go 语言中，if 语句常被作为卫述语句。 &nbsp;如果检查无误，那么就调用op并把那两个操作数传给它，最后返回op返回的结果和代表没有错误发生的nil。 &nbsp;calculate函数的其中一个参数是operate类型的，而且后者是一个函数类型。在调用calculate函数的时候，我们需要传入一个operate类型的函数值。这个函数值应该怎么写？&nbsp;只要它的签名与operate类型的签名一致，并且实现得当就可以了。我们可以像上一个例子那样先声明好一个函数，再把它赋给一个变量，也可以直接编写一个实现了operate类型的匿名函数。&nbsp;123op := func(x, y int) int &#123; return x + y&#125;&nbsp; calculate函数就是一个高阶函数。但是我们说高阶函数的特点有两个，而该函数只展示了其中一个特点，即：接受其他的函数作为参数传入。 &nbsp;那另一个特点，把其他的函数作为结果返回。&nbsp;12345678910type calculateFunc func(x int, y int) (int, error)func genCalculator(op operate) calculateFunc &#123; return func(x int, y int) (int, error) &#123; if op == nil &#123; return 0, errors.New(&quot;invalid operation&quot;) &#125; return op(x, y), nil &#125;&#125;&nbsp; 这里声明的函数类型是calculateFunc和函数genCalculator。其中，genCalculator函数的唯一结果的类型就是calculateFunc。 &nbsp;12345x, y = 56, 78add := genCalculator(op)result, err = add(x, y)fmt.Printf(&quot;The result: %d (error: %v)\\n&quot;, result, err)&nbsp; 闭包&nbsp; 闭包又是什么？你可以想象一下，在一个函数中存在对外来标识符的引用。所谓的外来标识符，既不代表当前函数的任何参数或结果，也不是函数内部声明的，它是直接从外边拿过来的。&nbsp;还有个专门的术语称呼它，叫自由变量，可见它代表的肯定是个变量。实际上，如果它是个常量，那也就形成不了闭包了，因为常量是不可变的程序实体，而闭包体现的却是由“不确定”变为“确定”的一个过程。&nbsp;我们说的这个函数（以下简称闭包函数）就是因为引用了自由变量，而呈现出了一种“不确定”的状态，也叫“开放”状态。&nbsp;也就是说，它的内部逻辑并不是完整的，有一部分逻辑需要这个自由变量参与完成，而后者到底代表了什么在闭包函数被定义的时候却是未知的。&nbsp;即使对于像 Go 语言这种静态类型的编程语言而言，我们在定义闭包函数的时候最多也只能知道自由变量的类型。&nbsp;在我们刚刚提到的genCalculator函数内部，实际上就实现了一个闭包，而genCalculator函数也是一个高阶函数。&nbsp;genCalculator函数只做了一件事，那就是定义一个匿名的、calculateFunc类型的函数并把它作为结果值返回。&nbsp;而这个匿名的函数就是一个闭包函数。它里面使用的变量op既不代表它的任何参数或结果也不是它自己声明的，而是定义它的genCalculator函数的参数，所以是一个自由变量。&nbsp;这个自由变量究竟代表了什么，这一点并不是在定义这个闭包函数的时候确定的，而是在genCalculator函数被调用的时候确定的。&nbsp;只有给定了该函数的参数op，我们才能知道它返回给我们的闭包函数可以用于什么运算。&nbsp;看到if op == nil &#123;那一行了吗？Go 语言编译器读到这里时会试图去寻找op所代表的东西，它会发现op代表的是genCalculator函数的参数，然后，它会把这两者联系起来。这时可以说，自由变量op被“捕获”了。&nbsp;当程序运行到这里的时候，op就是那个参数值了。如此一来，这个闭包函数的状态就由“不确定”变为了“确定”，或者说转到了“闭合”状态，至此也就真正地形成了一个闭包。&nbsp;看出来了吗？我们在用高阶函数实现闭包。这也是高阶函数的一大功用。&nbsp;&nbsp;那么，实现闭包的意义又在哪里呢？表面上看，我们只是延迟实现了一部分程序逻辑或功能而已，但实际上，我们是在动态地生成那部分程序逻辑。&nbsp;我们可以借此在程序运行的过程中，根据需要生成功能不同的函数，继而影响后续的程序行为。这与 GoF 设计模式中的“模板方法”模式有着异曲同工之妙，不是吗？&nbsp; 传入函数的那些参数值&nbsp;让我们把目光再次聚焦到函数本身。我们先看一个示例。&nbsp;12345678910111213141516package main import &quot;fmt&quot; func main() &#123; array1 := [3]string&#123;&quot;a&quot;, &quot;b&quot;, &quot;c&quot;&#125; fmt.Printf(&quot;The array: %v\\n&quot;, array1) array2 := modifyArray(array1) fmt.Printf(&quot;The modified array: %v\\n&quot;, array2) fmt.Printf(&quot;The original array: %v\\n&quot;, array1)&#125; func modifyArray(a [3]string) [3]string &#123; a[1] = &quot;x&quot; return a&#125;&nbsp; 这个命令源码文件在运行之后会输出什么？ &nbsp;main函数中声明了一个数组array1，然后把它传给了函数modify，modify对参数值稍作修改后将其作为结果值返回。main函数中的代码拿到这个结果之后打印了它（即array2），以及原来的数组array1。关键问题是，原数组会因modify函数对参数值的修改而改变吗？&nbsp;答案是：原数组不会改变。为什么呢？原因是，所有传给函数的参数值都会被复制，函数在其内部使用的并不是参数值的原值，而是它的副本。&nbsp;由于数组是值类型，所以每一次复制都会拷贝它，以及它的所有元素值。我在modify函数中修改的只是原数组的副本而已，并不会对原数组造成任何影响。&nbsp;注意，对于引用类型，比如：切片、字典、通道，像上面那样复制它们的值，只会拷贝它们本身而已，并不会拷贝它们引用的底层数据。也就是说，这时只是浅表复制，而不是深层复制。&nbsp;以切片值为例，如此复制的时候，只是拷贝了它指向底层数组中某一个元素的指针，以及它的长度值和容量值，而它的底层数组并不会被拷贝。&nbsp;另外还要注意，就算我们传入函数的是一个值类型的参数值，但如果这个参数值中的某个元素是引用类型的，那么我们仍然要小心。&nbsp;比如：&nbsp;12345complexArray1 := [3][]string&#123; []string&#123;&quot;d&quot;, &quot;e&quot;, &quot;f&quot;&#125;, []string&#123;&quot;g&quot;, &quot;h&quot;, &quot;i&quot;&#125;, []string&#123;&quot;j&quot;, &quot;k&quot;, &quot;l&quot;&#125;,&#125;&nbsp; 变量complexArray1是[3][]string类型的，也就是说，虽然它是一个数组，但是其中的每个元素又都是一个切片。这样一个值被传入函数的话，函数中对该参数值的修改会影响到它的原值吗？答案：如果对complexArray1中的元素进行增减，那么原值就不会受到影响。但若要修改它已有的元素值，那么原值也会跟着改变。 &nbsp; 函数真正拿到的参数值其实只是它们的副本，那么函数返回给调用方的结果值也会被复制吗？答：函数返回给调用方的结果值也会被复制。不过，在一般情况下，我们不用太在意。但如果函数在返回结果值之后依然保持执行并会对结果值进行修改，那么我们就需要注意了。 结构体及其方法的使用结构体类型表示的是实实在在的数据结构。一个结构体类型可以包含若干个字段，每个字段通常都需要有确切的名字和类型。 &nbsp; 结构体类型也可以不包含任何字段，这样并不是没有意义的，因为我们还可以为类型关联上一些方法，这里你可以把方法看做是函数的特殊版本。&nbsp;函数是独立的程序实体。我们可以声明有名字的函数，也可以声明没名字的函数，还可以把它们当做普通的值传来传去。我们能把具有相同签名的函数抽象成独立的函数类型，以作为一组输入、输出（或者说一类逻辑组件）的代表。&nbsp;方法却不同，它需要有名字，不能被当作值来看待，最重要的是，它必须隶属于某一个类型。方法所属的类型会通过其声明中的接收者（receiver）声明体现出来。&nbsp;接收者声明就是在关键字func和方法名称之间的圆括号包裹起来的内容，其中必须包含确切的名称和类型字面量。&nbsp;接收者的类型其实就是当前方法所属的类型，而接收者的名称，则用于在当前方法中引用它所属的类型的当前值。&nbsp;我们举个例子来看一下。&nbsp; 12345678910111213141516// AnimalCategory 代表动物分类学中的基本分类法。type AnimalCategory struct &#123; kingdom string // 界。 phylum string // 门。 class string // 纲。 order string // 目。 family string // 科。 genus string // 属。 species string // 种。&#125; func (ac AnimalCategory) String() string &#123; return fmt.Sprintf(&quot;%s%s%s%s%s%s%s&quot;, ac.kingdom, ac.phylum, ac.class, ac.order, ac.family, ac.genus, ac.species)&#125; &nbsp; 结构体类型AnimalCategory代表了动物的基本分类法，其中有 7 个string类型的字段，分别表示各个等级的分类。 &nbsp;下边有个名叫String的方法，从它的接收者声明可以看出它隶属于AnimalCategory类型。&nbsp;通过该方法的接收者名称ac，我们可以在其中引用到当前值的任何一个字段，或者调用到当前值的任何一个方法（也包括String方法自己）。&nbsp;这个String方法的功能是提供当前值的字符串表示形式，其中的各个等级分类会按照从大到小的顺序排列。使用时，我们可以这样表示：&nbsp; 12category := AnimalCategory&#123;species: &quot;cat&quot;&#125;fmt.Printf(&quot;The animal category: %s\\n&quot;, category) &nbsp; 这里，我用字面量初始化了一个AnimalCategory类型的值，并把它赋给了变量category。只为其中的species字段指定了字符串值&quot;cat&quot;，该字段代表最末级分类“种”。 &nbsp;在 Go 语言中，我们可以通过为一个类型编写名为String的方法，来自定义该类型的字符串表示形式。这个String方法不需要任何参数声明，但需要有一个string类型的结果声明。&nbsp;正因为如此，我在调用fmt.Printf函数时，使用占位符%s和category值本身就可以打印出后者的字符串表示形式，而无需显式地调用它的String方法。&nbsp;fmt.Printf函数会自己去寻找它。此时的打印内容会是The animal category: cat。显而易见，category的String方法成功地引用了当前值的所有字段。&nbsp; 方法隶属的类型其实并不局限于结构体类型，但必须是某个自定义的数据类型，并且不能是任何接口类型。 一个数据类型关联的所有方法，共同组成了该类型的方法集合。同一个方法集合中的方法不能出现重名。并且，如果它们所属的是一个结构体类型，那么它们的名称与该类型中任何字段的名称也不能重复。 我们可以把结构体类型中的一个字段看作是它的一个属性或者一项数据，再把隶属于它的一个方法看作是附加在其中数据之上的一个能力或者一项操作。将属性及其能力（或者说数据及其操作）封装在一起，是面向对象编程（object-oriented programming）的一个主要原则。 Go 语言摄取了面向对象编程中的很多优秀特性，同时也推荐这种封装的做法。从这方面看，Go 语言其实是支持面向对象编程的，但它选择摒弃了一些在实际运用过程中容易引起程序开发者困惑的特性和规则。 &nbsp;现在，让我们再把目光放到结构体类型的字段声明上。 &nbsp; 1234type Animal struct &#123; scientificName string // 学名。 AnimalCategory // 动物基本分类。&#125; &nbsp; 这里声明了一个结构体类型，名叫Animal。它有两个字段。一个是string类型的字段scientificName，代表了动物的学名。而另一个字段声明中只有AnimalCategory，它正是前面编写的那个结构体类型的名字。 &nbsp;字段声明AnimalCategory代表了Animal类型的一个嵌入字段。Go 语言规范规定，如果一个字段的声明中只有字段的类型名而没有字段的名称，那么它就是一个嵌入字段，也可以被称为匿名字段。我们可以通过此类型变量的名称后跟“.”，再后跟嵌入字段类型的方式引用到该字段。也就是说，嵌入字段的类型既是类型也是名称。&nbsp; 123func (a Animal) Category() string &#123; return a.AnimalCategory.String()&#125; &nbsp; Category是Animal的一个方法，Category方法的接收者类型是Animal，接收者名称是a。在该方法中，我通过表达式a.AnimalCategory选择到了a的这个嵌入字段，然后又选择了该字段的String方法并调用了它。 &nbsp;顺便提一下，在某个代表变量的标识符的右边加“.”，再加上字段名或方法名的表达式被称为选择表达式，它用来表示选择了该变量的某个字段或者方法。&nbsp;实际上，把一个结构体类型嵌入到另一个结构体类型中的意义不止如此。嵌入字段的方法集合会被无条件地合并进被嵌入类型的方法集合中。例如下面这种：&nbsp; 12345animal := Animal&#123; scientificName: &quot;American Shorthair&quot;, AnimalCategory: category,&#125;fmt.Printf(&quot;The animal: %s\\n&quot;, animal) &nbsp; 我声明了一个Animal类型的变量animal并对它进行初始化。我把字符串值&quot;American Shorthair&quot;赋给它的字段scientificName，并把前面声明过的变量category赋给它的嵌入字段AnimalCategory。 &nbsp;我在后面使用fmt.Printf函数和%s占位符试图打印animal的字符串表示形式，相当于调用animal的String方法。虽然我们还没有为Animal类型编写String方法，但这样做是没问题的。因为在这里，嵌入字段AnimalCategory的String方法会被当做animal的方法调用。&nbsp;那如果我也为Animal类型编写一个String方法呢？这里会调用哪一个呢？&nbsp;答案是，animal的String方法会被调用。这时，我们说，嵌入字段AnimalCategory的String方法被“屏蔽”了。注意，只要名称相同，无论这两个方法的签名是否一致，被嵌入类型的方法都会“屏蔽”掉嵌入字段的同名方法。&nbsp;类似的，由于我们同样可以像访问被嵌入类型的字段那样，直接访问嵌入字段的字段，所以如果这两个结构体类型里存在同名的字段，那么嵌入字段中的那个字段一定会被“屏蔽”。这与可重名变量之间可能存在的“屏蔽”现象很相似。&nbsp;正因为嵌入字段的字段和方法都可以“嫁接”到被嵌入类型上，所以即使在两个同名的成员一个是字段，另一个是方法的情况下，这种“屏蔽”现象依然会存在。&nbsp;不过，即使被屏蔽了，我们仍然可以通过链式的选择表达式，选择到嵌入字段的字段或方法，就像我在Category方法中所做的那样。这种“屏蔽”其实还带来了一些好处。我们看看下面这个Animal类型的String方法的实现：&nbsp; 1234func (a Animal) String() string &#123; return fmt.Sprintf(&quot;%s (category: %s)&quot;, a.scientificName, a.AnimalCategory)&#125; &nbsp; 在这里，我们把对嵌入字段的String方法的调用结果融入到了Animal类型的同名方法的结果中。这种将同名方法的结果逐层“包装”的手法是很常见和有用的，也算是一种惯用法了。 &nbsp;&nbsp;最后，我还要提一下多层嵌入的问题。也就是说，嵌入字段本身也有嵌入字段的情况。请看我声明的Cat类型：&nbsp; 123456789type Cat struct &#123; name string Animal&#125; func (cat Cat) String() string &#123; return fmt.Sprintf(&quot;%s (category: %s, name: %q)&quot;, cat.scientificName, cat.Animal.AnimalCategory, cat.name)&#125; &nbsp; 结构体类型Cat中有一个嵌入字段Animal，而Animal类型还有一个嵌入字段AnimalCategory。 &nbsp;在这种情况下，“屏蔽”现象会以嵌入的层级为依据，嵌入层级越深的字段或方法越可能被“屏蔽”。&nbsp;例如，当我们调用Cat类型值的String方法时，如果该类型确有String方法，那么嵌入字段Animal和AnimalCategory的String方法都会被“屏蔽”。&nbsp;如果该类型没有String方法，那么嵌入字段Animal的String方法会被调用，而它的嵌入字段AnimalCategory的String方法仍然会被屏蔽。&nbsp;只有当Cat类型和Animal类型都没有String方法的时候，AnimalCategory的String方法才会被调用。&nbsp;最后的最后，如果处于同一个层级的多个嵌入字段拥有同名的字段或方法，那么从被嵌入类型的值那里，选择此名称的时候就会引发一个编译错误，因为编译器无法确定被选择的成员到底是哪一个。&nbsp;Go 语言是否是用嵌入字段实现了继承呢，答案是否，Go 语言中根本没有继承的概念，它所做的是通过嵌入字段的方式实现了类型之间的组合。&nbsp;简单来说，面向对象编程中的继承，其实是通过牺牲一定的代码简洁性来换取可扩展性，而且这种可扩展性是通过侵入的方式来实现的。&nbsp;类型之间的组合采用的是非声明的方式，我们不需要显式地声明某个类型实现了某个接口，或者一个类型继承了另一个类型。&nbsp;同时，类型组合也是非侵入式的，它不会破坏类型的封装或加重类型之间的耦合。&nbsp;我们要做的只是把类型当做字段嵌入进来，然后坐享其成地使用嵌入字段所拥有的一切。如果嵌入字段有哪里不合心意，我们还可以用“包装”或“屏蔽”的方式去调整和优化。&nbsp;另外，类型间的组合也是灵活的，我们总是可以通过嵌入字段的方式把一个类型的属性和能力“嫁接”给另一个类型。&nbsp;这时候，被嵌入类型也就自然而然地实现了嵌入字段所实现的接口。再者，组合要比继承更加简洁和清晰，Go 语言可以轻而易举地通过嵌入多个字段来实现功能强大的类型，却不会有多重继承那样复杂的层次结构和可观的管理成本。&nbsp;接口类型之间也可以组合。在 Go 语言中，接口类型之间的组合甚至更加常见，我们常常以此来扩展接口定义的行为或者标记接口的特征。&nbsp; 值方法和指针方法&nbsp;方法的接收者类型必须是某个自定义的数据类型，而且不能是接口类型或接口的指针类型。所谓的值方法，就是接收者类型是非指针的自定义数据类型的方法。&nbsp;比如，我们在前面为AnimalCategory、Animal以及Cat类型声明的那些方法都是值方法。就拿Cat来说，它的String方法的接收者类型就是Cat，一个非指针类型。那什么叫指针类型呢？请看这个方法：&nbsp; 123func (cat *Cat) SetName(name string) &#123; cat.name = name&#125; &nbsp; 方法SetName的接收者类型是*Cat。Cat左边再加个*代表的就是Cat类型的指针类型。 &nbsp;这时，Cat可以被叫做*Cat的基本类型。你可以认为这种指针类型的值表示的是指向某个基本类型值的指针。&nbsp;我们可以通过把取值操作符*放在这样一个指针值的左边来组成一个取值表达式，以获取该指针值指向的基本类型值，也可以通过把取址操作符&amp;放在一个可寻址的基本类型值的左边来组成一个取址表达式，以获取该基本类型值的指针值。&nbsp;所谓的指针方法，就是接收者类型是上述指针类型的方法。&nbsp;那么值方法和指针方法之间有什么不同点呢？它们的不同如下所示。&nbsp; 值方法的接收者是该方法所属的那个类型值的一个副本。我们在该方法内对该副本的修改一般都不会体现在原值上，除非这个类型本身是某个引用类型（比如切片或字典）的别名类型。 而指针方法的接收者，是该方法所属的那个基本类型值的指针值的一个副本。我们在这样的方法内对该副本指向的值进行修改，却一定会体现在原值上。 一个自定义数据类型的方法集合中仅会包含它的所有值方法，而该类型的指针类型的方法集合却囊括了前者的所有方法，包括所有值方法和所有指针方法。 严格来讲，我们在这样的基本类型的值上只能调用到它的值方法。但是，Go 语言会适时地为我们进行自动地转译，使得我们在这样的值上也能调用到它的指针方法。 比如，在Cat类型的变量cat之上，之所以我们可以通过cat.SetName(&quot;monster&quot;)修改猫的名字，是因为 Go 语言把它自动转译为了(&amp;cat).SetName(&quot;monster&quot;)，即：先取cat的指针值，然后在该指针值上调用SetName方法。 在后边你会了解到，一个类型的方法集合中有哪些方法与它能实现哪些接口类型是息息相关的。如果一个基本类型和它的指针类型的方法集合是不同的，那么它们具体实现的接口类型的数量就也会有差异，除非这两个数量都是零。 比如，一个指针类型实现了某某接口类型，但它的基本类型却不一定能够作为该接口的实现类型。 &nbsp;&nbsp; 原文链接：https://time.geekbang.org/column/intro/112","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Go语言","slug":"Go语言","permalink":"http://wht6.github.io/tags/Go%E8%AF%AD%E8%A8%80/"}]},{"title":"Go语言的字典和通道","slug":"字典的操作和约束","date":"2021-04-07T03:00:00.000Z","updated":"2022-04-10T09:44:50.679Z","comments":true,"path":"posts/69aa.html","link":"","permalink":"http://wht6.github.io/posts/69aa.html","excerpt":"","text":"字典的操作和约束字典（map）存储的不是单一值的集合，而是键值对的集合。 &nbsp; 在 Go 语言规范中，应该是为了避免歧义，他们将键值对换了一种称呼，叫做：“键 - 元素对”。&nbsp;Go 语言的字典类型其实是一个哈希表（hash table）的特定实现，在这个实现中，键和元素的最大不同在于，键的类型是受限的，而元素却可以是任意类型的。&nbsp;如果要探究限制的原因，我们就先要了解哈希表中最重要的一个过程：映射。&nbsp;你可以把键理解为元素的一个索引，我们可以在哈希表中通过键查找与它成对的那个元素。&nbsp;键和元素的这种对应关系，在数学里就被称为“映射”，这也是“map”这个词的本意，哈希表的映射过程就存在于对键 - 元素对的增、删、改、查的操作之中。&nbsp;123456789101112aMap := map[string]int&#123; &quot;one&quot;: 1, &quot;two&quot;: 2, &quot;three&quot;: 3,&#125;k := &quot;two&quot;v, ok := aMap[k]if ok &#123; fmt.Printf(&quot;The element of key %q: %d\\n&quot;, k, v)&#125; else &#123; fmt.Println(&quot;Not found!&quot;)&#125;&nbsp; 比如，我们要在哈希表中查找与某个键值对应的那个元素值，那么我们需要先把键值作为参数传给这个哈希表。 &nbsp;哈希表会先用哈希函数（hash function）把键值转换为哈希值。哈希值通常是一个无符号的整数。一个哈希表会持有一定数量的桶（bucket），我们也可以叫它哈希桶，这些哈希桶会均匀地储存其所属哈希表收纳的键 - 元素对。&nbsp;因此，哈希表会先用这个键哈希值的低几位去定位到一个哈希桶，然后再去这个哈希桶中，查找这个键。&nbsp;由于键 - 元素对总是被捆绑在一起存储的，所以一旦找到了键，就一定能找到对应的元素值。随后，哈希表就会把相应的元素值作为结果返回。&nbsp;映射过程的第一步就是：把键值转换为哈希值。&nbsp;在 Go 语言的字典中，每一个键值都是由它的哈希值代表的。也就是说，字典不会独立存储任何键的值，但会独立存储它们的哈希值。&nbsp;Go 语言字典的键类型不可以是函数类型、字典类型和切片类型。&nbsp;Go 语言规范规定，在键类型的值之间必须可以施加操作符==和!=。换句话说，键类型的值必须要支持判等操作。由于函数类型、字典类型和切片类型的值并不支持判等操作，所以字典的键类型不能是这些类型。&nbsp;另外，如果键的类型是接口类型的，那么键值的实际类型也不能是上述三种类型，否则在程序运行过程中会引发 panic（即运行时恐慌）。&nbsp;12345var badMap2 = map[interface&#123;&#125;]int&#123; &quot;1&quot;: 1, []int&#123;2&#125;: 2, // 这里会引发 panic。 3: 3,&#125;&nbsp; 这里的变量badMap2的类型是键类型为interface&#123;&#125;、值类型为int的字典类型。这样声明并不会引起什么错误。或者说，我通过这样的声明躲过了 Go 语言编译器的检查。 &nbsp;注意，我用字面量在声明该字典的同时对它进行了初始化，使它包含了三个键 - 元素对。其中第二个键 - 元素对的键值是[]int&#123;2&#125;，元素值是2。这样的键值也不会让 Go 语言编译器报错，因为从语法上说，这样做是可以的。&nbsp;但是，当我们运行这段代码的时候，Go 语言的运行时（runtime）系统就会发现这里的问题，它会抛出一个 panic，并把根源指向字面量中定义第二个键 - 元素对的那一行。我们越晚发现问题，修正问题的成本就会越高，所以最好不要把字典的键类型设定为任何接口类型。如果非要这么做，请一定确保代码在可控的范围之内。&nbsp;比如，由于类型[1][]string的元素类型是[]string，所以它就不能作为字典类型的键类型。另外，如果键的类型是结构体类型，那么还要保证其中字段的类型的合法性。无论不合法的类型被埋藏得有多深，比如map[[1][2][3][]string]int，Go 语言编译器都会把它揪出来。&nbsp;首先，每个哈希桶都会把自己包含的所有键的哈希值存起来。Go 语言会用被查找键的哈希值与这些哈希值逐个对比，看看是否有相等的。如果一个相等的都没有，那么就说明这个桶中没有要查找的键值，这时 Go 语言就会立刻返回结果了。&nbsp;如果有相等的，那就再用键值本身去对比一次。为什么还要对比？原因是，不同值的哈希值是可能相同的。这有个术语，叫做“哈希碰撞”。&nbsp;&nbsp;所以，即使哈希值一样，键值也不一定一样。如果键类型的值之间无法判断相等，那么此时这个映射的过程就没办法继续下去了。最后，只有键的哈希值和键值都相等，才能说明查找到了匹配的键 - 元素对。&nbsp;应该优先考虑哪些类型作为字典的键类型？&nbsp;这里先抛开我们使用字典时的上下文，只从性能的角度看。在前文所述的映射过程中，“把键值转换为哈希值”以及“把要查找的键值与哈希桶中的键值做对比”， 明显是两个重要且比较耗时的操作。&nbsp;因此，可以说，求哈希和判等操作的速度越快，对应的类型就越适合作为键类型。&nbsp;对于所有的基本类型、指针类型，以及数组类型、结构体类型和接口类型，Go 语言都有一套算法与之对应。这套算法中就包含了哈希和判等。以求哈希的操作为例，宽度越小的类型速度通常越快。对于布尔类型、整数类型、浮点数类型、复数类型和指针类型来说都是如此。对于字符串类型，由于它的宽度是不定的，所以要看它的值的具体长度，长度越短求哈希越快。&nbsp;类型的宽度是指它的单个值需要占用的字节数。比如，bool、int8和uint8类型的一个值需要占用的字节数都是1，因此这些类型的宽度就都是1。&nbsp;以上说的都是基本类型，再来看高级类型。对数组类型的值求哈希实际上是依次求得它的每个元素的哈希值并进行合并，所以速度就取决于它的元素类型以及它的长度。细则同上。&nbsp;与之类似，对结构体类型的值求哈希实际上就是对它的所有字段值求哈希并进行合并，所以关键在于它的各个字段的类型以及字段的数量。而对于接口类型，具体的哈希算法，则由值的实际类型决定。&nbsp;不建议你使用这些高级数据类型作为字典的键类型，不仅仅是因为对它们的值求哈希，以及判等的速度较慢，更是因为在它们的值中存在变数。&nbsp;那么，在那些基本类型中应该优先选择哪一个？答案是，优先选用数值类型和指针类型，通常情况下类型的宽度越小越好。如果非要选择字符串类型的话，最好对键值的长度进行额外的约束。&nbsp;Go 语言有时会对字典的增、删、改、查操作做一些优化。比如，在字典的键类型为字符串类型的情况下；又比如，在字典的键类型为宽度为4或8的整数类型的情况下。&nbsp;当我们仅声明而不初始化一个字典类型的变量的时候，它的值会是nil。&nbsp;除了添加键 - 元素对，我们在一个值为nil的字典上做任何操作都不会引起错误。当我们试图在一个值为nil的字典中添加键 - 元素对的时候，Go 语言的运行时系统就会立即抛出一个 panic。&nbsp;字典类型的值是并发安全的吗？&nbsp;字典类型的值不是并发安全的，即使我们只是增减其中的键值对也是如此。其根本原因是，字典值内部有时候会根据需要进行存储方面的调整。 &nbsp;作为 Go 语言最有特色的数据类型，通道（channel）完全可以与 goroutine（也可称为 go 程）并驾齐驱，共同代表 Go 语言独有的并发编程模式和编程哲学。 &nbsp; 通道通道的基础知识&nbsp;通道类型的值本身就是并发安全的，这也是 Go 语言自带的、唯一一个可以满足并发安全性的类型。&nbsp;在声明并初始化一个通道的时候，我们需要用到 Go 语言的内建函数make。就像用make初始化切片那样，我们传给这个函数的第一个参数应该是代表了通道的具体类型的类型字面量。&nbsp;在声明一个通道类型变量的时候，我们首先要确定该通道类型的元素类型，这决定了我们可以通过这个通道传递什么类型的数据。&nbsp;比如，类型字面量chan int，其中的chan是表示通道类型的关键字，而int则说明了该通道类型的元素类型。又比如，chan string代表了一个元素类型为string的通道类型。&nbsp;在初始化通道的时候，make函数除了必须接收这样的类型字面量作为参数，还可以接收一个int类型的参数用于表示该通道的容量。所谓通道的容量，就是指通道最多可以缓存多少个元素值。由此，虽然这个参数是int类型的，但是它是不能小于0的。&nbsp;当容量为0时，我们可以称通道为非缓冲通道，也就是不带缓冲的通道。而当容量大于0时，我们可以称为缓冲通道，也就是带有缓冲的通道。非缓冲通道和缓冲通道有着不同的数据传递方式。&nbsp;一个通道相当于一个先进先出（FIFO）的队列。也就是说，通道中的各个元素值都是严格地按照发送的顺序排列的，先被发送通道的元素值一定会先被接收。元素值的发送和接收都需要用到操作符&lt;-。我们也可以叫它接送操作符。一个左尖括号紧接着一个减号形象地代表了元素值的传输方向。&nbsp; 12345678910111213package main import &quot;fmt&quot; func main() &#123; ch1 := make(chan int, 3) ch1 &lt;- 2 ch1 &lt;- 1 ch1 &lt;- 3 elem1 := &lt;-ch1 fmt.Printf(&quot;The first element received from channel ch1: %v\\n&quot;, elem1)&#125; &nbsp; 这里声明并初始化了一个元素类型为int、容量为3的通道ch1，并用三条语句，向该通道先后发送了三个元素值2、1和3。 &nbsp;这里的语句需要这样写：依次敲入通道变量的名称（比如ch1）、接送操作符&lt;-以及想要发送的元素值（比如2），并且这三者之间最好用空格进行分割。&nbsp;这显然表达了“这个元素值将被发送该通道”这个语义。由于该通道的容量为 3，所以，我可以在通道不包含任何元素值的时候，连续地向该通道发送三个值，此时这三个值都会被缓存在通道之中。&nbsp;当我们需要从通道接收元素值的时候，同样要用接送操作符&lt;-，只不过，这时需要把它写在变量名的左边，用于表达“要从该通道接收一个元素值”的语义。&nbsp;比如：&lt;-ch1，这也可以被叫做接收表达式。在一般情况下，接收表达式的结果将会是通道中的一个元素值。&nbsp;如果我们需要把如此得来的元素值存起来，那么在接收表达式的左边就需要依次添加赋值符号（=或:=）和用于存值的变量的名字。因此，语句elem1 := &lt;-ch1会将最先进入ch1的元素2接收来并存入变量elem1。&nbsp; 通道的基本特性&nbsp;对通道的发送和接收操作的基本特性如下。&nbsp; 对于同一个通道，发送操作之间是互斥的，接收操作之间也是互斥的。 发送操作和接收操作中对元素值的处理都是不可分割的。 发送操作在完全完成之前会被阻塞。接收操作也是如此。&nbsp;第一个基本特性。 在同一时刻，Go 语言的运行时系统（以下简称运行时系统）只会执行对同一个通道的任意个发送操作中的某一个。直到这个元素值被完全复制进该通道之后，其他针对该通道的发送操作才可能被执行。&nbsp;类似的，在同一时刻，运行时系统也只会执行，对同一个通道的任意个接收操作中的某一个。直到这个元素值完全被移出该通道之后，其他针对该通道的接收操作才可能被执行。即使这些操作是并发执行的也是如此。&nbsp;这里所谓的并发执行，你可以这样认为，多个代码块分别在不同的 goroutine 之中，并有机会在同一个时间段内被执行。&nbsp;另外，对于通道中的同一个元素值来说，发送操作和接收操作之间也是互斥的。例如，虽然会出现，正在被复制进通道但还未复制完成的元素值，但是这时它绝不会被想接收它的一方看到和取走。&nbsp;这里要注意的一个细节是，元素值从外界进入通道时会被复制。更具体地说，进入通道的并不是在接收操作符右边的那个元素值，而是它的副本。&nbsp;另一方面，元素值从通道进入外界时会被移动。这个移动操作实际上包含了两步，第一步是生成正在通道中的这个元素值的副本，并准备给到接收方，第二步是删除在通道中的这个元素值。&nbsp;第二个基本特性。 这里的“不可分割”的意思是，它们处理元素值时都是一气呵成的，绝不会被打断。&nbsp;例如，发送操作要么还没复制元素值，要么已经复制完毕，绝不会出现只复制了一部分的情况。&nbsp;又例如，接收操作在准备好元素值的副本之后，一定会删除掉通道中的原值，绝不会出现通道中仍有残留的情况。&nbsp;这既是为了保证通道中元素值的完整性，也是为了保证通道操作的唯一性。对于通道中的同一个元素值来说，它只可能是某一个发送操作放入的，同时也只可能被某一个接收操作取出。&nbsp;第三个基本特性。 一般情况下，发送操作包括了“复制元素值”和“放置副本到通道内部”这两个步骤。&nbsp;在这两个步骤完全完成之前，发起这个发送操作的那句代码会一直阻塞在那里。也就是说，在它之后的代码不会有执行的机会，直到这句代码的阻塞解除。&nbsp;更细致地说，在通道完成发送操作之后，运行时系统会通知这句代码所在的 goroutine，以使它去争取继续运行代码的机会。&nbsp;另外，接收操作通常包含了“复制通道内的元素值”“放置副本到接收方”“删掉原值”三个步骤。&nbsp;在所有这些步骤完全完成之前，发起该操作的代码也会一直阻塞，直到该代码所在的 goroutine 收到了运行时系统的通知并重新获得运行机会为止。&nbsp;如此阻塞代码其实就是为了实现操作的互斥和元素值的完整。&nbsp; 扩展&nbsp;发送操作和接收操作在什么时候可能被长时间的阻塞？&nbsp;先说针对缓冲通道的情况。如果通道已满，那么对它的所有发送操作都会被阻塞，直到通道中有元素值被接收走。这时，通道会优先通知最早因此而等待的、那个发送操作所在的 goroutine，后者会再次执行发送操作。&nbsp;由于发送操作在这种情况下被阻塞后，它们所在的 goroutine 会顺序地进入通道内部的发送等待队列，所以通知的顺序总是公平的。&nbsp;相对的，如果通道已空，那么对它的所有接收操作都会被阻塞，直到通道中有新的元素值出现。这时，通道会通知最早等待的那个接收操作所在的 goroutine，并使它再次执行接收操作。&nbsp;因此而等待的、所有接收操作所在的 goroutine，都会按照先后顺序被放入通道内部的接收等待队列。&nbsp;对于非缓冲通道，情况要简单一些。无论是发送操作还是接收操作，一开始执行就会被阻塞，直到配对的操作也开始执行，才会继续传递。由此可见，非缓冲通道是在用同步的方式传递数据。也就是说，只有收发双方对接上了，数据才会被传递。&nbsp;并且，数据是直接从发送方复制到接收方的，中间并不会用非缓冲通道做中转。相比之下，缓冲通道则在用异步的方式传递数据。&nbsp;在大多数情况下，缓冲通道会作为收发双方的中间件。正如前文所述，元素值会先从发送方复制到缓冲通道，之后再由缓冲通道复制给接收方。但是，当发送操作在执行的时候发现空的通道中，正好有等待的接收操作，那么它会直接把元素值复制给接收方。&nbsp;特别说明一下，由于错误使用通道而造成的阻塞。&nbsp;对于值为nil的通道，不论它的具体类型是什么，对它的发送操作和接收操作都会永久地处于阻塞状态。它们所属的 goroutine 中的任何代码，都不再会被执行。&nbsp;注意，由于通道类型是引用类型，所以它的零值就是nil。换句话说，当我们只声明该类型的变量但没有用make函数对它进行初始化时，该变量的值就会是nil。我们一定不要忘记初始化通道！&nbsp;发送操作和接收操作在什么时候会引发 panic？&nbsp;对于一个已初始化，但并未关闭的通道来说，收发操作一定不会引发 panic。但是通道一旦关闭，再对它进行发送操作，就会引发 panic。&nbsp;另外，如果我们试图关闭一个已经关闭了的通道，也会引发 panic。注意，接收操作是可以感知到通道的关闭的，并能够安全退出。&nbsp;更具体地说，当我们把接收表达式的结果同时赋给两个变量时，第二个变量的类型就是一定bool类型。它的值如果为false就说明通道已经关闭，并且再没有元素值可取了。&nbsp;注意，如果通道关闭时，里面还有元素值未被取出，那么接收表达式的第一个结果，仍会是通道中的某一个元素值，而第二个结果值一定会是true。&nbsp;因此，通过接收表达式的第二个结果值，来判断通道是否关闭是可能有延时的。&nbsp;由于通道的收发操作有上述特性，所以除非有特殊的保障措施，我们千万不要让接收方关闭通道，而应当让发送方做这件事。 &nbsp; 通道的长度代表它当前包含的元素值的个数。当通道已满时，其长度会与容量相同。 &nbsp; 通道的进阶使用&nbsp; 单向通道&nbsp;所谓单向通道就是，只能发不能收，或者只能收不能发的通道。一个通道是双向的，还是单向的是由它的类型字面量体现的。&nbsp;还记得我们在上篇文章中说过的接收操作符&lt;-吗？如果我们把它用在通道的类型字面量中，那么它代表的就不是“发送”或“接收”的动作了，而是表示通道的方向。&nbsp; 1var uselessChan = make(chan&lt;- int, 1) &nbsp; uselessChan变量的类型是chan&lt;- int，容量是1。 &nbsp;请注意紧挨在关键字chan右边的那个&lt;-，这表示了这个通道是单向的，并且只能发而不能收。&nbsp;类似的，如果这个操作符紧挨在chan的左边，那么就说明该通道只能收不能发。所以，前者可以被简称为发送通道，后者可以被简称为接收通道。&nbsp;注意，与发送操作和接收操作对应，这里的“发”和“收”都是站在操作通道的代码的角度上说的。&nbsp;从上述变量的名字上你也能猜到，这样的通道是没用的。通道就是为了传递数据而存在的，声明一个只有一端（发送端或者接收端）能用的通道没有任何意义。那么，单向通道的用途究竟在哪儿呢？&nbsp;概括地说，单向通道最主要的用途就是约束其他代码的行为。&nbsp; 123func SendInt(ch chan&lt;- int) &#123; ch &lt;- rand.Intn(1000)&#125; &nbsp; 我用func关键字声明了一个叫做SendInt的函数。这个函数只接受一个chan&lt;- int类型的参数。在这个函数中的代码只能向参数ch发送元素值，而不能从它那里接收元素值。这就起到了约束函数行为的作用。 &nbsp;你可能会问，我自己写的函数自己肯定能确定操作通道的方式，为什么还要再约束？好吧，这个例子可能过于简单了。在实际场景中，这种约束一般会出现在接口类型声明中的某个方法定义上。请看这个叫Notifier的接口类型声明：&nbsp; 123type Notifier interface &#123; SendInt(ch chan&lt;- int)&#125; &nbsp; 在接口类型声明的花括号中，每一行都代表着一个方法的定义。接口中的方法定义与函数声明很类似，但是只包含了方法名称、参数列表和结果列表。 &nbsp;一个类型如果想成为一个接口类型的实现类型，那么就必须实现这个接口中定义的所有方法。因此，如果我们在某个方法的定义中使用了单向通道类型，那么就相当于在对它的所有实现做出约束。&nbsp;在这里，Notifier接口中的SendInt方法只会接受一个发送通道作为参数，所以，在该接口的所有实现类型中的SendInt方法都会受到限制。这种约束方式还是很有用的，尤其是在我们编写模板代码或者可扩展的程序库的时候。&nbsp;顺便说一下，我们在调用SendInt函数的时候，只需要把一个元素类型匹配的双向通道传给它就行了，没必要用发送通道，因为 Go 语言在这种情况下会自动地把双向通道转换为函数所需的单向通道。&nbsp; 12intChan1 := make(chan int, 3)SendInt(intChan1) &nbsp; 在另一个方面，我们还可以在函数声明的结果列表中使用单向通道。如下所示： &nbsp; 123456789func getIntChan() &lt;-chan int &#123; num := 5 ch := make(chan int, num) for i := 0; i &lt; num; i++ &#123; ch &lt;- i &#125; close(ch) return ch&#125; &nbsp; 函数getIntChan会返回一个&lt;-chan int类型的通道，这就意味着得到该通道的程序，只能从通道中接收元素值。这实际上就是对函数调用方的一种约束了。 &nbsp;另外，我们在 Go 语言中还可以声明函数类型，如果我们在函数类型中使用了单向通道，那么就相等于在约束所有实现了这个函数类型的函数。&nbsp; 1234intChan2 := getIntChan()for elem := range intChan2 &#123; fmt.Printf(&quot;The element in intChan2: %v\\n&quot;, elem)&#125; &nbsp; 我把调用getIntChan得到的结果值赋给了变量intChan2，然后用for语句循环地取出了该通道中的所有元素值，并打印出来。 &nbsp;这里的for语句也可以被称为带有range子句的for语句。它的用法我在后面讲for语句的时候专门说明。现在你只需要知道关于它的三件事。&nbsp; 一、这样一条for语句会不断地尝试从intChan2种取出元素值，即使intChan2被关闭，它也会在取出所有剩余的元素值之后再结束执行。 二、当intChan2中没有元素值时，它会被阻塞在有for关键字的那一行，直到有新的元素值可取。 三、假设intChan2的值为nil，那么它会被永远阻塞在有for关键字的那一行。 &nbsp; 这就是带range子句的for语句与通道的联用方式。不过，它是一种用途比较广泛的语句，还可以被用来从其他一些类型的值中获取元素。除此之外，Go 语言还有一种专门为了操作通道而存在的语句：select语句。&nbsp; select语句&nbsp; select语句与通道联用&nbsp;select语句只能与通道联用，它一般由若干个分支组成。每次执行这种语句的时候，一般只有一个分支中的代码会被运行。&nbsp;select语句的分支分为两种，一种叫做候选分支，另一种叫做默认分支。候选分支总是以关键字case开头，后跟一个case表达式和一个冒号，然后我们可以从下一行开始写入当分支被选中时需要执行的语句。&nbsp;默认分支其实就是 default case，因为，当且仅当没有候选分支被选中时它才会被执行，所以它以关键字default开头并直接后跟一个冒号。同样的，我们可以在default:的下一行写入要执行的语句。&nbsp;由于select语句是专为通道而设计的，所以每个case表达式中都只能包含操作通道的表达式，比如接收表达式。&nbsp;当然，如果我们需要把接收表达式的结果赋给变量的话，还可以把这里写成赋值语句或者短变量声明。下面展示一个简单的例子。&nbsp; 123456789101112131415161718192021// 准备好几个通道。intChannels := [3]chan int&#123; make(chan int, 1), make(chan int, 1), make(chan int, 1),&#125;// 随机选择一个通道，并向它发送元素值。index := rand.Intn(3)fmt.Printf(&quot;The index: %d\\n&quot;, index)intChannels[index] &lt;- index// 哪一个通道中有可取的元素值，哪个对应的分支就会被执行。select &#123;case &lt;-intChannels[0]: fmt.Println(&quot;The first candidate case is selected.&quot;)case &lt;-intChannels[1]: fmt.Println(&quot;The second candidate case is selected.&quot;)case elem := &lt;-intChannels[2]: fmt.Printf(&quot;The third candidate case is selected, the element is %d.\\n&quot;, elem)default: fmt.Println(&quot;No candidate case is selected!&quot;)&#125; &nbsp; 我先准备好了三个类型为chan int、容量为1的通道，并把它们存入了一个叫做intChannels的数组。 &nbsp;然后，我随机选择一个范围在 [0, 2] 的整数，把它作为索引在上述数组中选择一个通道，并向其中发送一个元素值。&nbsp;最后，我用一个包含了三个候选分支的select语句，分别尝试从上述三个通道中接收元素值，哪一个通道中有值，哪一个对应的候选分支就会被执行。后面还有一个默认分支，不过在这里它是不可能被选中的。&nbsp;在使用select语句的时候，我们首先需要注意下面几个事情。&nbsp; 如果像上述示例那样加入了默认分支，那么无论涉及通道操作的表达式是否有阻塞，select语句都不会被阻塞。如果那几个表达式都阻塞了，或者说都没有满足求值的条件，那么默认分支就会被选中并执行。 如果没有加入默认分支，那么一旦所有的case表达式都没有满足求值条件，那么select语句就会被阻塞。直到至少有一个case表达式满足条件为止。 还记得吗？我们可能会因为通道关闭了，而直接从通道接收到一个其元素类型的零值。所以，在很多时候，我们需要通过接收表达式的第二个结果值来判断通道是否已经关闭。一旦发现某个通道关闭了，我们就应该及时地屏蔽掉对应的分支或者采取其他措施。这对于程序逻辑和程序性能都是有好处的。 select语句只能对其中的每一个case表达式各求值一次。所以，如果我们想连续或定时地操作其中的通道的话，就往往需要通过在for语句中嵌入select语句的方式实现。但这时要注意，简单地在select语句的分支中使用break语句，只能结束当前的select语句的执行，而并不会对外层的for语句产生作用。这种错误的用法可能会让这个for语句无休止地运行下去。&nbsp; 12345678910111213intChan := make(chan int, 1)// 一秒后关闭通道。time.AfterFunc(time.Second, func() &#123; close(intChan)&#125;)select &#123;case _, ok := &lt;-intChan: if !ok &#123; fmt.Println(&quot;The candidate case is closed.&quot;) break &#125; fmt.Println(&quot;The candidate case is selected.&quot;)&#125; &nbsp; 我先声明并初始化了一个叫做intChan的通道，然后通过time包中的AfterFunc函数约定在一秒钟之后关闭该通道。 &nbsp;后面的select语句只有一个候选分支，我在其中利用接收表达式的第二个结果值对intChan通道是否已关闭做了判断，并在得到肯定结果后，通过break语句立即结束当前select语句的执行。&nbsp; select语句的分支选择规则&nbsp;规则如下面所示。&nbsp; 对于每一个case表达式，都至少会包含一个代表发送操作的发送表达式或者一个代表接收操作的接收表达式，同时也可能会包含其他的表达式。比如，如果case表达式是包含了接收表达式的短变量声明时，那么在赋值符号左边的就可以是一个或两个表达式，不过此处的表达式的结果必须是可以被赋值的。当这样的case表达式被求值时，它包含的多个表达式总会以从左到右的顺序被求值。 select语句包含的候选分支中的case表达式都会在该语句执行开始时先被求值，并且求值的顺序是依从代码编写的顺序从上到下的。结合上一条规则，在select语句开始执行时，排在最上边的候选分支中最左边的表达式会最先被求值，然后是它右边的表达式。仅当最上边的候选分支中的所有表达式都被求值完毕后，从上边数第二个候选分支中的表达式才会被求值，顺序同样是从左到右，然后是第三个候选分支、第四个候选分支，以此类推。 对于每一个case表达式，如果其中的发送表达式或者接收表达式在被求值时，相应的操作正处于阻塞状态，那么对该case表达式的求值就是不成功的。在这种情况下，我们可以说，这个case表达式所在的候选分支是不满足选择条件的。 仅当select语句中的所有case表达式都被求值完毕后，它才会开始选择候选分支。这时候，它只会挑选满足选择条件的候选分支执行。如果所有的候选分支都不满足选择条件，那么默认分支就会被执行。如果这时没有默认分支，那么select语句就会立即进入阻塞状态，直到至少有一个候选分支满足选择条件为止。一旦有一个候选分支满足选择条件，select语句（或者说它所在的 goroutine）就会被唤醒，这个候选分支就会被执行。 如果select语句发现同时有多个候选分支满足选择条件，那么它就会用一种伪随机的算法在这些分支中选择一个并执行。注意，即使select语句是在被唤醒时发现的这种情况，也会这样做。 一条select语句中只能够有一个默认分支。并且，默认分支只在无候选分支可选时才会被执行，这与它的编写位置无关。 select语句的每次执行，包括case表达式求值和分支选择，都是独立的。不过，至于它的执行是否是并发安全的，就要看其中的case表达式以及分支中，是否包含并发不安全的代码了。 &nbsp; &nbsp; 原文链接：https://time.geekbang.org/column/intro/112","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Go语言","slug":"Go语言","permalink":"http://wht6.github.io/tags/Go%E8%AF%AD%E8%A8%80/"}]},{"title":"Go的数组、切片和链表","slug":"Golang的数组和切片","date":"2021-04-04T09:00:00.000Z","updated":"2022-05-23T06:43:45.789Z","comments":true,"path":"posts/4dc9.html","link":"","permalink":"http://wht6.github.io/posts/4dc9.html","excerpt":"","text":"数组和切片数组（array）和切片（slice）的共同点是都属于集合类的类型，并且，它们的值也都可以用来存储某一种类型的值（或者说元素）。&nbsp;不过，它们最重要的不同是：数组类型的值的长度是固定的，而切片类型的值是可变长的。&nbsp;数组的长度在声明它的时候就必须给定，并且之后不会再改变。可以说，数组的长度是其类型的一部分。比如，[1]string和[2]string就是两个不同的数组类型。 &nbsp; 数组也就不能改变长度。数组之间的赋值是值的赋值，即当把一个数组作为参数传入函数的时候，传入的其实是该数组的副本，而不是它的指针。如果要使用指针，那么就需要用到slice类型了。 &nbsp;而切片的类型字面量中只有元素的类型，而没有长度。切片的长度可以自动地随着其中元素数量的增长而增长，但不会随着元素数量的减少而减小。&nbsp;&nbsp;我们其实可以把切片看做是对数组的一层简单的封装，因为在每个切片的底层数据结构中，一定会包含一个数组。数组可以被叫做切片的底层数组，而切片也可以被看作是对数组的某个连续片段的引用。&nbsp; 也正因为如此，Go 语言的切片类型属于引用类型，同属引用类型的还有字典类型、通道类型、函数类型等；而 Go 语言的数组类型则属于值类型，同属值类型的有基础数据类型以及结构体类型。 注意，Go 语言里不存在像 Java 等编程语言中令人困惑的“传值或传引用”问题。在 Go 语言中，我们判断所谓的“传值”或者“传引用”只要看被传递的值的类型就好了。如果传递的值是引用类型的，那么就是“传引用”。如果传递的值是值类型的，那么就是“传值”。从传递成本的角度讲，引用类型的值往往要比值类型的值低很多。（所以传递数组时传递的是数组的副本，传递slice时就是传递指针） 我们在数组和切片之上都可以应用索引表达式，得到的都会是某个元素。我们在它们之上也都可以应用切片表达式，也都会得到一个新的切片。 &nbsp;我们通过调用内建函数len，得到数组和切片的长度。通过调用内建函数cap，我们可以得到它们的容量。&nbsp;但要注意，数组的容量永远等于其长度，都是不可变的。切片的容量却不是这样，并且它的变化是有规律可寻的。&nbsp; 估算切片的长度和容量&nbsp;123456789101112131415package main import &quot;fmt&quot; func main() &#123; // 示例 1。 s1 := make([]int, 5) fmt.Printf(&quot;The length of s1: %d\\n&quot;, len(s1)) fmt.Printf(&quot;The capacity of s1: %d\\n&quot;, cap(s1)) fmt.Printf(&quot;The value of s1: %d\\n&quot;, s1) s2 := make([]int, 5, 8) fmt.Printf(&quot;The length of s2: %d\\n&quot;, len(s2)) fmt.Printf(&quot;The capacity of s2: %d\\n&quot;, cap(s2)) fmt.Printf(&quot;The value of s2: %d\\n&quot;, s2)&#125;&nbsp; 首先，我用内建函数make声明了一个[]int类型的变量s1。我传给make函数的第二个参数是5，从而指明了该切片的长度。我用几乎同样的方式声明了切片s2，只不过多传入了一个参数8以指明该切片的容量。 &nbsp;切片s1和s2的容量分别是5和8。当我们用make函数初始化切片时，如果不指明其容量，那么它就会和长度一致。如果在初始化时指明了容量，那么切片的实际容量也就是它了。&nbsp;可以把切片看做是对数组的一层简单的封装，因为在每个切片的底层数据结构中，一定会包含一个数组。数组可以被叫做切片的底层数组，而切片也可以被看作是对数组的某个连续片段的引用。切片的容量实际上代表了它的底层数组的长度。&nbsp;有一个窗口，你可以通过这个窗口看到一个数组，但是不一定能看到该数组中的所有元素，有时候只能看到连续的一部分元素。&nbsp;现在，这个数组就是切片s2的底层数组，而这个窗口就是切片s2本身。s2的长度实际上指明的就是这个窗口的宽度，决定了你透过s2，可以看到其底层数组中的哪几个连续的元素。&nbsp;由于s2的长度是5，所以你可以看到底层数组中的第 1 个元素到第 5 个元素，对应的底层数组的索引范围是 [0, 4]。s2中的索引从0到4所指向的元素恰恰就是其底层数组中索引从0到4代表的那 5 个元素。&nbsp;请记住，当我们用make函数或切片值字面量（比如[]int&#123;1, 2, 3&#125;）初始化一个切片时，该窗口最左边的那个小格子总是会对应其底层数组中的第 1 个元素。&nbsp;但是当我们通过切片表达式基于某个数组或切片生成新切片的时候，情况就变得复杂起来了。&nbsp; 12345s3 := []int&#123;1, 2, 3, 4, 5, 6, 7, 8&#125;s4 := s3[3:6]fmt.Printf(&quot;The length of s4: %d\\n&quot;, len(s4))fmt.Printf(&quot;The capacity of s4: %d\\n&quot;, cap(s4))fmt.Printf(&quot;The value of s4: %d\\n&quot;, s4) &nbsp; 切片s3中有 8 个元素，分别是从1到8的整数。s3的长度和容量都是8。然后，我用切片表达式s3[3:6]初始化了切片s4。问题是，这个s4的长度和容量分别是多少？ &nbsp;这并不难，用减法就可以搞定。首先你要知道，切片表达式中的方括号里的那两个整数都代表什么。我换一种表达方式你也许就清楚了，即：[3, 6)。&nbsp;这是数学中的区间表示法，常用于表示取值范围，我其实已经在本专栏用过好几次了。由此可知，[3:6]要表达的就是透过新窗口能看到的s3中元素的索引范围是从3到5（注意，不包括6）。&nbsp;这里的3可被称为起始索引，6可被称为结束索引。那么s4的长度就是6减去3，即3。因此可以说，s4中的索引从0到2指向的元素对应的是s3及其底层数组中索引从3到5的那 3 个元素。&nbsp;&nbsp;在前面说过，切片的容量代表了它的底层数组的长度，但这仅限于使用make函数或者切片值字面量初始化切片的情况。&nbsp;更通用的规则是：一个切片的容量可以被看作是透过这个窗口最多可以看到的底层数组中元素的个数。&nbsp;由于s4是通过在s3上施加切片操作得来的，所以s3的底层数组就是s4的底层数组。&nbsp;又因为，在底层数组不变的情况下，切片代表的窗口可以向右扩展，直至其底层数组的末尾。&nbsp;所以，s4的容量就是其底层数组的长度8, 减去上述切片表达式中的那个起始索引3，即5。&nbsp;注意，切片代表的窗口是无法向左扩展的。也就是说，我们永远无法透过s4看到s3中最左边的那 3 个元素。&nbsp;最后，顺便提一下把切片的窗口向右扩展到最大的方法。对于s4来说，切片表达式s4[0:cap(s4)]就可以做到。该表达式的结果值（即一个新的切片）会是[]int&#123;4, 5, 6, 7, 8&#125;，其长度和容量都是5。&nbsp;估算切片容量的增长&nbsp;一旦一个切片无法容纳更多的元素，Go 语言就会想办法扩容。但它并不会改变原来的切片，而是会生成一个容量更大的切片，然后将把原有的元素和新元素一并拷贝到新切片中。在一般的情况下，你可以简单地认为新切片的容量（以下简称新容量）将会是原切片容量（以下简称原容量）的 2 倍。&nbsp;但是，当原切片的长度（以下简称原长度）大于或等于1024时，Go 语言将会以原容量的1.25倍作为新容量的基准（以下新容量基准）。新容量基准会被调整（不断地与1.25相乘），直到结果不小于原长度与要追加的元素数量之和（以下简称新长度）。最终，新容量往往会比新长度大一些，当然，相等也是可能的。&nbsp;另外，如果我们一次追加的元素过多，以至于使新长度比原容量的 2 倍还要大，那么新容量就会以新长度为基准。注意，与前面那种情况一样，最终的新容量在很多时候都要比新容量基准更大一些。&nbsp;切片的底层数组什么时候会被替换？&nbsp;确切地说，一个切片的底层数组永远不会被替换。为什么？虽然在扩容的时候 Go 语言一定会生成新的底层数组，但是它也同时生成了新的切片。&nbsp;它只是把新的切片作为了新底层数组的窗口，而没有对原切片，及其底层数组做任何改动。&nbsp;请记住，在无需扩容时，append函数返回的是指向原底层数组的新切片，而在需要扩容时，append函数返回的是指向新底层数组的新切片。所以，严格来讲，“扩容”这个词用在这里虽然形象但并不合适。不过鉴于这种称呼已经用得很广泛了，我们也没必要另找新词了。&nbsp;只要新长度不会超过切片的原容量，那么使用append函数对其追加元素的时候就不会引起扩容。这只会使紧邻切片窗口右边的（底层数组中的）元素被新的元素替换掉。 &nbsp; container包中的容器-链表Go 语言的链表实现在标准库的container/list代码包中。这个代码包中有两个公开的程序实体——List和Element，List 实现了一个双向链表（以下简称链表），而 Element 则代表了链表中元素的结构。 &nbsp; 我们在这里用到了List的四种方法。&nbsp;MoveBefore方法和MoveAfter方法，它们分别用于把给定的元素移动到另一个元素的前面和后面。&nbsp;MoveToFront方法和MoveToBack方法，分别用于把给定的元素移动到链表的最前端和最后端。&nbsp;在这些方法中，“给定的元素”都是*Element类型的，*Element类型是Element类型的指针类型，*Element的值就是元素的指针。&nbsp; 12345func (l *List) MoveBefore(e, mark *Element)func (l *List) MoveAfter(e, mark *Element) func (l *List) MoveToFront(e *Element)func (l *List) MoveToBack(e *Element) &nbsp; 具体问题是，如果我们自己生成这样的值，然后把它作为“给定的元素”传给链表的方法，那么会发生什么？链表会接受它吗？ &nbsp;答案是不会接受，这些方法将不会对链表做出任何改动。因为我们自己生成的Element值并不在链表中，所以也就谈不上“在链表中移动元素”。更何况链表不允许我们把自己生成的Element值插入其中。&nbsp;在List包含的方法中，用于插入新元素的那些方法都只接受interface&#123;&#125;类型的值。这些方法在内部会使用Element值，包装接收到的新元素。&nbsp;这样做正是为了避免直接使用我们自己生成的元素，主要原因是避免链表的内部关联，遭到外界破坏，这对于链表本身以及我们这些使用者来说都是有益的。&nbsp;List的方法还有下面这几种：&nbsp;Front和Back方法分别用于获取链表中最前端和最后端的元素，InsertBefore和InsertAfter方法分别用于在指定的元素之前和之后插入新元素，PushFront和PushBack方法则分别用于在链表的最前端和最后端插入新元素。&nbsp; 12345678func (l *List) Front() *Elementfunc (l *List) Back() *Element func (l *List) InsertBefore(v interface&#123;&#125;, mark *Element) *Elementfunc (l *List) InsertAfter(v interface&#123;&#125;, mark *Element) *Element func (l *List) PushFront(v interface&#123;&#125;) *Elementfunc (l *List) PushBack(v interface&#123;&#125;) *Element &nbsp; 这些方法都会把一个Element值的指针作为结果返回，它们就是链表留给我们的安全“接口”。拿到这些内部元素的指针，我们就可以去调用前面提到的用于移动元素的方法了。 &nbsp;List和Element都是结构体类型。结构体类型有一个特点，那就是它们的零值都会是拥有特定结构，但是没有任何定制化内容的值，相当于一个空壳。值中的字段也都会被分别赋予各自类型的零值。&nbsp; 广义来讲，所谓的零值就是只做了声明，但还未做初始化的变量被给予的缺省值。每个类型的零值都会依据该类型的特性而被设定。 比如，经过语句var a [2]int声明的变量a的值，将会是一个包含了两个0的整数数组。又比如，经过语句var s []int声明的变量s的值将会是一个[]int类型的、值为nil的切片。 &nbsp; 那么经过语句var l list.List声明的变量l的值将会是什么呢？这个零值将会是一个长度为0的链表。这个链表持有的根元素也将会是一个空壳，其中只会包含缺省的内容。那这样的链表我们可以直接拿来使用吗？&nbsp; 答案是，可以的。这被称为“开箱即用”。Go 语言标准库中很多结构体类型的程序实体都做到了开箱即用。这也是在编写可供别人使用的代码包（或者说程序库）时，我们推荐遵循的最佳实践之一。那么，语句var l list.List声明的链表l可以直接使用，这是怎么做到的呢？&nbsp;关键在于它的“延迟初始化”机制。&nbsp;所谓的延迟初始化，你可以理解为把初始化操作延后，仅在实际需要的时候才进行。延迟初始化的优点在于“延后”，它可以分散初始化操作带来的计算量和存储空间消耗。&nbsp;例如，如果我们需要集中声明非常多的大容量切片的话，那么那时的 CPU 和内存空间的使用量肯定都会一个激增，并且只有设法让其中的切片及其底层数组被回收，内存使用量才会有所降低。&nbsp;如果数组是可以被延迟初始化的，那么计算量和存储空间的压力就可以被分散到实际使用它们的时候。这些数组被实际使用的时间越分散，延迟初始化带来的优势就会越明显。&nbsp; 实际上，Go 语言的切片就起到了延迟初始化其底层数组的作用。 延迟初始化的缺点恰恰也在于“延后”。你可以想象一下，如果我在调用链表的每个方法的时候，它们都需要先去判断链表是否已经被初始化，那这也会是一个计算量上的浪费。在这些方法被非常频繁地调用的情况下，这种浪费的影响就开始显现了，程序的性能将会降低。 &nbsp;又比如，在用于删除元素、移动元素，以及一些用于插入元素的方法中，只要判断一下传入的元素中指向所属链表的指针，是否与当前链表的指针相等就可以了。 &nbsp;如果不相等，就一定说明传入的元素不是这个链表中的，后续的操作就不用做了。反之，就一定说明这个链表已经被初始化了。&nbsp;原因在于，链表的PushFront方法、PushBack方法、PushBackList方法以及PushFrontList方法总会先判断链表的状态，并在必要时进行初始化，这就是延迟初始化。&nbsp;而且，我们在向一个空的链表中添加新元素的时候，肯定会调用这四个方法中的一个，这时新元素中指向所属链表的指针，一定会被设定为当前链表的指针。所以，指针相等是链表已经初始化的充分必要条件。&nbsp;明白了吗？List利用了自身以及Element在结构上的特点，巧妙地平衡了延迟初始化的优缺点，使得链表可以开箱即用，并且在性能上可以达到最优。&nbsp;补充：List这个结构体类型有两个字段，一个是Element类型的字段root，另一个是int类型的字段len。顾名思义，前者代表的就是那个根元素，而后者用于存储链表的长度。注意，它们都是包级私有的，也就是说使用者无法查看和修改它们。&nbsp;像前面那样声明的l，其字段root和len都会被赋予相应的零值。len的零值是0，正好可以表明该链表还未包含任何元素。由于root是Element类型的，所以它的零值就是该类型的空壳，用字面量表示的话就是Element&#123;&#125;。&nbsp;Element类型包含了几个包级私有的字段，分别用于存储前一个元素、后一个元素以及所属链表的指针值。另外还有一个名叫Value的公开的字段，该字段的作用就是持有元素的实际值，它是interface&#123;&#125;类型的。在Element类型的零值中，这些字段的值都会是nil。&nbsp; 循环链表&nbsp;container/ring包中的Ring类型实现的是一个循环链表，也就是我们俗称的环。其实List在内部就是一个循环链表。它的根元素永远不会持有任何实际的元素值，而该元素的存在就是为了连接这个循环链表的首尾两端。&nbsp;所以也可以说，List的零值是一个只包含了根元素，但不包含任何实际元素值的空链表。那么，既然Ring和List在本质上都是循环链表，那它们到底有什么不同呢？&nbsp;最主要的不同有下面几种。&nbsp; Ring类型的数据结构仅由它自身即可代表，而List类型则需要由它以及Element类型联合表示。这是表示方式上的不同，也是结构复杂度上的不同。 一个Ring类型的值严格来讲，只代表了其所属的循环链表中的一个元素，而一个List类型的值则代表了一个完整的链表。这是表示维度上的不同。 在创建并初始化一个Ring值的时候，我们可以指定它包含的元素的数量，但是对于一个List值来说却不能这样做（也没有必要这样做）。循环链表一旦被创建，其长度是不可变的。这是两个代码包中的New函数在功能上的不同，也是两个类型在初始化值方面的第一个不同。 仅通过var r ring.Ring语句声明的r将会是一个长度为1的循环链表，而List类型的零值则是一个长度为0的链表。别忘了List中的根元素不会持有实际元素值，因此计算长度时不会包含它。这是两个类型在初始化值方面的第二个不同。 Ring值的Len方法的算法复杂度是 O(N) 的，而List值的Len方法的算法复杂度则是 O(1) 的。这是两者在性能方面最显而易见的差别。 &nbsp;其他的不同基本上都是方法方面的了。比如，循环链表也有用于插入、移动或删除元素的方法，不过用起来都显得更抽象一些，等等。 &nbsp; 切片、数组和链表的比较&nbsp;切片本身有着占用内存少和创建便捷等特点，但它的本质上还是数组。切片的一大好处是可以让我们通过窗口快速地定位并获取，或者修改底层数组中的元素。&nbsp;不过，当我们想删除切片中的元素的时候就没那么简单了。元素复制一般是免不了的，就算只删除一个元素，有时也会造成大量元素的移动。这时还要注意空出的元素槽位的“清空”，否则很可能会造成内存泄漏。&nbsp;另一方面，在切片被频繁“扩容”的情况下，新的底层数组会不断产生，这时内存分配的量以及元素复制的次数可能就很可观了，这肯定会对程序的性能产生负面的影响。&nbsp;尤其是当我们没有一个合理、有效的”缩容“策略的时候，旧的底层数组无法被回收，新的底层数组中也会有大量无用的元素槽位。过度的内存浪费不但会降低程序的性能，还可能会使内存溢出并导致程序崩溃。&nbsp;由此可见，正确地使用切片是多么的重要。不过，一个更重要的事实是，任何数据结构都不是银弹。不是吗？数组的自身特点和适用场景都非常鲜明，切片也是一样。它们都是 Go 语言原生的数据结构，使用起来也都很方便. 不过，你的集合类工具箱中不应该只有它们。这就是我们使用链表的原因。&nbsp;不过，对比来看，一个链表所占用的内存空间，往往要比包含相同元素的数组所占内存大得多。这是由于链表的元素并不是连续存储的，所以相邻的元素之间需要互相保存对方的指针。不但如此，每个元素还要存有它所属链表的指针。&nbsp;有了这些关联，链表的结构反倒更简单了。它只持有头部元素（或称为根元素）基本上就可以了。当然了，为了防止不必要的遍历和计算，链表的长度记录在内也是必须的。&nbsp;&nbsp; 原文链接：https://time.geekbang.org/column/intro/112","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Go语言","slug":"Go语言","permalink":"http://wht6.github.io/tags/Go%E8%AF%AD%E8%A8%80/"}]},{"title":"Go语言基础知识","slug":"Go语言基础知识","date":"2021-04-01T00:00:00.000Z","updated":"2022-04-10T08:50:47.018Z","comments":true,"path":"posts/1094.html","link":"","permalink":"http://wht6.github.io/posts/1094.html","excerpt":"","text":"Go的环境与文件工作区与GOPATH&nbsp; 安装GO语言环境后存在三个环境变量： &nbsp; GOROOT：Go 语言安装根目录的路径，也就是 GO 语言的安装路径。 GOPATH：若干工作区目录的路径。是我们自己定义的工作空间。 GOBIN：GO 程序生成的可执行文件（executable file）的路径。 &nbsp; 你可以把 GOPATH 简单理解成 Go 语言的工作目录，它的值是一个目录的路径，也可以是多个目录路径，每个目录都代表 Go 语言的一个工作区（workspace）。 &nbsp; 我们需要利于这些工作区，去放置 Go 语言的源码文件（source file），以及安装（install）后的归档文件（archive file，也就是以“.a”为扩展名的文件）和可执行文件（executable file）。 &nbsp; Go源码的组织方式&nbsp; Go 语言的源码也是以代码包为基本组织单位的。在文件系统中，这些代码包其实是与目录一一对应的。由于目录可以有子目录，所以代码包也可以有子包。 &nbsp; 一个代码包中可以包含任意个以.go 为扩展名的源码文件，这些源码文件都需要被声明属于同一个代码包。 &nbsp; 代码包的名称一般会与源码文件所在的目录同名。如果不同名，那么在构建、安装的过程中会以代码包名称为准。 &nbsp; 每个代码包都会有导入路径。代码包的导入路径是其他代码在使用该包中的程序实体时，需要引入的路径。在实际使用程序实体之前，我们必须先导入其所在的代码包。具体的方式就是import该代码包的导入路径。就像这样： &nbsp; 1import &quot;github.com/labstack/echo&quot; &nbsp; 在工作区中，一个代码包的导入路径实际上就是从 src 子目录，到该包的实际存储位置的相对路径。 &nbsp; 所以说，Go 语言源码的组织方式就是以环境变量 GOPATH、工作区、src 目录和代码包为主线的。一般情况下，Go 语言的源码文件都需要被存放在环境变量 GOPATH 包含的某个工作区（目录）中的 src 目录下的某个代码包（目录）中。 &nbsp; 了解源码安装后的结果&nbsp; 在安装后如果产生了归档文件（以“.a”为扩展名的文件），就会放进该工作区的 pkg 子目录；如果产生了可执行文件，就可能会放进该工作区的 bin 子目录。 &nbsp; 归档文件存放的具体位置和规则&nbsp; 源码文件会以代码包的形式组织起来，一个代码包其实就对应一个目录。安装某个代码包而产生的归档文件是与这个代码包同名的。 &nbsp; 放置它的相对目录就是该代码包的导入路径的直接父级。比如，一个已存在的代码包的导入路径是 &nbsp; 1github.com/labstack/echo， &nbsp;那么执行命令&nbsp;1go install github.com/labstack/echo&nbsp;生成的归档文件的相对目录就是 github.com/labstack，文件名为 echo.a。 &nbsp; 顺便说一下，上面这个代码包导入路径还有另外一层含义，那就是：该代码包的源码文件存在于 GitHub 网站的 labstack 组的代码仓库 echo 中。 &nbsp; 再说回来，归档文件的相对目录与 pkg 目录之间还有一级目录，叫做平台相关目录。平台相关目录的名称是由 build（也称“构建”）的目标操作系统、下划线和目标计算架构的代号组成的。 &nbsp; 比如，构建某个代码包时的目标操作系统是 Linux，目标计算架构是 64 位的，那么对应的平台相关目录就是 linux_amd64。 &nbsp; 因此，上述代码包的归档文件就会被放置在当前工作区的子目录 pkg/linux_amd64/github.com/labstack 中。 &nbsp; &nbsp; 理解构建和安装Go程序的过程&nbsp; 构建使用命令go build，安装使用命令go install。构建和安装代码包的时候都会执行编译、打包等操作，并且，这些操作生成的任何文件都会先被保存到某个临时的目录中。 &nbsp; 如果构建的是库源码文件，那么操作后产生的结果文件只会存在于临时目录中。这里的构建的主要意义在于检查和验证。如果构建的是命令源码文件，那么操作的结果文件会被搬运到源码文件所在的目录中。 &nbsp; 安装操作会先执行构建，然后还会进行链接操作，并且把结果文件搬运到指定目录。进一步说，如果安装的是库源码文件，那么结果文件会被搬运到它所在工作区的 pkg 目录下的某个子目录中。如果安装的是命令源码文件，那么结果文件会被搬运到它所在工作区的 bin 目录中，或者环境变量GOBIN指向的目录中。 &nbsp; go build 命令一些可选项的用途和用法&nbsp; 在运行go build命令的时候，默认不会编译目标代码包所依赖的那些代码包。当然，如果被依赖的代码包的归档文件不存在，或者源码文件有了变化，那它还是会被编译。 &nbsp; 如果要强制编译它们，可以在执行命令的时候加入标记-a。此时，不但目标代码包总是会被编译，它依赖的代码包也总会被编译，即使依赖的是标准库中的代码包也是如此。 &nbsp; 另外，如果不但要编译依赖的代码包，还要安装它们的归档文件，那么可以加入标记-i。 &nbsp; 那么我们怎么确定哪些代码包被编译了呢？有两种方法。 &nbsp; 运行go build命令时加入标记-x，这样可以看到go build命令具体都执行了哪些操作。另外也可以加入标记-n，这样可以只查看具体操作而不执行它们。 运行go build命令时加入标记-v，这样可以看到go build命令编译的代码包的名称。它在与-a标记搭配使用时很有用。 &nbsp; 下面再说一说与 Go 源码的安装联系很紧密的一个命令：go get。 &nbsp; 命令go get会自动从一些主流公用代码仓库（比如 GitHub）下载目标代码包，并把它们安装到环境变量GOPATH包含的第 1 工作区的相应目录中。如果存在环境变量GOBIN，那么仅包含命令源码文件的代码包会被安装到GOBIN指向的那个目录。 &nbsp; 最常用的几个标记有下面几种。 &nbsp; -u：下载并安装代码包，不论工作区中是否已存在它们。 -d：只下载代码包，不安装代码包。 -fix：在下载代码包后先运行一个用于根据当前 Go 语言版本修正代码的工具，然后再安装代码包。 -t：同时下载测试所需的代码包。 -insecure：允许通过非安全的网络协议下载和安装代码包。HTTP 就是这样的协议。 &nbsp; 命令源码文件&nbsp; 命令源码文件是程序的运行入口，是每个可独立运行的程序必须拥有的。我们可以通过构建或安装，生成与其对应的可执行文件，后者一般会与该命令源码文件的直接父目录同名。 &nbsp; 如果一个源码文件声明属于main包，并且包含一个无参数声明且无结果声明的main函数，那么它就是命令源码文件。 &nbsp; 当需要模块化编程时，我们往往会将代码拆分到多个文件，甚至拆分到不同的代码包中。但无论怎样，对于一个独立的程序来说，命令源码文件永远只会也只能有一个。如果有与命令源码文件同包的源码文件，那么它们也应该声明属于main包。 &nbsp; 通过构建或安装命令源码文件，生成的可执行文件就可以被视为“命令”，既然是命令，那么就应该具备接收参数的能力。 &nbsp; 命令源码文件接收参数&nbsp; Go 语言标准库中有一个代码包专门用于接收和解析命令参数。这个代码包的名字叫flag。 &nbsp; 函数flag.StringVar接受 4 个参数。 第 1 个参数是用于存储该命令参数值的地址，具体到这里就是在前面声明的变量name的地址了，由表达式&amp;name表示。 第 2 个参数是为了指定该命令参数的名称，这里是name。 第 3 个参数是为了指定在未追加该命令参数时的默认值，这里是everyone。 至于第 4 个函数参数，即是该命令参数的简短说明了，这在打印命令说明时会用到。 &nbsp; 顺便说一下，还有一个与flag.StringVar函数类似的函数，叫flag.String。这两个函数的区别是，后者会直接返回一个已经分配好的用于存储命令参数值的地址。 &nbsp; 函数flag.Parse用于真正解析命令参数，并把它们的值赋给相应的变量。对该函数的调用必须在所有命令参数存储载体的声明（这里是对变量name的声明）和设置（对flag.StringVar函数的调用）之后，并且在读取任何命令参数值之前进行。 &nbsp; 123456789101112131415161718package mainimport ( &quot;flag&quot; &quot;fmt&quot;)var name stringfunc init() &#123; flag.StringVar(&amp;name, &quot;name&quot;, &quot;everyone&quot;, &quot;The greeting object.&quot;) //var name = flag.String(&quot;name&quot;, &quot;everyone&quot;, &quot;The greeting object.&quot;)&#125;func main() &#123; flag.Parse() fmt.Printf(&quot;Hello, %s!\\n&quot;, name)&#125; &nbsp; 运行下面的命令为name传值： &nbsp; 1$ go run demo.go -name=&quot;Robert&quot; &nbsp; 运行下面的命令查看该命令源码文件的参数说明： &nbsp; 1$ go run demo.go --help &nbsp; 其中的$表示我们是在命令提示符后运行go run命令的。运行后输出的内容会类似： &nbsp; 1234Usage of &#x2F;var&#x2F;folders&#x2F;ts&#x2F;7lg_tl_x2gd_k1lm5g_48c7w0000gn&#x2F;T&#x2F;go-build155438482&#x2F;b001&#x2F;exe&#x2F;demo: -name string The greeting object. (default &quot;everyone&quot;)exit status 2 &nbsp; 其中第一行是go run命令构建上述命令源码文件时临时生成的可执行文件的完整路径。 &nbsp; 如果我们先构建这个命令源码文件再运行生成的可执行文件，像这样： &nbsp; 12$ go build demo.go$ ./demo2 --help &nbsp; 那么输出就会是 &nbsp; 123Usage of .&#x2F;demo: -name string The greeting object. (default &quot;everyone&quot;) &nbsp; 自定义命令源码文件的参数使用说明&nbsp; 方法一：对变量flag.Usage重新赋值 &nbsp; 1234flag.Usage = func() &#123; fmt.Fprintf(os.Stderr, &quot;Usage of %s:\\n&quot;, &quot;question&quot;) flag.PrintDefaults()&#125; &nbsp; 方法二：对flag.CommandLine重新赋值，我们可以更深层次地定制当前命令源码文件的参数使用说明。（flag.CommandLine相当于默认情况下的命令参数容器。） &nbsp; 12345flag.CommandLine = flag.NewFlagSet(&quot;&quot;, flag.ExitOnError)flag.CommandLine.Usage = func() &#123; fmt.Fprintf(os.Stderr, &quot;Usage of %s:\\n&quot;, &quot;question&quot;) flag.PrintDefaults()&#125; &nbsp; 方法三：自己创建一个私有的命令参数容器。 &nbsp; 1var cmdLine = flag.NewFlagSet(&quot;question&quot;, flag.ExitOnError) &nbsp; 库源码文件&nbsp; 库源码文件是不能被直接运行的源码文件，它仅用于存放程序实体，这些程序实体可以被其他代码使用（只要遵从 Go 语言规范的话）。 &nbsp; 在 Go 语言中，程序实体是变量、常量、函数、结构体和接口的统称。 &nbsp; 库源码文件：demo4_lib.go &nbsp; 1234567package mainimport &quot;fmt&quot; func hello(name string) &#123; fmt.Printf(&quot;Hello, %s!\\n&quot;, name)&#125; &nbsp; 命令源码文件：demo4.go &nbsp; 12345678910111213141516package main import ( &quot;flag&quot;) var name string func init() &#123; flag.StringVar(&amp;name, &quot;name&quot;, &quot;everyone&quot;, &quot;The greeting object.&quot;)&#125; func main() &#123; flag.Parse() hello(name)&#125; &nbsp; 在同一个目录下的源码文件都需要被声明为属于同一个代码包。两个源文件的代码包声明语句都是package main。 &nbsp; 直接运行 &nbsp; 1$ go run demo4.go demo4_lib.go &nbsp; 或者，像下面这样先构建当前的代码包再运行。 &nbsp; 12$ go build puzzlers&#x2F;article3&#x2F;q1$ .&#x2F;q1 &nbsp; 如果构建失败，显示package不在GOROOT，可以运行go env -w GO111MODULE=off把go mod关闭。（编译器没有去gopath下找包，原因是GO111MODULE没有关， gomod 和 gopath 两个包管理方案，并且相互不兼容，在 gopath 查找包，按照 goroot 和多 gopath 目录下 src/xxx 依次查找。在 gomod 下查找包，解析 go.mod 文件查找包，mod 包名就是包的前缀，里面的目录就后续路径了。在 gomod 模式下，查找包就不会去 gopath 查找，只是 gomod 包缓存在 gopath/pkg/mod 里面。） &nbsp; 注意，demo4.go 和 demo4_lib.go 都声明自己属于main包。源码文件声明的包名可以与其所在目录的名称不同，只要这些文件声明的包名一致就可以。 &nbsp; demo4.go 另存为 demo5.go，并放到一个相对路径为puzzlers/article3/q2的目录中。再创建一个相对路径为puzzlers/article3/q2/lib的目录，再把 demo4_lib.go 复制一份并改名为 demo5_lib.go 放到该目录中。对 demo5_lib.go 文件进行修改。（为了不让该代码包的使用者产生困惑，我们总是应该让声明的包名与其父目录的名称一致。） &nbsp; 1234567package lib import &quot;fmt&quot; func Hello(name string) &#123; fmt.Printf(&quot;Hello, %s!\\n&quot;, name)&#125; &nbsp; （这里Hello首字母大写的原因：名称的首字母为大写的程序实体才可以被当前包外的代码引用，否则它就只能被当前包内的其他代码引用。通过名称，Go 语言自然地把程序实体的访问权限划分为了包级私有的和公开的。对于包级私有的程序实体，即使你导入了它所在的代码包也无法引用到它。） &nbsp; 我们在构建或者安装这个代码包的时候，提供给go命令的路径应该是目录的相对路径，就像这样： &nbsp; 1$ go install puzzlers&#x2F;article3&#x2F;q2&#x2F;lib &nbsp; 该命令会成功完成。之后，当前工作区的 pkg 子目录下会产生相应的归档文件，具体的相对路径是pkg/windows_amd64/puzzlers/article3/q2/lib.a &nbsp; demo5.go使用lib代码包的方法：导入puzzlers/article3/q2/lib，然后调用lib.Hello函数。 &nbsp; 如果你需要导入两个代码包，而这两个代码包的导入路径的最后一级是相同的，比如：dep/lib/flag和flag，那么会产生冲突。因为代表两个代码包的标识符重复了，都是flag。 &nbsp; 怎样解决这种冲突？导入代码包的时候给它起一个别名就可以了，比如： import libflag “dep/lib/flag”。或者，以本地化的方式导入代码包，如：import . “dep/lib/flag”。 &nbsp; Go 语言中的程序实体包括变量、常量、函数、结构体和接口。 Go 语言是静态类型的编程语言，所以我们在声明变量或常量的时候，都需要指定它们的类型，或者给予足够的信息，这样才可以让 Go 语言能够推导出它们的类型。&nbsp;在 Go 语言中，变量的类型可以是其预定义的那些类型，也可以是程序自定义的函数、结构体或接口。常量的合法类型不多，只能是那些 Go 语言预定义的基本类型。它的声明方式也更简单一些。&nbsp; GO的程序实体变量&nbsp; 声明变量的方式&nbsp;方式一：&nbsp; 12345678910111213package main import ( &quot;flag&quot; &quot;fmt&quot;) func main() &#123; var name string // [1] flag.StringVar(&amp;name, &quot;name&quot;, &quot;everyone&quot;, &quot;The greeting object.&quot;) // [2] flag.Parse() fmt.Printf(&quot;Hello, %v!\\n&quot;, name)&#125; &nbsp;方式二：[1]和[2]处代码合并:&nbsp; 1var name = flag.String(&quot;name&quot;, &quot;everyone&quot;, &quot;The greeting object.&quot;) &nbsp; 注意，flag.String函数返回的结果值的类型是*string而不是string。类型*string代表的是字符串的指针类型，而不是字符串类型。因此，这里的变量name代表的是一个指向字符串值的指针。 &nbsp; 我们可以通过操作符*把这个指针指向的字符串值取出来了。因此，在这种情况下，那个被用来打印内容的函数调用就需要微调一下，把其中的参数name改为*name，即：fmt.Printf(&quot;Hello, %v!\\n&quot;, *name)。 &nbsp; 方式三： &nbsp; 1name := flag.String(&quot;name&quot;, &quot;everyone&quot;, &quot;The greeting object.&quot;) &nbsp; Go的类型推断&nbsp;第二种方式在声明变量name的同时，还为它赋了值，而这时声明中并没有显式指定name的类型。之前的变量声明语句是var name string。这里利用了 Go 语言自身的类型推断，而省去了对该变量的类型的声明。&nbsp;类型推断是一种编程语言在编译期自动解释表达式类型的能力。表达式类型就是对表达式进行求值后得到结果的类型。&nbsp;第三种类型推断的方式是短变量声明。我们只能在函数体内部使用短变量声明来代替var声明。在编写if、for或switch语句的时候，我们经常把它安插在初始化子句中，并用来声明一些临时的变量。而相比之下，第二种方式更加通用，它可以被用在任何地方。&nbsp;&nbsp;类型推断的一个好处是方便进行代码重构：&nbsp; 12345678910111213141516package main import ( &quot;flag&quot; &quot;fmt&quot;) func main() &#123; var name = getTheFlag() flag.Parse() fmt.Printf(&quot;Hello, %v!\\n&quot;, *name)&#125; func getTheFlag() *string &#123; return flag.String(&quot;name&quot;, &quot;everyone&quot;, &quot;The greeting object.&quot;)&#125; &nbsp;我们可以用getTheFlag函数包裹（或者说包装）那个对flag.String函数的调用，并把其结果直接作为getTheFlag函数的结果，结果的类型是*string。 &nbsp; 这样一来，var name =右边的表达式，可以变为针对getTheFlag函数的调用表达式了。这实际上是对“声明并赋值name变量的那行代码”的重构。 &nbsp; 我们通常把不改变某个程序与外界的任何交互方式和规则，而只改变其内部实现”的代码修改方式，叫做对该程序的重构。重构的对象可以是一行代码、一个函数、一个功能模块，甚至一个软件系统。 &nbsp;我们不显式地指定变量name的类型，使得它可以被赋予任何类型的值。也就是说，变量name的类型可以在其初始化时，由其他程序动态地确定。&nbsp;在你改变getTheFlag函数的结果类型之后，Go 语言的编译器会在你再次构建该程序的时候，自动地更新变量name的类型。如果你使用过Python或Ruby这种动态类型的编程语言的话，一定会觉得这情景似曾相识。&nbsp;没错，通过这种类型推断，你可以体验到动态类型编程语言所带来的一部分优势，即程序灵活性的明显提升。但在那些编程语言中，这种提升可以说是用程序的可维护性和运行效率换来的。&nbsp;Go 语言是静态类型的，所以一旦在初始化变量时确定了它的类型，之后就不可能再改变。这就避免了在后面维护程序时的一些问题。另外，请记住，这种类型的确定是在编译期完成的，因此不会对程序的运行效率产生任何影响。&nbsp;Go 语言的类型推断可以明显提升程序的灵活性，使得代码重构变得更加容易，同时又不会给代码的维护带来额外负担（实际上，它恰恰可以避免散弹式的代码修改），更不会损失程序的运行效率。&nbsp;Go 语言的类型推断只应用在了对变量或常量的初始化方面。&nbsp; 变量的重声明&nbsp;这涉及了短变量声明。通过使用它，我们可以对同一个代码块中的变量进行重声明。&nbsp; 在 Go 语言中，代码块一般就是一个由花括号括起来的区域，里面可以包含表达式和语句。Go 语言本身以及我们编写的代码共同形成了一个非常大的代码块，也叫全域代码块。 这主要体现在，只要是公开的全局变量，都可以被任何代码所使用。相对小一些的代码块是代码包，一个代码包可以包含许多子代码包，所以这样的代码块也可以很大。 接下来，每个源码文件也都是一个代码块，每个函数也是一个代码块，每个if语句、for语句、switch语句和select语句都是一个代码块。甚至，switch或select语句中的case子句也都是独立的代码块。 走个极端，我就在main函数中写一对紧挨着的花括号算不算一个代码块？当然也算，这甚至还有个名词，叫“空代码块”。 &nbsp;变量重声明的含义是对已经声明过的变量再次声明。变量重声明的前提条件如下。&nbsp; 由于变量的类型在其初始化时就已经确定了，所以对它再次声明时赋予的类型必须与其原本的类型相同，否则会产生编译错误。 变量的重声明只可能发生在某一个代码块中。如果与当前的变量重名的是外层代码块中的变量，那么就是另外一种含义了，我在下一篇文章中会讲到。 变量的重声明只有在使用短变量声明时才会发生，否则也无法通过编译。如果要在此处声明全新的变量，那么就应该使用包含关键字var的声明语句，但是这时就不能与同一个代码块中的任何变量有重名了。 被“声明并赋值”的变量必须是多个，并且其中至少有一个是新的变量。这时我们才可以说对其中的旧变量进行了重声明。&nbsp;变量重声明其实算是一个语法糖（或者叫便利措施）。它允许我们在使用短变量声明时不用理会被赋值的多个变量中是否包含旧变量。&nbsp; 12var err errorn, err := io.WriteString(os.Stdout, &quot;Hello, everyone!\\n&quot;) &nbsp;我使用短变量声明对新变量n和旧变量err进行了“声明并赋值”，这时也是对后者的重声明。&nbsp; 变量的作用域&nbsp;一个代码块可以有若干个子代码块；但对于每个代码块，最多只会有一个直接包含它的代码块（后者可以简称为前者的外层代码块）。这种代码块的划分，也间接地决定了程序实体的作用域。&nbsp;一个程序实体被创造出来，是为了让别的代码引用的。那么，哪里的代码可以引用它呢，这就涉及了它的作用域。&nbsp;程序实体的访问权限有三种：包级私有的、模块级私有的和公开的。这其实就是 Go 语言在语言层面，依据代码块对程序实体作用域进行的定义。&nbsp;一个程序实体的作用域总是会被限制在某个代码块中，而这个作用域最大的用处，就是对程序实体的访问权限的控制。对“高内聚，低耦合”这种程序设计思想的实践，恰恰可以从这里开始。&nbsp; 1234567891011121314package main import &quot;fmt&quot; var block = &quot;package&quot; func main() &#123; block := &quot;function&quot; &#123; block := &quot;inner&quot; fmt.Printf(&quot;The block is %s.\\n&quot;, block) &#125; fmt.Printf(&quot;The block is %s.\\n&quot;, block)&#125; &nbsp;这个命令源码文件中有四个代码块，它们是：全域代码块、main包代表的代码块、main函数代表的代码块，以及在main函数中的一个用花括号包起来的代码块。&nbsp;运行后打印出的内容是：&nbsp; 12The block is inner.The block is function. &nbsp;你可能会认为它无法通过编译，因为三处代码都声明了相同名称的变量。的确，声明重名的变量是无法通过编译的，用短变量声明对已有变量进行重声明除外，但这只是对于同一个代码块而言的。&nbsp;对于不同的代码块来说，其中的变量重名没什么大不了，照样可以通过编译。即使这些代码块有直接的嵌套关系也是如此，就像 demo10.go 中的main包代码块、main函数代码块和那个最内层的代码块那样。&nbsp;这样规定显然很方便也很合理，否则我们会每天为了选择变量名而烦恼。但是这会导致另外一个问题，我引用变量时到底用的是哪一个？&nbsp;这其实有一个很有画面感的查找过程。这个查找过程不只针对于变量，还适用于任何程序实体。如下面所示。&nbsp; 首先，代码引用变量的时候总会最优先查找当前代码块中的那个变量。注意，这里的“当前代码块”仅仅是引用变量的代码所在的那个代码块，并不包含任何子代码块。 其次，如果当前代码块中没有声明以此为名的变量，那么程序会沿着代码块的嵌套关系，从直接包含当前代码块的那个代码块开始，一层一层地查找。 一般情况下，程序会一直查到当前代码包代表的代码块。如果仍然找不到，那么 Go 语言的编译器就会报错了。 &nbsp;如果我们在当前源码文件中导入了其他代码包，那么引用其中的程序实体时，是需要以限定符为前缀的。所以程序在找代表变量未加限定符的名字（即标识符）的时候，是不会去被导入的代码包中查找的。&nbsp; 但有个特殊情况，如果我们把代码包导入语句写成import . XXX的形式（注意中间的那个“.”），那么就会让这个“XXX”包中公开的程序实体，被当前源码文件中的代码，视为当前代码包中的程序实体。 比如，如果有代码包导入语句import . fmt，那么我们在当前源码文件中引用fmt.Printf函数的时候直接用Printf就可以了。在这个特殊情况下，程序在查找当前源码文件后会先去查用这种方式导入的那些代码包。 &nbsp;不同代码块中的重名变量与变量重声明中的变量区别：&nbsp;(为了方便描述，就把不同代码块中的重名变量叫做“可重名变量”吧。注意，在同一个代码块中不允许出现重名的变量，这违背了 Go 语言的语法。)&nbsp; 变量重声明中的变量一定是在某一个代码块内的。注意，这里的“某一个代码块内”并不包含它的任何子代码块，否则就变成了“多个代码块之间”。而可重名变量指的正是在多个代码块之间由相同的标识符代表的变量。 变量重声明是对同一个变量的多次声明，这里的变量只有一个。而可重名变量中涉及的变量肯定是有多个的。 不论对变量重声明多少次，其类型必须始终一致，具体遵从它第一次被声明时给定的类型。而可重名变量之间不存在类似的限制，它们的类型可以是任意的。 如果可重名变量所在的代码块之间，存在直接或间接的嵌套关系，那么它们之间一定会存在“屏蔽”的现象。但是这种现象绝对不会在变量重声明的场景下出现。 &nbsp;&nbsp;下面看一种特殊情况：&nbsp; 12345678910package main import &quot;fmt&quot; var container = []string&#123;&quot;zero&quot;, &quot;one&quot;, &quot;two&quot;&#125; func main() &#123; container := map[int]string&#123;0: &quot;zero&quot;, 1: &quot;one&quot;, 2: &quot;two&quot;&#125; fmt.Printf(&quot;The element is %q.\\n&quot;, container[1])&#125; &nbsp; 这里有两个都叫做container的变量，分别位于main包代码块和main函数代码块。main包代码块中的变量是切片（slice）类型的，另一个是字典（map）类型的。在main函数的最后，我试图打印出container变量的值中索引为1的那个元素。&nbsp;如果你熟悉这两个类型肯定会知道，在它们的值上我们都可以施加索引表达式，比如container[0]。只要中括号里的整数在有效范围之内（这里是 [0, 2]），它就可以把值中的某一个元素取出来。&nbsp;如果container的类型不是数组、切片或字典类型，那么索引表达式就会引发编译错误。这正是利用 Go 语言语法，帮我们约束程序的一个例子；但是当我们想知道 container 确切类型的时候，利用索引表达式的方式就不够了。&nbsp;当可重名变量的值被转换成某个接口类型值，或者它们的类型本身就是接口类型的时候，严格的类型检查就很有必要了。&nbsp; 变量的类型&nbsp; 类型断言&nbsp;接着上一段代码，怎么判断变量container的类型？&nbsp;答案是使用“类型断言”表达式。&nbsp; 1value, ok := interface&#123;&#125;(container).([]string) &nbsp;在赋值符号的右边，是一个类型断言表达式。 &nbsp;它包括了用来把container变量的值转换为空接口值的interface&#123;&#125;(container)。以及一个用于判断前者的类型是否为切片类型 []string 的 .([]string)。&nbsp;这个表达式的结果可以被赋给两个变量，在这里由value和ok代表。变量ok是布尔（bool）类型的，它将代表类型判断的结果，true或false。如果是true，那么被判断的值将会被自动转换为[]string类型的值，并赋给变量value，否则value将被赋予nil（即“空”）。&nbsp;顺便提一下，这里的ok也可以没有。也就是说，类型断言表达式的结果，可以只被赋给一个变量，在这里是value。&nbsp;但是这样的话，当判断为否时就会引发异常。这种异常在 Go 语言中被叫做panic，我把它翻译为运行时恐慌。因为它是一种在 Go 程序运行期间才会被抛出的异常，而“恐慌”二字是英文 Panic 的中文直译。除非显式地“恢复”这种“恐慌”，否则它会使 Go 程序崩溃并停止。所以，在一般情况下，我们还是应该使用带ok变量的写法。&nbsp;类型断言表达式的语法形式是x.(T)。其中的x代表要被判断类型的值。这个值当下的类型必须是接口类型的，不过具体是哪个接口类型其实是无所谓的。所以，当这里的container变量类型不是任何的接口类型时，我们就需要先把它转成某个接口类型的值。&nbsp;如果container是某个接口类型的，那么这个类型断言表达式就可以是container.([]string)。这样看是不是清晰一些了？&nbsp;在 Go 语言中，interface&#123;&#125;代表空接口，任何类型都是它的实现类型。任何类型的值都可以很方便地被转换成空接口的值就行了。&nbsp;这里的具体语法是interface&#123;&#125;(x)，例如前面展示的interface&#123;&#125;(container)。&nbsp;你可能会对这里的&#123;&#125;产生疑惑，为什么在关键字interface的右边还要加上这个东西？&nbsp;请记住，一对不包裹任何东西的花括号，除了可以代表空的代码块之外，还可以用于表示不包含任何内容的数据结构（或者说数据类型）。&nbsp;比如你今后肯定会遇到的struct&#123;&#125;，它就代表了不包含任何字段和方法的、空的结构体类型。&nbsp;而空接口interface&#123;&#125;则代表了不包含任何方法定义的、空的接口类型。&nbsp;当然了，对于一些集合类的数据类型来说，&#123;&#125;还可以用来表示其值不包含任何元素，比如空的切片值[]string&#123;&#125;，以及空的字典值map[int]string&#123;&#125;。&nbsp;&nbsp;我们再向答案的最右边看。圆括号中[]string是一个类型字面量。所谓类型字面量，就是用来表示数据类型本身的若干个字符。&nbsp;比如，string是表示字符串类型的字面量，uint8是表示 8 位无符号整数类型的字面量。&nbsp;再复杂一些的就是我们刚才提到的[]string，用来表示元素类型为string的切片类型，以及map[int]string，用来表示键类型为int、值类型为string的字典类型。还有更复杂的结构体类型字面量、接口类型字面量，等等。&nbsp; 类型转换&nbsp;类型转换的语法形式是T(x)。其中的x可以是一个变量，也可以是一个代表值的字面量（比如1.23和struct&#123;&#125;），还可以是一个表达式。&nbsp;注意，如果是表达式，那么该表达式的结果只能是一个值，而不能是多个值。在这个上下文中，x可以被叫做源值，它的类型就是源类型，而那个T代表的类型就是目标类型。&nbsp;对于整数类型值、整数常量之间的类型转换，原则上只要源值在目标类型的可表示范围内就是合法的。&nbsp;比如，之所以uint8(255)可以把无类型的常量255转换为uint8类型的值，是因为255在 [0, 255] 的范围内。&nbsp;但需要特别注意的是，源整数类型的可表示范围较大，而目标类型的可表示范围较小的情况，比如把值的类型从int16转换为int8。请看下面这段代码：&nbsp; 12var srcInt = int16(-255)dstInt := int8(srcInt) &nbsp;变量srcInt的值是int16类型的-255，而变量dstInt的值是由前者转换而来的，类型是int8。int16类型的可表示范围可比int8类型大了不少。问题是，dstInt的值是多少？ &nbsp;首先你要知道，整数在 Go 语言以及计算机中都是以补码的形式存储的。这主要是为了简化计算机对整数的运算过程。补码其实就是原码各位求反再加 1。&nbsp;比如，int16类型的值-255的补码是1111111100000001。如果我们把该值转换为int8类型的值，那么 Go 语言会把在较高位置（或者说最左边位置）上的 8 位二进制数直接截掉，从而得到00000001。&nbsp;又由于其最左边一位是0，表示它是个正整数，以及正整数的补码就等于其原码，所以dstInt的值就是1。&nbsp;一定要记住，当整数值的类型的有效范围由宽变窄时，只需在补码形式下截掉一定数量的高位二进制数即可。&nbsp;类似的快刀斩乱麻规则还有：当把一个浮点数类型的值转换为整数类型值时，前者的小数部分会被全部截掉。&nbsp;虽然直接把一个整数值转换为一个string类型的值是可行的，但值得关注的是，被转换的整数值应该可以代表一个有效的 Unicode 代码点，否则转换的结果将会是&quot;�&quot;（仅由高亮的问号组成的字符串值）。&nbsp;字符&#39;�&#39;的 Unicode 代码点是U+FFFD。它是 Unicode 标准中定义的 Replacement Character，专用于替换那些未知的、不被认可的以及无法展示的字符。&nbsp; 1string(-1) &nbsp;由于-1肯定无法代表一个有效的 Unicode 代码点，所以得到的总会是&quot;�&quot;。在实际工作中，我们在排查问题时可能会遇到�，你需要知道这可能是由于什么引起的。 &nbsp;string类型与各种切片类型之间的互转的。&nbsp;一个值在从string类型向[]byte类型转换时代表着以 UTF-8 编码的字符串会被拆分成零散、独立的字节。&nbsp;除了与 ASCII 编码兼容的那部分字符集，以 UTF-8 编码的某个单一字节是无法代表一个字符的。&nbsp; 1string([]byte&#123;&#x27;\\xe4&#x27;, &#x27;\\xbd&#x27;, &#x27;\\xa0&#x27;, &#x27;\\xe5&#x27;, &#x27;\\xa5&#x27;, &#x27;\\xbd&#x27;&#125;) // 你好 &nbsp;比如，UTF-8 编码的三个字节\\xe4、\\xbd和\\xa0合在一起才能代表字符&#39;你&#39;，而\\xe5、\\xa5和\\xbd合在一起才能代表字符&#39;好&#39;。 &nbsp;其次，一个值在从string类型向[]rune类型转换时代表着字符串会被拆分成一个个 Unicode 字符。&nbsp; 1string([]rune&#123;&#x27;\\u4F60&#x27;, &#x27;\\u597D&#x27;&#125;) // 你好 &nbsp; 别名类型和潜在类型&nbsp; 1type MyString = string &nbsp;这条声明语句表示，MyString是string类型的别名类型。顾名思义，别名类型与其源类型的区别恐怕只是在名称上，它们是完全相同的。源类型与别名类型是一对概念，是两个对立的称呼。别名类型主要是为了代码重构而存在的。 &nbsp;Go 语言内建的基本类型中就存在两个别名类型。byte是uint8的别名类型，而rune是int32的别名类型。&nbsp;一定要注意，如果我这样声明：&nbsp; 1type MyString2 string // 注意，这里没有等号。 &nbsp;MyString2和string就是两个不同的类型了。这里的MyString2是一个新的类型，不同于其他任何类型。这种方式也可以被叫做对类型的再定义。我们刚刚把string类型再定义成了另外一个类型MyString2。 &nbsp;&nbsp;对于这里的类型再定义来说，string可以被称为MyString2的潜在类型。潜在类型的含义是，某个类型在本质上是哪个类型。&nbsp;潜在类型相同的不同类型的值之间是可以进行类型转换的。因此，MyString2类型的值与string类型的值可以使用类型转换表达式进行互转。&nbsp;但对于集合类的类型[]MyString2与[]string来说这样做却是不合法的，因为[]MyString2与[]string的潜在类型不同，分别是[]MyString2和[]string。另外，即使两个不同类型的潜在类型相同，它们的值之间也不能进行判等或比较，它们的变量之间也不能赋值。&nbsp;如果通过import . XXX这种方式导入的代码包中的变量与当前代码包中的变量重名了，那么 Go 语言是会把它们当做“可重名变量”看待还是会报错呢？&nbsp;这两个变量会成为“可重名变量”。虽然这两个变量在这种情况下的作用域都是当前代码包的当前文件，但是它们所处的代码块是不同的。&nbsp;当前文件中的变量处在该文件所代表的代码块中，而被导入的代码包中的变量却处在声明它的那个文件所代表的代码块中。当然，我们也可以说被导入的代码包所代表的代码块包含了这个变量。&nbsp;在当前文件中，本地的变量会“屏蔽”掉被导入的变量。 &nbsp;&nbsp; 原文链接：https://time.geekbang.org/column/intro/112","categories":[{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Go语言","slug":"Go语言","permalink":"http://wht6.github.io/tags/Go%E8%AF%AD%E8%A8%80/"}]},{"title":"STM32实现德飞莱LED文字滚动效果","slug":"STM32实现德飞莱LED文字滚动效果","date":"2021-03-22T01:32:15.000Z","updated":"2022-03-20T01:44:33.480Z","comments":true,"path":"posts/dc68.html","link":"","permalink":"http://wht6.github.io/posts/dc68.html","excerpt":"","text":"STM32实现德飞莱LED文字滚动效果显示静止文字各个引脚功能：OE：使能端，输出高电平关闭屏幕，行选和列选切换的时候需要关闭一下屏幕，防止产生虚影。 D/C/B/A：每个引脚有0、1两个状态，四个引脚用于存储一个四位的二进制数，0/0/0/0是第一行，0/0/1/0是第三行。 R1/G1:R1是红色，G1是绿色，低电平点亮。 SCK：时钟信号。 LAT：锁存器。0打开，1关闭。 列选中原理：将一个16位的二进制串行输入到R1或G1，利用时钟脉冲信号触发寄存器存储当前值，然后通过 LATCH 锁存器将寄存器的值保存。 使用取字软件取出的C51格式的值。 例如：/— 文字: 中 —//— 宋体12; 此字体下对应的点阵为：宽x高=16x16 —/0x00,0x00,0x0F,0x08,0x08,0x08,0x08,0xFF,0x08,0x08,0x08,0x08,0x0F,0x00,0x00,0x00,0x00,0x00,0xF0,0x20,0x20,0x20,0x20,0xFF,0x20,0x20,0x20,0x20,0xF0,0x00,0x00,0x00, 其中第i位十六进制数和第i+16位十六进制数表示的是第i行的状态。i从0到15，逐次点亮16行。 12345678910111213141516171819202122232425void display_char()&#123;&#x2F;&#x2F;显示一个静止的汉字 int i&#x3D;0; int j&#x3D;0; int change; for(i&#x3D;0;i&lt;16;i++) &#123; change&#x3D;(zi[i]&lt;&lt;8)|zi[i+16];&#x2F;&#x2F;两个8位的二进制数组成一个16位的二进制数 LAT &#x3D; 0; for(j &#x3D; 0; j &lt; 16; j++)&#123; R1 &#x3D; ((~change)&gt;&gt;j)&amp;1; SCK &#x3D; 0; SCK &#x3D; 1; &#125; OE &#x3D; 1; get_row(i); LAT &#x3D; 1; OE &#x3D; 0; &#125;&#125;void get_row(int i)&#123;&#x2F;&#x2F;选中行 A&#x3D;i&amp;1; B&#x3D;(i&amp;2)&gt;&gt;1; C&#x3D;(i&amp;4)&gt;&gt;2; D&#x3D;(i&amp;8)&gt;&gt;3;&#125; 文字滚动显示方式选择：首先搞清楚，取模的方式有横向取模和纵向取模两种。 一般以C51格式取出来的16x16的汉字，由32个十六进制组成。 每个十六进制，0是白色的像素点，1是黑色的像素点，文字是由黑色的像素点组成。 我们需要指导每个十六进制对应汉字的哪个位置。 横向取模： /— 文字: 口 —//— 新宋体12; 此字体下对应的点阵为：宽x高=16x16 —/0x00,0x00,0x00,0x00,0x3F,0xF8,0x20,0x08,0x20,0x08,0x20,0x08,0x20,0x08,0x20,0x08,0x20,0x08,0x20,0x08,0x20,0x08,0x20,0x08,0x20,0x08,0x3F,0xF8,0x20,0x08,0x00,0x00, 其中第一行的前两个十六进制，代表的是最上边的16个像素点。每两个十六进制一组，自上而下表示完所有的像素点。 纵向取模： /— 文字: 口 —//— 新宋体12; 此字体下对应的点阵为：宽x高=16x16 —/0x00,0x00,0x3F,0x20,0x20,0x20,0x20,0x20,0x20,0x20,0x20,0x20,0x3F,0x00,0x00,0x00,0x00,0x00,0xFE,0x04,0x04,0x04,0x04,0x04,0x04,0x04,0x04,0x04,0xFE,0x00,0x00,0x00, 其中上边一行的16个十六进制，表示的是汉字的上半部分，第一行的第一个表示纵着的8个像素点，自左向右表示汉字的上半部分，第二行十六进制以同样的方式表示汉字的下半部分。 然后，需要根据LED显示的方式，选择合适的取模方式。（当然，也可以不这么麻烦，无非多试几次） R1或者G1 的寄存器，是要输入十六进制锁存的。因为一行或一列LED点有16个，所以需要输入两个字节大小的数据。 输入1，转换成连个16进制就是0x00,0x01。观察LED屏，发现亮点在一个角上。 根据上面取模的方式，需要直接把取模得到的16进制直接输入到R1或者G1 的寄存器，所以只能由两种摆放的方式，一个是把亮点放到右上角，对应横向取模，一个是把亮点放到左下角，对应纵向取模。 横向取模，每次把左右相邻的两个十六进制输入到寄存器，自上而下刷新显示。 纵向取模，每次把两行的同一列的两个十六进制输入到寄存器，自左向右刷新显示。 如果，不级联显示，这两种方式都是可以的，单个屏幕的滚动和静止显示都没问题。 但是，当你想要级联的时候，想要做一个较长屏幕的滚动显示，你会发现级联接口正好对应着横向取模的方式。 所以，放弃纵向取模的显示方式，改用横向取模的显示方式。 滚动效果原理其实很简单，若干个静止的状态连续的有规律地切换，就形成了滚动的效果。 而每次切换改变的仅仅是每块屏的最左边一列和最右边一列。 因此只需要，前一个字节左移n位|后一个字节右移8-n位, 但是，这个寄存器在输入的时候必须把bit流倒着输入，所以就需要倒序取模，前一个字节右移n位|后一个字节左移8-n位。 然后需要保持这个状态静止一段时间才完成一次的移动。保持的方法是每次刷新加一个细小的延时，延后循环n次。 当完成8次移动之后，需要取出一个新的字节。所以需要预先多取出两个字节，移动16次一个循环，循环完成后取出下一个汉字。","categories":[{"name":"嵌入式","slug":"嵌入式","permalink":"http://wht6.github.io/categories/%E5%B5%8C%E5%85%A5%E5%BC%8F/"}],"tags":[]},{"title":"CNN的三个重要性质","slug":"CNN的三个重要性质","date":"2021-03-22T00:48:44.000Z","updated":"2022-03-20T01:44:16.525Z","comments":true,"path":"posts/2c33.html","link":"","permalink":"http://wht6.github.io/posts/2c33.html","excerpt":"","text":"CNN的三个重要性质参数共享权值共享得益于卷积自身的性质，假设输入时$W \\times H$的图像，卷积核为$m\\times m$，进行权值共享时总共参数个数是$m\\times m\\times channels$；若不进行权值共享，每个像素需要一个单独的参数，则总参数数量则为$W\\times H\\times channels$。总参数大大增加，导致占用大量内存，模型训练缓慢。 局部连接对于局部连接而言：层间神经只有局部范围内的连接，在这个范围内采用全连接的方式，超过这个范围的神经元则没有连接；连接与连接之间独立参数，相比于全连接减少了感受域外的连接，有效减少参数规模。全连接：层间神经元完全连接，每个输出神经元可以获取到所有神经元的信息，有利于信息汇总，常置于网络末尾。 平移等变性卷积神经网络参数共享的特殊形式使得神经网络层具有平移等变性(equivariance)。对于图像而言，卷积产生一个 2 维映射来表明某些特征在输入中出现的位置。如果我们移动输入中的对象，它的表示也会在输出中移动同样的量。对于放缩和旋转等其他变换，卷积却不是天然等变的。关于池化，无论采用何种池化函数，当输入作出少量平移时，池化能帮助输入的表示近似不变(invariant)。对于平移的不变性是指当我们对输入进行少量平移时，经过池化函数后的大多数输出并不会发生改变。这意味着池化对特征位置不敏感，只有当我们不关心特征具体出现的位置时，池化才是合理的。CNN 中的卷积操作具有平移等变性，但池化操作具有局部平移不变性。两者矛盾地统一于 CNN 中。 参考： https://zhuanlan.zhihu.com/p/89109931 http://blog.sciencenet.cn/blog-3428464-1255252.html","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://wht6.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"MathType使用技巧","slug":"MathType使用技巧","date":"2021-03-18T01:57:44.000Z","updated":"2022-03-20T01:45:12.117Z","comments":true,"path":"posts/a469.html","link":"","permalink":"http://wht6.github.io/posts/a469.html","excerpt":"","text":"MathType使用技巧安装下载官网下载 安装后免费使用30天 删除注册表30天结束之后，win+R 打开运行窗口，输入 regedit 打开注册表编辑器。 删除目录 HKEY_CURRENT_USER\\Software\\Install Options下的 Options6.9 文件。 重新获得30天的免费试用，不过需要每个月重新删除一次。 使用使用LaTex编辑公式点击MathType菜单中的“预置”—“工作区预置”，勾选“允许从键盘输入Tex语言”。 LaTex格式复制公式点击MathType菜单中的“预置”—“剪切和复制预置”，选中下面两项，点击确定。然后就可以以LaTex格式直接复制到LaTex或markdown中使用了。 使用空心字母空心字母还是比较常用，这里说明空心字母的使用。 “编辑”—“插入符号”—选择字体“Euclid Math Two”—“插入”。 如果要复制成LaTex，需要更改“剪切与复制预置”中的LaTex格式为AMSLaTex。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://wht6.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"工具","slug":"工具","permalink":"http://wht6.github.io/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"ubuntu20.04配置pytorch环境","slug":"ubuntu20-04配置pytorch环境","date":"2021-03-17T06:17:44.000Z","updated":"2022-03-20T01:43:45.415Z","comments":true,"path":"posts/ee8d.html","link":"","permalink":"http://wht6.github.io/posts/ee8d.html","excerpt":"","text":"ubuntu20.04配置pytorch环境Linux相关命令cd /home 进入 ‘/ home’ 目录’cd .. 返回上一级目录cd ~ 进入个人的主目录cd - 返回上次所在的目录ls 查看目录中的文件ls -a 显示隐藏文件mkdir dir1 创建一个叫做 ‘dir1’ 的目录’rm -f file1 删除一个叫做 ‘file1’ 的文件’rm -rf dir1 删除一个叫做 ‘dir1’ 的目录并同时删除其内容cp -i file1 file2 将文档 file1复制成file2，复制后名称被改file2cp -i file1 dir1 将文档 file1复制到dir1目录下，复制后名称仍未file1cp -r dir1 dir2 将dir1下的所有文件及其子目录复制到dir2中locate 文件名 搜索文件sudo 管理员权限chmod a+x file 给file添加用户可执行权限chmod 777 file 或 chmod a=rwx file 设置所有人可以读写及执行unrar x file1.rar 解压rar包tar -zxvf archive.tar.gz 解压一个gzip格式的压缩包tar -jxvf archive.tar.bz2 解压一个bzip2格式的压缩包unzip file1.zip 解压一个zip格式压缩包dpkg -i package.deb 安装/更新一个 deb 包apt-get install package_name 安装/更新一个 deb 包apt-get update 升级列表中的软件包apt-get upgrade 升级所有已安装的软件cat file1 从第一个字节开始正向查看文件的内容ifconfig eth0 显示一个以太网卡的配置reboot 重启 准备工作如果需要远程，可以在控制端和本机上安装teamviewer，邮箱注册，邮箱会收到TeamViewer发过来的激活账户邮件，点击激活连接就能够登录客户端了。然后会再收到一封TeamViewer发过来的设备授权认证邮件，选择信任。连接的时候还要验证，需要手机号验证。如果感觉太繁琐，可以装VNC。 先用自带的编辑器更改apt源，不然后面下载和更新软件会很难受。清华镜像 vim安装 配置anaconda安装anacondaanaconda安装包选择最新版本安装，下载到本地后sudo sh xxx.sh执行。 添加anaconda源清华镜像也可以运行下面命令添加12345678910conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/rconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 创建虚拟conda环境选择需要的python版本，新建一个env虚拟conda环境1234conda create -n torch_1_7 python=3.7conda env listconda activate torch_1_7pip list 配置CUDA下载和安装nvidia-smi 查看本机显卡驱动、显存使用率和GPU利用率（显示的cuda version是驱动对应的cuda版本，而不是已经安装了该版本的cuda）查看本机显卡的算力显卡的受支持情况选择合适的cuda版本。下载cuda选择系统版本-runfile文件 可以直接用官方提供的命令安装（如果网络慢，可以先下载到本地在安装）安装的时候，驱动不要选（驱动已经有了），其他默认。（如果之前有装cuda，先卸载 cd /usr/local/cuda/bin sudo ./cuda-uninstaller） 配置环境变量12345678vim ~/.bashrcexport CUDA_HOME=/usr/local/cudaexport LD_LIBRARY_PATH=$&#123;LD_LIBRARY_PATH&#125;:$&#123;CUDA_HOME&#125;/lib64export PATH=$&#123;CUDA_HOME&#125;/bin:$&#123;PATH&#125;source ~/.bashrc 用nvcc -V或nvcc —version验证是否安装成功 安装对应版本的cudnncuDNN Archive 需要注册登录下载，解压，进入解压后文件夹。123sudo cp cuda/include/cudnn*.h /usr/local/cuda/includesudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*验证新版本：cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR -A 2旧版本：cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 配置pytorch环境旧版本最新版选择正确的版本，cudatoolkit的版本要和已经安装cuda版本一致（避免冲突）。感兴趣可以看cudatoolkit和cuda调用安装的时候去掉 -c pytorch(带上会影响下载速度)也可以从网上找对应的包（可以先用命令行获取包名，再退出，然后手动下载安装）（感兴趣移步：conda和pip安装库之间的区别）下面是几个提供免费下载的网站：https://www.lfd.uci.edu/~gohlke/pythonlibs/https://pypi.org/https://anaconda.org/pytorch/repo 配置需要的gcc版本如果需要构建框架，忽略这一步。 下载gcc解压后进入解压后的文件夹中运行./contrib/download_prerequisites（查看需要的依赖，手动下载，手动构建）下载依赖依赖构建过程（下边是个例子）12345tar -jxvf gmp-6.1.0.tar.bz2cd gmp-6.1.0./configuremakemake install构建gcc1234567mkdir gcc-build-7.3.0 cd gcc-build-7.3.0../configure --enable-checking=release --enable-languages=c,c++ --disable-multilibmakemake installgcc版本越高，构建的越慢。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://wht6.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://wht6.github.io/tags/pytorch/"}]},{"title":"深度学习的线性回归","slug":"深度学习的线性回归","date":"2021-03-17T06:14:48.000Z","updated":"2022-03-20T01:44:58.027Z","comments":true,"path":"posts/5620.html","link":"","permalink":"http://wht6.github.io/posts/5620.html","excerpt":"","text":"深度学习的线性回归线性回归定义线性回归（ Linear Regression） 是机器学习和统计学中最基础和最广泛应用的模型， 是一种对自变量和因变量之间关系进行建模的回归分析．线性回归模型的一般形式： \\hat{y}=\\omega^Tx+b其中权重向量$\\omega$与偏置b都是可学习的参数。给定一个训练集，每个样本包括特征向量x和标签y，我们希望能够学习到一组参数$\\omega$使模型估计的$\\hat{y}$能够很好的预测真实值y。这就引出了两个问题，如何训练以及如何评估。 模型的训练在模型训练中，我们需要衡量预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。我们把衡量这些误差的函数称为损失函数。当模型和损失函数形式较为简单时，误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数值。这类解叫作数值解（numerical solution）。 在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）$\\mathcal{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数（学习率）的乘积作为模型参数在本次迭代的减小量。 \\omega \\leftarrow \\omega - \\eta \\frac{1}{K}\\sum {\\frac{{\\partial L(y,\\hat y)}}{{\\partial \\omega }}}其中，$\\eta$是学习率，K是小批量样本的个数，L是损失函数。 神经网络图在深度学习中，我们可以使用神经网络图直观地表现模型结构。 线性回归模型：$\\hat{y}=x_1w_1+x_2w_2+b$ 为了更清晰地展示线性回归作为神经网络的结构，下图使用神经网络图表示上面的线性回归模型。神经网络图隐去了模型参数权重和偏差。 由于输入层并不涉及计算，上图神经网络的层数为1。其中，$x_1$和$x_2$是输入，输入个数为2，输入个数也叫特征数或特征向量维度。箭头指向输出节点表示加权求和的操作。输出层中的神经元和输入层中各个输入完全连接。因此，这里的输出层又叫全连接层（fully-connected layer）。 Softmax回归Logistic回归Logistic 回归（ Logistic Regression，LR） 是一种常用的处理二分类问题的线性模型． ${\\rm{y}} \\in \\{ 0,1\\}$是一个二分类问题，显然用单纯的线性模型无法解决。我们需要一个非线性的激活函数，这里选择Logistic函数作为激活函数，将线性函数的值域挤压的0到1之间来表示概率。那么标签$y = 1$的概率为 \\hat{y}=p(y = 1) = \\frac{1}{{1 + \\exp ( - \\omega x)}}当概率值大于0.5是认为$y = 1$，否则$y = 0$。 Logistic 回归采用交叉熵作为损失函数， 并使用梯度下降法来对参数进行优化。 Softmax回归Logistic 回归只使用二分类问题，放在神经网络中只有单一的输出。对于多分类问题就要用到Softmax回归。 o_1=x_1w_11+x_2w_21+x_3w_31+x_4w_41+b_1,\\\\\\\\ o_2=x_1w_12+x_2w_22+x_3w_32+x_4w_42+b_2,\\\\\\\\ o_3=x_1w_13+x_2w_23+x_3w_33+x_4w_43+b_3.我们需要将三个输出的至于压缩到0到1之间，并且三者之和为1，就能够表示成概率值，那么哪个输出的概率值大就分类成哪个标签。 {\\hat y_1} = \\frac{{exp({o_1})}}{{\\sum {exp({o_i})} }},{\\hat y_2} = \\frac{{exp({o_2})}}{{\\sum {exp({o_i})} }},{\\hat y_3} = \\frac{{exp({o_3})}}{{\\sum {exp({o_i})} }}经过上面的映射，${\\hat y_1}+{\\hat y_2}+{\\hat y_3}=1$且$0\\le{\\hat y_1},{\\hat y_2},{\\hat y_3}\\le1$。 可以看出，Softmax回归同线性回归一样也是单层的全连接网络。 Softmax回归采用的也是交叉熵损失函数， 并使用梯度下降法来对参数进行优化。 Softmax交叉熵损失函数的公式： Loss({{\\bf{y}}^{(i)}},{{\\bf{\\hat y}}^{(i)}}) = - \\sum\\limits_{j = 1}^c {{y_j}^{(i)}} \\log {\\hat y_j}^{(i)}其中，c是类别数，i表示样本i。 参考《神经网络与深度学习 》邱锡鹏 《动手学深度学习》","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://wht6.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[]}],"categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://wht6.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"编程","slug":"编程","permalink":"http://wht6.github.io/categories/%E7%BC%96%E7%A8%8B/"},{"name":"运维","slug":"运维","permalink":"http://wht6.github.io/categories/%E8%BF%90%E7%BB%B4/"},{"name":"Linux","slug":"Linux","permalink":"http://wht6.github.io/categories/Linux/"},{"name":"网络","slug":"网络","permalink":"http://wht6.github.io/categories/%E7%BD%91%E7%BB%9C/"},{"name":"云原生","slug":"云原生","permalink":"http://wht6.github.io/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"name":"嵌入式","slug":"嵌入式","permalink":"http://wht6.github.io/categories/%E5%B5%8C%E5%85%A5%E5%BC%8F/"}],"tags":[{"name":"实例分割","slug":"实例分割","permalink":"http://wht6.github.io/tags/%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2/"},{"name":"Go","slug":"Go","permalink":"http://wht6.github.io/tags/Go/"},{"name":"Python","slug":"Python","permalink":"http://wht6.github.io/tags/Python/"},{"name":"MySQL","slug":"MySQL","permalink":"http://wht6.github.io/tags/MySQL/"},{"name":"GitLab","slug":"GitLab","permalink":"http://wht6.github.io/tags/GitLab/"},{"name":"DevOps","slug":"DevOps","permalink":"http://wht6.github.io/tags/DevOps/"},{"name":"进程","slug":"进程","permalink":"http://wht6.github.io/tags/%E8%BF%9B%E7%A8%8B/"},{"name":"Docker","slug":"Docker","permalink":"http://wht6.github.io/tags/Docker/"},{"name":"防火墙","slug":"防火墙","permalink":"http://wht6.github.io/tags/%E9%98%B2%E7%81%AB%E5%A2%99/"},{"name":"iptables","slug":"iptables","permalink":"http://wht6.github.io/tags/iptables/"},{"name":"Nginx","slug":"Nginx","permalink":"http://wht6.github.io/tags/Nginx/"},{"name":"InnoDB","slug":"InnoDB","permalink":"http://wht6.github.io/tags/InnoDB/"},{"name":"Ansible","slug":"Ansible","permalink":"http://wht6.github.io/tags/Ansible/"},{"name":"Shell","slug":"Shell","permalink":"http://wht6.github.io/tags/Shell/"},{"name":"负载均衡","slug":"负载均衡","permalink":"http://wht6.github.io/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"},{"name":"高可用","slug":"高可用","permalink":"http://wht6.github.io/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"},{"name":"Zabbix","slug":"Zabbix","permalink":"http://wht6.github.io/tags/Zabbix/"},{"name":"Linux","slug":"Linux","permalink":"http://wht6.github.io/tags/Linux/"},{"name":"SNMP","slug":"SNMP","permalink":"http://wht6.github.io/tags/SNMP/"},{"name":"Python语言","slug":"Python语言","permalink":"http://wht6.github.io/tags/Python%E8%AF%AD%E8%A8%80/"},{"name":"监控","slug":"监控","permalink":"http://wht6.github.io/tags/%E7%9B%91%E6%8E%A7/"},{"name":"网络","slug":"网络","permalink":"http://wht6.github.io/tags/%E7%BD%91%E7%BB%9C/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://wht6.github.io/tags/Kubernetes/"},{"name":"集群","slug":"集群","permalink":"http://wht6.github.io/tags/%E9%9B%86%E7%BE%A4/"},{"name":"容器","slug":"容器","permalink":"http://wht6.github.io/tags/%E5%AE%B9%E5%99%A8/"},{"name":"数据库","slug":"数据库","permalink":"http://wht6.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"阿里云","slug":"阿里云","permalink":"http://wht6.github.io/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"},{"name":"系统配置","slug":"系统配置","permalink":"http://wht6.github.io/tags/%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE/"},{"name":"网站搭建","slug":"网站搭建","permalink":"http://wht6.github.io/tags/%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA/"},{"name":"网络配置","slug":"网络配置","permalink":"http://wht6.github.io/tags/%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/"},{"name":"日志","slug":"日志","permalink":"http://wht6.github.io/tags/%E6%97%A5%E5%BF%97/"},{"name":"计划任务","slug":"计划任务","permalink":"http://wht6.github.io/tags/%E8%AE%A1%E5%88%92%E4%BB%BB%E5%8A%A1/"},{"name":"包管理","slug":"包管理","permalink":"http://wht6.github.io/tags/%E5%8C%85%E7%AE%A1%E7%90%86/"},{"name":"磁盘管理","slug":"磁盘管理","permalink":"http://wht6.github.io/tags/%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86/"},{"name":"权限管理","slug":"权限管理","permalink":"http://wht6.github.io/tags/%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/"},{"name":"用户管理","slug":"用户管理","permalink":"http://wht6.github.io/tags/%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86/"},{"name":"文件系统","slug":"文件系统","permalink":"http://wht6.github.io/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"},{"name":"Go语言","slug":"Go语言","permalink":"http://wht6.github.io/tags/Go%E8%AF%AD%E8%A8%80/"},{"name":"网络安全","slug":"网络安全","permalink":"http://wht6.github.io/tags/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"},{"name":"工具","slug":"工具","permalink":"http://wht6.github.io/tags/%E5%B7%A5%E5%85%B7/"},{"name":"代理","slug":"代理","permalink":"http://wht6.github.io/tags/%E4%BB%A3%E7%90%86/"},{"name":"路由器","slug":"路由器","permalink":"http://wht6.github.io/tags/%E8%B7%AF%E7%94%B1%E5%99%A8/"},{"name":"DNS","slug":"DNS","permalink":"http://wht6.github.io/tags/DNS/"},{"name":"pytorch","slug":"pytorch","permalink":"http://wht6.github.io/tags/pytorch/"}]}